<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.2"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Distributed FFT for heterogeneous GPU Systems: MPIcuFFT_Slab&lt; T &gt; Class Template Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Distributed FFT for heterogeneous GPU Systems
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.2 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",'Search','.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#nested-classes">Classes</a> &#124;
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="#pro-methods">Protected Member Functions</a> &#124;
<a href="#pro-static-methods">Static Protected Member Functions</a> &#124;
<a href="classMPIcuFFT__Slab-members.html">List of all members</a>  </div>
  <div class="headertitle"><div class="title">MPIcuFFT_Slab&lt; T &gt; Class Template Reference</div></div>
</div><!--header-->
<div class="contents">

<p><code>#include &lt;<a class="el" href="mpicufft__slab_8hpp_source.html">mpicufft_slab.hpp</a>&gt;</code></p>
<div class="dynheader">
Inheritance diagram for MPIcuFFT_Slab&lt; T &gt;:</div>
<div class="dyncontent">
 <div class="center">
  <img src="classMPIcuFFT__Slab.png" usemap="#MPIcuFFT_5FSlab_3C_20T_20_3E_map" alt=""/>
  <map id="MPIcuFFT_5FSlab_3C_20T_20_3E_map" name="MPIcuFFT_5FSlab_3C_20T_20_3E_map">
<area href="classMPIcuFFT.html" alt="MPIcuFFT&lt; T &gt;" shape="rect" coords="0,0,168,24"/>
<area href="classMPIcuFFT__Slab__Opt1.html" alt="MPIcuFFT_Slab_Opt1&lt; T &gt;" shape="rect" coords="0,112,168,136"/>
  </map>
</div></div>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="nested-classes"></a>
Classes</h2></td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">struct &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structMPIcuFFT__Slab_1_1Callback__Params.html">Callback_Params</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">struct &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structMPIcuFFT__Slab_1_1Callback__Params__Base.html">Callback_Params_Base</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:a8436fe5bff5454300df186c8e6ce3003"><td class="memItemLeft" align="right" valign="top"><a id="a8436fe5bff5454300df186c8e6ce3003"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>MPIcuFFT_Slab</b> (<a class="el" href="structConfigurations.html">Configurations</a> config, MPI_Comm comm=MPI_COMM_WORLD, int max_world_size=-1)</td></tr>
<tr class="memdesc:a8436fe5bff5454300df186c8e6ce3003"><td class="mdescLeft">&#160;</td><td class="mdescRight">Prepares for initialization (see initFFT) <br /></td></tr>
<tr class="separator:a8436fe5bff5454300df186c8e6ce3003"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a416030c5191666fc630f5d2a929486c6"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMPIcuFFT__Slab.html#a416030c5191666fc630f5d2a929486c6">initFFT</a> (<a class="el" href="structGlobalSize.html">GlobalSize</a> *global_size, <a class="el" href="structPartition.html">Partition</a> *partition, bool allocate=true)</td></tr>
<tr class="memdesc:a416030c5191666fc630f5d2a929486c6"><td class="mdescLeft">&#160;</td><td class="mdescRight">Creates the <a class="el" href="structcuFFT.html">cuFFT</a> plans and allocates the required memory space.  <a href="classMPIcuFFT__Slab.html#a416030c5191666fc630f5d2a929486c6">More...</a><br /></td></tr>
<tr class="separator:a416030c5191666fc630f5d2a929486c6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae02b3d5c8bf6bee633ba0bc3e998e4b4"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMPIcuFFT__Slab.html#ae02b3d5c8bf6bee633ba0bc3e998e4b4">setWorkArea</a> (void *device=nullptr, void *host=nullptr)</td></tr>
<tr class="memdesc:ae02b3d5c8bf6bee633ba0bc3e998e4b4"><td class="mdescLeft">&#160;</td><td class="mdescRight">Allocates the required host and device memory (see <a class="el" href="classMPIcuFFT__Slab__Opt1.html#Details">Details</a>)  <a href="classMPIcuFFT__Slab.html#ae02b3d5c8bf6bee633ba0bc3e998e4b4">More...</a><br /></td></tr>
<tr class="separator:ae02b3d5c8bf6bee633ba0bc3e998e4b4"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3e73b6a77a21e7d1046d1b7b91a4021a"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMPIcuFFT__Slab.html#a3e73b6a77a21e7d1046d1b7b91a4021a">execR2C</a> (void *out, const void *in)</td></tr>
<tr class="memdesc:a3e73b6a77a21e7d1046d1b7b91a4021a"><td class="mdescLeft">&#160;</td><td class="mdescRight">Computes a 3D FFT as illustrated by <a class="el" href="classMPIcuFFT__Slab__Opt1.html#Visualisation">Visualisation</a>.  <a href="classMPIcuFFT__Slab.html#a3e73b6a77a21e7d1046d1b7b91a4021a">More...</a><br /></td></tr>
<tr class="separator:a3e73b6a77a21e7d1046d1b7b91a4021a"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pro-methods"></a>
Protected Member Functions</h2></td></tr>
<tr class="memitem:ac98b78c3784f2fdd04ea8f40f5169107"><td class="memItemLeft" align="right" valign="top"><a id="ac98b78c3784f2fdd04ea8f40f5169107"></a>
void&#160;</td><td class="memItemRight" valign="bottom"><b>MPIsend_Thread</b> (<a class="el" href="structMPIcuFFT__Slab_1_1Callback__Params__Base.html">Callback_Params_Base</a> &amp;params, void *ptr)</td></tr>
<tr class="memdesc:ac98b78c3784f2fdd04ea8f40f5169107"><td class="mdescLeft">&#160;</td><td class="mdescRight">Only used if send_method == Streams. Then this function is used by the sending thread for the global redistribution. <br /></td></tr>
<tr class="separator:ac98b78c3784f2fdd04ea8f40f5169107"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3e2cf1133a371b4424f6d86265d8c43a"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMPIcuFFT__Slab.html#a3e2cf1133a371b4424f6d86265d8c43a">Peer2Peer_Communication</a> (void *complex_)</td></tr>
<tr class="memdesc:a3e2cf1133a371b4424f6d86265d8c43a"><td class="mdescLeft">&#160;</td><td class="mdescRight">This method implements the Peer2Peer communication method described in <a class="el" href="classMPIcuFFT__Slab__Opt1.html#Communication_Methods">Communication_Methods</a>.  <a href="classMPIcuFFT__Slab.html#a3e2cf1133a371b4424f6d86265d8c43a">More...</a><br /></td></tr>
<tr class="separator:a3e2cf1133a371b4424f6d86265d8c43a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a440e9601624ea468af3463755b054f58"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMPIcuFFT__Slab.html#a440e9601624ea468af3463755b054f58">Peer2Peer_Sync</a> (void *complex_, void *recv_ptr_)</td></tr>
<tr class="memdesc:a440e9601624ea468af3463755b054f58"><td class="mdescLeft">&#160;</td><td class="mdescRight">This method implements the <em>Sync</em> (default) Peer2Peer communication method described in <a class="el" href="classMPIcuFFT__Slab__Opt1.html#Communication_Methods">Communication_Methods</a>.  <a href="classMPIcuFFT__Slab.html#a440e9601624ea468af3463755b054f58">More...</a><br /></td></tr>
<tr class="separator:a440e9601624ea468af3463755b054f58"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2f68c714ff05771df42b153bf5dfb6e0"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMPIcuFFT__Slab.html#a2f68c714ff05771df42b153bf5dfb6e0">Peer2Peer_Streams</a> (void *complex_, void *recv_ptr_)</td></tr>
<tr class="memdesc:a2f68c714ff05771df42b153bf5dfb6e0"><td class="mdescLeft">&#160;</td><td class="mdescRight">This method implements the <em>Streams</em> Peer2Peer communication method described in <a class="el" href="classMPIcuFFT__Slab__Opt1.html#Communication_Methods">Communication_Methods</a>.  <a href="classMPIcuFFT__Slab.html#a2f68c714ff05771df42b153bf5dfb6e0">More...</a><br /></td></tr>
<tr class="separator:a2f68c714ff05771df42b153bf5dfb6e0"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0003abe16ed42e11bacb4c64c26ee24e"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMPIcuFFT__Slab.html#a0003abe16ed42e11bacb4c64c26ee24e">Peer2Peer_MPIType</a> (void *complex_, void *recv_ptr_)</td></tr>
<tr class="memdesc:a0003abe16ed42e11bacb4c64c26ee24e"><td class="mdescLeft">&#160;</td><td class="mdescRight">This method implements the <em>MPI_Type</em> Peer2Peer communication method described in <a class="el" href="classMPIcuFFT__Slab__Opt1.html#Communication_Methods">Communication_Methods</a>.  <a href="classMPIcuFFT__Slab.html#a0003abe16ed42e11bacb4c64c26ee24e">More...</a><br /></td></tr>
<tr class="separator:a0003abe16ed42e11bacb4c64c26ee24e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac0c6902ed32d3be177e8880bedef573f"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMPIcuFFT__Slab.html#ac0c6902ed32d3be177e8880bedef573f">All2All_Communication</a> (void *complex_)</td></tr>
<tr class="memdesc:ac0c6902ed32d3be177e8880bedef573f"><td class="mdescLeft">&#160;</td><td class="mdescRight">This method implements the All2All communication method described in <a class="el" href="classMPIcuFFT__Slab__Opt1.html#Communication_Methods">Communication_Methods</a>.  <a href="classMPIcuFFT__Slab.html#ac0c6902ed32d3be177e8880bedef573f">More...</a><br /></td></tr>
<tr class="separator:ac0c6902ed32d3be177e8880bedef573f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:afc098dcb1bc392b03b9d69f01abb7cd4"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMPIcuFFT__Slab.html#afc098dcb1bc392b03b9d69f01abb7cd4">All2All_Sync</a> (void *complex_)</td></tr>
<tr class="memdesc:afc098dcb1bc392b03b9d69f01abb7cd4"><td class="mdescLeft">&#160;</td><td class="mdescRight">This method implements the <em>Sync</em> (default) All2All communication method described in <a class="el" href="classMPIcuFFT__Slab__Opt1.html#Communication_Methods">Communication_Methods</a>.  <a href="classMPIcuFFT__Slab.html#afc098dcb1bc392b03b9d69f01abb7cd4">More...</a><br /></td></tr>
<tr class="separator:afc098dcb1bc392b03b9d69f01abb7cd4"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aed8b41097a28a04a2b35633c7485b695"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMPIcuFFT__Slab.html#aed8b41097a28a04a2b35633c7485b695">All2All_MPIType</a> (void *complex_)</td></tr>
<tr class="memdesc:aed8b41097a28a04a2b35633c7485b695"><td class="mdescLeft">&#160;</td><td class="mdescRight">This method implements the <em>MPI_Type</em> (default) All2All communication method described in <a class="el" href="classMPIcuFFT__Slab__Opt1.html#Communication_Methods">Communication_Methods</a>.  <a href="classMPIcuFFT__Slab.html#aed8b41097a28a04a2b35633c7485b695">More...</a><br /></td></tr>
<tr class="separator:aed8b41097a28a04a2b35633c7485b695"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pro-static-methods"></a>
Static Protected Member Functions</h2></td></tr>
<tr class="memitem:a7966ee38d41875c68cd893279951bdd2"><td class="memItemLeft" align="right" valign="top"><a id="a7966ee38d41875c68cd893279951bdd2"></a>
static void CUDART_CB&#160;</td><td class="memItemRight" valign="bottom"><b>MPIsend_Callback</b> (void *data)</td></tr>
<tr class="memdesc:a7966ee38d41875c68cd893279951bdd2"><td class="mdescLeft">&#160;</td><td class="mdescRight">Only used if send_method == Streams. Then this function is called by cudaCallHostFunc to notify a second thread to start sending the copied data. <br /></td></tr>
<tr class="separator:a7966ee38d41875c68cd893279951bdd2"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><div class="compoundTemplParams">template&lt;typename T&gt;<br />
class MPIcuFFT_Slab&lt; T &gt;</div><h1><a class="anchor" id="Visualisation"></a>
Visualisation</h1>
<div class="image">
<img src="Slab_Default.png" alt=""/>
</div>
<p> The above example illustrates the procedure for P = 3. Each slab is processed by a single rank (e.g. the green slab by rank 2). At first, the FFT is computed in z- and y- direction. After the global redistribution, the remaining FFT in x-direction is computed as well. </p>
<h1><a class="anchor" id="Details"></a>
Details</h1>
<p >There are a few technical details to consider when using this option:</p><ul>
<li>Required memory space (besides workspace required by <a class="el" href="structcuFFT.html">cuFFT</a>):<ol type="1">
<li>Send Method: <em>Sync</em> or <em>Streams:</em> <ul>
<li>An additional send- and recv-buffer is required</li>
<li>The recv-buffer can be used as the input for the remaining FFT in x-direction</li>
<li>Depending on whether MPI is CUDA-aware, the send- and recv-buffer are allocated on device or host memory.</li>
</ul>
</li>
<li>Send Method: <em>MPI_Types:</em> <ul>
<li>Same as above, only that an additional send-buffer can be omitted if MPI is CUDA-aware.</li>
</ul>
</li>
</ol>
</li>
<li>Required cudaMemcpy operations for each send/recv/local transpose:<ol type="1">
<li>Send Method: <em>Sync</em> or <em>Streams:</em> <ul>
<li>send: 2D (or 3D) memcpy</li>
<li>recv: 1D memcpy (only if MPI is not CUDA-aware)</li>
</ul>
</li>
<li>Send Method: <em>MPI_Types:</em> <ul>
<li>send &amp; recv: 1D memcpy (only if MPI is not CUDA-aware)</li>
<li>Beware: CUDA-aware MPI + <em>MPI_Types</em> might result in an enormous performance loss</li>
</ul>
</li>
</ol>
</li>
<li>For the two different FFT's, we use the following <a class="el" href="structcuFFT.html">cuFFT</a> plans (with cufftMakePlanMay64):<ol type="1">
<li>zy-direction:<ul>
<li>default data layout</li>
<li>batch: input_sizes_x[pidx]</li>
</ul>
</li>
<li>x-direction:<ul>
<li>istride = output_sizes_y[pidx]*output_size_z, idist = 1</li>
<li>ostride = output_sizes_y[pidx]*output_size_z, odist = 1</li>
<li>batch = output_sizes_y[pidx]*output_size_z </li>
</ul>
</li>
</ol>
</li>
</ul>
<h1><a class="anchor" id="Communication_Methods"></a>
Communication_Methods</h1>
<p >There are two available communication methods:</p><ol type="1">
<li>Peer2Peer MPI Communication: Here, the MPI procedures <em>MPI_Isend</em> and <em>MPI_Irecv</em> are used for non-blocking communication between the different ranks. As can be seen in Visualization, each rank has to send a non-continuous region (highlighted in red) to rank 2. Therefore, the sending procedure has to perform a cudaMemcpy2D before it can start sending. To interleave cudaMemcpy2D with MPI_Isend, there are three available options:<ol type="a">
<li><em>Sync</em> (default): Copy each non-continuous region (e.g. the one highlighted in red) and call cudaDeviceSynchronize before MPI_Isend.</li>
<li><em>Streams:</em> Instead of using cudaDeviceSynchronize, the sending procedure is called in a second thread. The thread is notified by cudaCallHostFunc after the relevant memcpy is complete.</li>
<li><em>MPI_Type:</em> Here, we avoid the 2D memcpy altogether and use MPI_Type_vector to send a non-continuous data region.</li>
</ol>
</li>
<li>All2All MPI Communication: Here, the MPI procedures <em>MPI_Alltoallv</em> (for <em>Sync</em>) and <em>MPI_Alltoallw</em> (for <em>MPI_Type</em>) are used for global communication between all ranks. As above, there are multiple options to prepare the sending procedure:<ol type="a">
<li><em>Sync</em> (default): Copy the non-continuous regions in a seperated send-buffer using cudaMemcpy2DAsync. Before calling <em>MPI_Alltoallv</em>, one has to call cudaDeviceSynchronize.</li>
<li><em>MPI_Type:</em> Again, MPI_Type_vector can be used to avoid the 2D memcpy altogether. </li>
</ol>
</li>
</ol>
</div><h2 class="groupheader">Member Function Documentation</h2>
<a id="ac0c6902ed32d3be177e8880bedef573f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac0c6902ed32d3be177e8880bedef573f">&#9670;&nbsp;</a></span>All2All_Communication()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename T &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void <a class="el" href="classMPIcuFFT__Slab.html">MPIcuFFT_Slab</a>&lt; T &gt;::All2All_Communication </td>
          <td>(</td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>complex_</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>This method implements the All2All communication method described in <a class="el" href="classMPIcuFFT__Slab__Opt1.html#Communication_Methods">Communication_Methods</a>. </p>

<p>Reimplemented in <a class="el" href="classMPIcuFFT__Slab__Opt1.html#a6ce8229df9653d64215fa79c3e536da1">MPIcuFFT_Slab_Opt1&lt; T &gt;</a>.</p>

</div>
</div>
<a id="aed8b41097a28a04a2b35633c7485b695"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aed8b41097a28a04a2b35633c7485b695">&#9670;&nbsp;</a></span>All2All_MPIType()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename T &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void <a class="el" href="classMPIcuFFT__Slab.html">MPIcuFFT_Slab</a>&lt; T &gt;::All2All_MPIType </td>
          <td>(</td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>complex_</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>This method implements the <em>MPI_Type</em> (default) All2All communication method described in <a class="el" href="classMPIcuFFT__Slab__Opt1.html#Communication_Methods">Communication_Methods</a>. </p>

<p>Reimplemented in <a class="el" href="classMPIcuFFT__Slab__Opt1.html#a745cf973adf0bb8bc6b6a5af3f39d373">MPIcuFFT_Slab_Opt1&lt; T &gt;</a>.</p>

</div>
</div>
<a id="afc098dcb1bc392b03b9d69f01abb7cd4"></a>
<h2 class="memtitle"><span class="permalink"><a href="#afc098dcb1bc392b03b9d69f01abb7cd4">&#9670;&nbsp;</a></span>All2All_Sync()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename T &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void <a class="el" href="classMPIcuFFT__Slab.html">MPIcuFFT_Slab</a>&lt; T &gt;::All2All_Sync </td>
          <td>(</td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>complex_</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>This method implements the <em>Sync</em> (default) All2All communication method described in <a class="el" href="classMPIcuFFT__Slab__Opt1.html#Communication_Methods">Communication_Methods</a>. </p>

<p>Reimplemented in <a class="el" href="classMPIcuFFT__Slab__Opt1.html#a62c0a691a95e1085f005ddb706b3212b">MPIcuFFT_Slab_Opt1&lt; T &gt;</a>.</p>

</div>
</div>
<a id="a3e73b6a77a21e7d1046d1b7b91a4021a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a3e73b6a77a21e7d1046d1b7b91a4021a">&#9670;&nbsp;</a></span>execR2C()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename T &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void <a class="el" href="classMPIcuFFT__Slab.html">MPIcuFFT_Slab</a>&lt; T &gt;::execR2C </td>
          <td>(</td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>out</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const void *&#160;</td>
          <td class="paramname"><em>in</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Computes a 3D FFT as illustrated by <a class="el" href="classMPIcuFFT__Slab__Opt1.html#Visualisation">Visualisation</a>. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">out</td><td>is reused for each <a class="el" href="structcuFFT.html">cuFFT</a> computation. Therefore is must hold that:<ul>
<li>2D FFT (zy-direction): size(out) &gt;= input_sizes_x[pidx]*Ny*(Nz/2+1)</li>
<li>1D FFT (x-direction): size(out) &gt;= Nx*output_sizes_y[pidx]*(Nz/2+1) </li>
</ul>
</td></tr>
  </table>
  </dd>
</dl>

<p>Implements <a class="el" href="classMPIcuFFT.html">MPIcuFFT&lt; T &gt;</a>.</p>

<p>Reimplemented in <a class="el" href="classMPIcuFFT__Slab__Opt1.html#a4b2ad9b69c2aa832675b08157123c81f">MPIcuFFT_Slab_Opt1&lt; T &gt;</a>.</p>

</div>
</div>
<a id="a5b31e71b2c1db95a98f6d4ba2e9ce2d8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a5b31e71b2c1db95a98f6d4ba2e9ce2d8">&#9670;&nbsp;</a></span>getInSize()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename T &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void <a class="el" href="classMPIcuFFT__Slab.html">MPIcuFFT_Slab</a>&lt; T &gt;::getInSize </td>
          <td>(</td>
          <td class="paramtype">size_t *&#160;</td>
          <td class="paramname"><em>isize</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Implements <a class="el" href="classMPIcuFFT.html">MPIcuFFT&lt; T &gt;</a>.</p>

</div>
</div>
<a id="ac75bdc852964b0d9c705ed26f1b0a7ef"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac75bdc852964b0d9c705ed26f1b0a7ef">&#9670;&nbsp;</a></span>getInStart()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename T &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void <a class="el" href="classMPIcuFFT__Slab.html">MPIcuFFT_Slab</a>&lt; T &gt;::getInStart </td>
          <td>(</td>
          <td class="paramtype">size_t *&#160;</td>
          <td class="paramname"><em>istart</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Implements <a class="el" href="classMPIcuFFT.html">MPIcuFFT&lt; T &gt;</a>.</p>

</div>
</div>
<a id="a8d76a8cdd7ea8a1139114694ac84af9b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a8d76a8cdd7ea8a1139114694ac84af9b">&#9670;&nbsp;</a></span>getOutSize()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename T &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void <a class="el" href="classMPIcuFFT__Slab.html">MPIcuFFT_Slab</a>&lt; T &gt;::getOutSize </td>
          <td>(</td>
          <td class="paramtype">size_t *&#160;</td>
          <td class="paramname"><em>osize</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Implements <a class="el" href="classMPIcuFFT.html">MPIcuFFT&lt; T &gt;</a>.</p>

</div>
</div>
<a id="a044f37c99c78c55e267ba446bb9f1cca"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a044f37c99c78c55e267ba446bb9f1cca">&#9670;&nbsp;</a></span>getOutStart()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename T &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void <a class="el" href="classMPIcuFFT__Slab.html">MPIcuFFT_Slab</a>&lt; T &gt;::getOutStart </td>
          <td>(</td>
          <td class="paramtype">size_t *&#160;</td>
          <td class="paramname"><em>ostart</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Implements <a class="el" href="classMPIcuFFT.html">MPIcuFFT&lt; T &gt;</a>.</p>

</div>
</div>
<a id="a416030c5191666fc630f5d2a929486c6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a416030c5191666fc630f5d2a929486c6">&#9670;&nbsp;</a></span>initFFT()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename T &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void <a class="el" href="classMPIcuFFT__Slab.html">MPIcuFFT_Slab</a>&lt; T &gt;::initFFT </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="structGlobalSize.html">GlobalSize</a> *&#160;</td>
          <td class="paramname"><em>global_size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="structPartition.html">Partition</a> *&#160;</td>
          <td class="paramname"><em>partition</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>allocate</em> = <code>true</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Creates the <a class="el" href="structcuFFT.html">cuFFT</a> plans and allocates the required memory space. </p>
<p >The function starts by initializing <em>pidx</em>, along with <em>input_size(s)_*</em> and <em>output_size(s)_*</em>. Afterwards, the <a class="el" href="structcuFFT.html">cuFFT</a> plans are created as described in <a class="el" href="classMPIcuFFT__Slab__Opt1.html#Details">Details</a>. Finally, setWorkArea is called to allocate the device memory.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">global_size</td><td>specifies the dimensions Nx, Ny and Nz (of the global input data) </td></tr>
    <tr><td class="paramname">partition</td><td>is omitted for slab decomposition. The number of partitions is given by <em>pcnt</em> (= MPI_Comm_size). </td></tr>
    <tr><td class="paramname">allocate</td><td>specifies if device memory has to be allocated </td></tr>
  </table>
  </dd>
</dl>

<p>Implements <a class="el" href="classMPIcuFFT.html">MPIcuFFT&lt; T &gt;</a>.</p>

<p>Reimplemented in <a class="el" href="classMPIcuFFT__Slab__Opt1.html#a919876bad968d430199a910606ea6c65">MPIcuFFT_Slab_Opt1&lt; T &gt;</a>.</p>

</div>
</div>
<a id="a3e2cf1133a371b4424f6d86265d8c43a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a3e2cf1133a371b4424f6d86265d8c43a">&#9670;&nbsp;</a></span>Peer2Peer_Communication()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename T &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void <a class="el" href="classMPIcuFFT__Slab.html">MPIcuFFT_Slab</a>&lt; T &gt;::Peer2Peer_Communication </td>
          <td>(</td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>complex_</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>This method implements the Peer2Peer communication method described in <a class="el" href="classMPIcuFFT__Slab__Opt1.html#Communication_Methods">Communication_Methods</a>. </p>

<p>Reimplemented in <a class="el" href="classMPIcuFFT__Slab__Opt1.html#a7560968500f6417b7c62e9fc857f67f1">MPIcuFFT_Slab_Opt1&lt; T &gt;</a>.</p>

</div>
</div>
<a id="a0003abe16ed42e11bacb4c64c26ee24e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a0003abe16ed42e11bacb4c64c26ee24e">&#9670;&nbsp;</a></span>Peer2Peer_MPIType()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename T &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void <a class="el" href="classMPIcuFFT__Slab.html">MPIcuFFT_Slab</a>&lt; T &gt;::Peer2Peer_MPIType </td>
          <td>(</td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>complex_</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>recv_ptr_</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>This method implements the <em>MPI_Type</em> Peer2Peer communication method described in <a class="el" href="classMPIcuFFT__Slab__Opt1.html#Communication_Methods">Communication_Methods</a>. </p>

<p>Reimplemented in <a class="el" href="classMPIcuFFT__Slab__Opt1.html#ac39871f1467fc0179c2f9f26fa68f707">MPIcuFFT_Slab_Opt1&lt; T &gt;</a>.</p>

</div>
</div>
<a id="a2f68c714ff05771df42b153bf5dfb6e0"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2f68c714ff05771df42b153bf5dfb6e0">&#9670;&nbsp;</a></span>Peer2Peer_Streams()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename T &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void <a class="el" href="classMPIcuFFT__Slab.html">MPIcuFFT_Slab</a>&lt; T &gt;::Peer2Peer_Streams </td>
          <td>(</td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>complex_</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>recv_ptr_</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>This method implements the <em>Streams</em> Peer2Peer communication method described in <a class="el" href="classMPIcuFFT__Slab__Opt1.html#Communication_Methods">Communication_Methods</a>. </p>

<p>Reimplemented in <a class="el" href="classMPIcuFFT__Slab__Opt1.html#a878a743732c3ad5c8182f7a4348eba68">MPIcuFFT_Slab_Opt1&lt; T &gt;</a>.</p>

</div>
</div>
<a id="a440e9601624ea468af3463755b054f58"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a440e9601624ea468af3463755b054f58">&#9670;&nbsp;</a></span>Peer2Peer_Sync()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename T &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void <a class="el" href="classMPIcuFFT__Slab.html">MPIcuFFT_Slab</a>&lt; T &gt;::Peer2Peer_Sync </td>
          <td>(</td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>complex_</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>recv_ptr_</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>This method implements the <em>Sync</em> (default) Peer2Peer communication method described in <a class="el" href="classMPIcuFFT__Slab__Opt1.html#Communication_Methods">Communication_Methods</a>. </p>

<p>Reimplemented in <a class="el" href="classMPIcuFFT__Slab__Opt1.html#a6d6beec0dd5b15566873013f6a7ad94c">MPIcuFFT_Slab_Opt1&lt; T &gt;</a>.</p>

</div>
</div>
<a id="ae02b3d5c8bf6bee633ba0bc3e998e4b4"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae02b3d5c8bf6bee633ba0bc3e998e4b4">&#9670;&nbsp;</a></span>setWorkArea()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename T &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void <a class="el" href="classMPIcuFFT__Slab.html">MPIcuFFT_Slab</a>&lt; T &gt;::setWorkArea </td>
          <td>(</td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>device</em> = <code>nullptr</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>host</em> = <code>nullptr</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Allocates the required host and device memory (see <a class="el" href="classMPIcuFFT__Slab__Opt1.html#Details">Details</a>) </p>

<p>Implements <a class="el" href="classMPIcuFFT.html">MPIcuFFT&lt; T &gt;</a>.</p>

</div>
</div>
<hr/>The documentation for this class was generated from the following files:<ul>
<li>include/<a class="el" href="mpicufft__slab_8hpp_source.html">mpicufft_slab.hpp</a></li>
<li>src/slab/default/mpicufft_slab.cpp</li>
</ul>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.2
</small></address>
</body>
</html>
