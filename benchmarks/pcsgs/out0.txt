start building
-- The CUDA compiler identification is NVIDIA 11.2.152
-- The CXX compiler identification is GNU 9.2.0
-- Detecting CUDA compiler ABI info
-- Detecting CUDA compiler ABI info - done
-- Check for working CUDA compiler: /usr/local.nfs/sw/cuda/cuda-11.2.2/bin/nvcc - skipped
-- Detecting CUDA compile features
-- Detecting CUDA compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/local.nfs/sw/gcc/gcc-9.2.0/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found MPI_CXX: /home/eggersn/opt/openmpi-4.1.1/lib/libmpi.so (found version "3.1") 
-- Found MPI: TRUE (found version "3.1")  
-- Found CUDAToolkit: /usr/local.nfs/sw/cuda/cuda-11.2.2/include (found version "11.2.152") 
-- Looking for C++ include pthread.h
-- Looking for C++ include pthread.h - found
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed
-- Looking for pthread_create in pthreads
-- Looking for pthread_create in pthreads - not found
-- Looking for pthread_create in pthread
-- Looking for pthread_create in pthread - found
-- Found Threads: TRUE  
-- Configuring done
-- Generating done
-- Build files have been written to: /home/eggersn/DistributedFFT/build_pcsgs
Scanning dependencies of target timer
[  3%] Building CXX object CMakeFiles/timer.dir/src/timer.cpp.o
[  6%] Linking CXX shared library libtimer.so
[  6%] Built target timer
Scanning dependencies of target mpicufft
[  9%] Building CXX object CMakeFiles/mpicufft.dir/src/mpicufft.cpp.o
[ 12%] Linking CXX shared library libmpicufft.so
[ 12%] Built target mpicufft
Scanning dependencies of target pencil_decomp
[ 15%] Building CXX object CMakeFiles/pencil_decomp.dir/src/pencil/mpicufft_pencil.cpp.o
[ 18%] Building CXX object CMakeFiles/pencil_decomp.dir/src/pencil/mpicufft_pencil_opt1.cpp.o
[ 21%] Linking CXX shared library libpencil_decomp.so
[ 21%] Built target pencil_decomp
Scanning dependencies of target test_base
[ 24%] Building CUDA object CMakeFiles/test_base.dir/tests/src/base.cu.o
nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 27%] Linking CUDA shared library libtest_base.so
[ 27%] Built target test_base
Scanning dependencies of target reference_tests
[ 30%] Building CUDA object CMakeFiles/reference_tests.dir/tests/src/reference/reference.cu.o
nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 33%] Linking CUDA shared library libreference_tests.so
[ 33%] Built target reference_tests
Scanning dependencies of target reference
[ 36%] Building CXX object CMakeFiles/reference.dir/tests/src/reference/main.cpp.o
[ 39%] Linking CXX executable reference
[ 39%] Built target reference
Scanning dependencies of target slab_decomp
[ 42%] Building CXX object CMakeFiles/slab_decomp.dir/src/slab/default/mpicufft_slab.cpp.o
[ 45%] Building CXX object CMakeFiles/slab_decomp.dir/src/slab/default/mpicufft_slab_opt1.cpp.o
[ 48%] Building CXX object CMakeFiles/slab_decomp.dir/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp.o
[ 51%] Building CXX object CMakeFiles/slab_decomp.dir/src/slab/z_then_yx/mpicufft_slab_z_then_yx_opt1.cpp.o
[ 54%] Building CXX object CMakeFiles/slab_decomp.dir/src/slab/y_then_zx/mpicufft_slab_y_then_zx.cpp.o
[ 57%] Linking CXX shared library libslab_decomp.so
[ 57%] Built target slab_decomp
Scanning dependencies of target slab_tests
[ 60%] Building CUDA object CMakeFiles/slab_tests.dir/tests/src/slab/base.cu.o
nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 63%] Building CUDA object CMakeFiles/slab_tests.dir/tests/src/slab/random_dist_default.cu.o
nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 66%] Building CUDA object CMakeFiles/slab_tests.dir/tests/src/slab/random_dist_y_then_zx.cu.o
nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 69%] Building CUDA object CMakeFiles/slab_tests.dir/tests/src/slab/random_dist_z_then_yx.cu.o
nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 72%] Linking CUDA shared library libslab_tests.so
[ 72%] Built target slab_tests
Scanning dependencies of target pencil_tests
[ 75%] Building CUDA object CMakeFiles/pencil_tests.dir/tests/src/pencil/base.cu.o
nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 78%] Building CUDA object CMakeFiles/pencil_tests.dir/tests/src/pencil/random_dist_1D.cu.o
nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 81%] Building CUDA object CMakeFiles/pencil_tests.dir/tests/src/pencil/random_dist_2D.cu.o
nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 84%] Building CUDA object CMakeFiles/pencil_tests.dir/tests/src/pencil/random_dist_3D.cu.o
nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 87%] Linking CUDA shared library libpencil_tests.so
[ 87%] Built target pencil_tests
Scanning dependencies of target pencil
[ 90%] Building CXX object CMakeFiles/pencil.dir/tests/src/pencil/main.cpp.o
[ 93%] Linking CXX executable pencil
[ 93%] Built target pencil
Scanning dependencies of target slab
[ 96%] Building CXX object CMakeFiles/slab.dir/tests/src/slab/main.cpp.o
[100%] Linking CXX executable slab
[100%] Built target slab
finished building
start python script
-----------------------------------------------------------------------------
Slab 2D->1D default
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
Starting computation for size 128
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 128 -ny 128 -nz 128
2021-08-26 08:21:29.857675
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 128 -ny 128 -nz 128
2021-08-26 08:21:36.861179
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 128 -ny 128 -nz 128
2021-08-26 08:21:39.451460
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 128 -ny 128 -nz 128
2021-08-26 08:21:42.475095
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 128 -ny 128 -nz 128
2021-08-26 08:21:45.379679
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 128 -ny 128 -nz 128
2021-08-26 08:21:48.030898
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 128 -ny 128 -nz 128
2021-08-26 08:21:50.706863
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 128 -ny 128 -nz 128
2021-08-26 08:21:53.315919
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 128 -ny 128 -nz 128
2021-08-26 08:21:56.011538
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 128 -ny 128 -nz 128
2021-08-26 08:21:58.619001
b''

Starting computation for size [128, 128, 256]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 128 -ny 128 -nz 256
2021-08-26 08:22:01.239662
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 128 -ny 128 -nz 256
2021-08-26 08:22:04.775705
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 128 -ny 128 -nz 256
2021-08-26 08:22:08.151476
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 128 -ny 128 -nz 256
2021-08-26 08:22:12.172258
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 128 -ny 128 -nz 256
2021-08-26 08:22:16.212723
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 128 -ny 128 -nz 256
2021-08-26 08:22:19.776985
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 128 -ny 128 -nz 256
2021-08-26 08:22:23.215827
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 128 -ny 128 -nz 256
2021-08-26 08:22:26.683762
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 128 -ny 128 -nz 256
2021-08-26 08:22:30.151971
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 128 -ny 128 -nz 256
2021-08-26 08:22:33.757814
b''

Starting computation for size [128, 256, 256]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 128 -ny 256 -nz 256
2021-08-26 08:22:37.395683
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 128 -ny 256 -nz 256
2021-08-26 08:22:42.615559
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 128 -ny 256 -nz 256
2021-08-26 08:22:47.839736
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 128 -ny 256 -nz 256
2021-08-26 08:22:53.815690
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 128 -ny 256 -nz 256
2021-08-26 08:23:00.028076
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 128 -ny 256 -nz 256
2021-08-26 08:23:05.491753
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 128 -ny 256 -nz 256
2021-08-26 08:23:10.739634
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 128 -ny 256 -nz 256
2021-08-26 08:23:16.116403
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 128 -ny 256 -nz 256
2021-08-26 08:23:21.487666
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 128 -ny 256 -nz 256
2021-08-26 08:23:26.719777
b''

Starting computation for size 256
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 256 -ny 256 -nz 256
2021-08-26 08:23:32.135793
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 256 -ny 256 -nz 256
2021-08-26 08:23:40.879376
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 256 -ny 256 -nz 256
2021-08-26 08:23:49.607822
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 256 -ny 256 -nz 256
2021-08-26 08:23:58.487517
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 256 -ny 256 -nz 256
2021-08-26 08:24:08.095768
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 256 -ny 256 -nz 256
2021-08-26 08:24:16.867777
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 256 -ny 256 -nz 256
2021-08-26 08:24:25.411524
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 256 -ny 256 -nz 256
2021-08-26 08:24:34.515667
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 256 -ny 256 -nz 256
2021-08-26 08:24:43.863778
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 256 -ny 256 -nz 256
2021-08-26 08:24:52.783794
b''

Starting computation for size [256, 256, 512]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 256 -ny 256 -nz 512
2021-08-26 08:25:01.721116
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 256 -ny 256 -nz 512
2021-08-26 08:25:17.927679
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 256 -ny 256 -nz 512
2021-08-26 08:25:34.107803
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 256 -ny 256 -nz 512
2021-08-26 08:25:51.223654
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 256 -ny 256 -nz 512
2021-08-26 08:26:07.315637
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 256 -ny 256 -nz 512
2021-08-26 08:26:23.059422
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 256 -ny 256 -nz 512
2021-08-26 08:26:38.551775
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 256 -ny 256 -nz 512
2021-08-26 08:26:54.635602
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 256 -ny 256 -nz 512
2021-08-26 08:27:09.847485
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 256 -ny 256 -nz 512
2021-08-26 08:27:25.539661
b''

Starting computation for size [256, 512, 512]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 256 -ny 512 -nz 512
2021-08-26 08:27:49.024858
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 256 -ny 512 -nz 512
2021-08-26 08:28:38.914954
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 256 -ny 512 -nz 512
2021-08-26 08:29:09.660306
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 256 -ny 512 -nz 512
2021-08-26 08:29:39.499608
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 256 -ny 512 -nz 512
2021-08-26 08:30:09.423875
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 256 -ny 512 -nz 512
2021-08-26 08:30:48.787140
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 256 -ny 512 -nz 512
2021-08-26 08:31:36.175624
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 256 -ny 512 -nz 512
2021-08-26 08:32:06.727940
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 256 -ny 512 -nz 512
2021-08-26 08:32:36.567554
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 256 -ny 512 -nz 512
2021-08-26 08:33:06.607221
b''

Starting computation for size 512
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 512 -ny 512 -nz 512
2021-08-26 08:33:36.947711
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 512 -ny 512 -nz 512
2021-08-26 08:34:35.895087
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 512 -ny 512 -nz 512
2021-08-26 08:35:31.197031
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 512 -ny 512 -nz 512
2021-08-26 08:36:28.739521
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 512 -ny 512 -nz 512
2021-08-26 08:37:26.444628
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 512 -ny 512 -nz 512
2021-08-26 08:38:23.983522
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 512 -ny 512 -nz 512
2021-08-26 08:39:19.579648
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 512 -ny 512 -nz 512
2021-08-26 08:40:18.715739
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 512 -ny 512 -nz 512
2021-08-26 08:41:14.571710
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 512 -ny 512 -nz 512
2021-08-26 08:42:13.619467
b''

Starting computation for size [512, 512, 1024]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 512 -ny 512 -nz 1024
2021-08-26 08:43:11.351796
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 512 -ny 512 -nz 1024
2021-08-26 08:45:18.331324
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 512 -ny 512 -nz 1024
2021-08-26 08:47:21.855868
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 512 -ny 512 -nz 1024
2021-08-26 08:49:48.639674
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 512 -ny 512 -nz 1024
2021-08-26 08:51:39.435487
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 512 -ny 512 -nz 1024
2021-08-26 08:53:31.479499
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 512 -ny 512 -nz 1024
2021-08-26 08:55:22.939544
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 512 -ny 512 -nz 1024
2021-08-26 08:57:18.167702
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 512 -ny 512 -nz 1024
2021-08-26 08:59:09.761058
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 512 -ny 512 -nz 1024
2021-08-26 09:01:04.227867
b''

Starting computation for size [512, 1024, 1024]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 512 -ny 1024 -nz 1024
2021-08-26 09:03:24.168184
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 512 -ny 1024 -nz 1024
2021-08-26 09:07:34.427533
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 512 -ny 1024 -nz 1024
2021-08-26 09:11:17.383260
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 512 -ny 1024 -nz 1024
2021-08-26 09:15:26.099207
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 512 -ny 1024 -nz 1024
2021-08-26 09:19:21.563634
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 512 -ny 1024 -nz 1024
2021-08-26 09:23:43.660019
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 512 -ny 1024 -nz 1024
2021-08-26 09:27:22.771409
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 512 -ny 1024 -nz 1024
2021-08-26 09:31:05.779551
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 512 -ny 1024 -nz 1024
2021-08-26 09:34:47.367631
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -nx 512 -ny 1024 -nz 1024
2021-08-26 09:39:09.149997
b''

Starting computation for size 128
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-08-26 09:43:09.291440
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-08-26 09:43:15.323394
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-08-26 09:43:17.491391
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-08-26 09:43:19.683736
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-08-26 09:43:21.831595
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-08-26 09:43:24.104547
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-08-26 09:43:26.409062
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-08-26 09:43:28.684304
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-08-26 09:43:30.915774
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-08-26 09:43:33.120715
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

Starting computation for size 256
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-08-26 09:43:35.277512
b'Result (avg): 5.01735e-09\nResult (max): 5.42241e-08\n'

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-08-26 09:43:38.344640
b'Result (avg): 5.01735e-09\nResult (max): 5.42241e-08\n'

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-08-26 09:43:41.368831
b'Result (avg): 5.01735e-09\nResult (max): 5.42241e-08\n'

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-08-26 09:43:44.648630
b'Result (avg): 5.01735e-09\nResult (max): 5.42241e-08\n'

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-08-26 09:43:47.604100
b'Result (avg): 5.01735e-09\nResult (max): 5.42241e-08\n'

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-08-26 09:43:50.859792
b'Result (avg): 5.01735e-09\nResult (max): 5.42241e-08\n'

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-08-26 09:43:53.935732
b'Result (avg): 5.01735e-09\nResult (max): 5.42241e-08\n'

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-08-26 09:43:57.295515
b'Result (avg): 5.01735e-09\nResult (max): 5.42241e-08\n'

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-08-26 09:44:00.380649
b'Result (avg): 5.01735e-09\nResult (max): 5.42241e-08\n'

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-08-26 09:44:03.499656
b'Result (avg): 5.01735e-09\nResult (max): 5.42241e-08\n'

Starting computation for size 512
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-08-26 09:44:06.536214
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-08-26 09:44:16.507215
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-08-26 09:44:26.540962
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-08-26 09:44:37.695747
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-08-26 09:44:47.512114
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-08-26 09:44:57.660890
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-08-26 09:45:07.288188
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-08-26 09:45:16.991342
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-08-26 09:45:26.975402
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-08-26 09:45:37.827304
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

Starting computation for size [512, 1024, 1024]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -t 4 -nx 512 -ny 1024 -nz 1024 --testcase 4
2021-08-26 09:45:47.676473
b'Result (avg): 0.000306937\nResult (max): 0.00119137\n'

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -t 4 -nx 512 -ny 1024 -nz 1024 --testcase 4
2021-08-26 09:46:21.079805
b'Result (avg): 0.000306937\nResult (max): 0.00119137\n'

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -t 4 -nx 512 -ny 1024 -nz 1024 --testcase 4
2021-08-26 09:46:53.444100
b'Result (avg): 0.000306937\nResult (max): 0.00119137\n'

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -t 4 -nx 512 -ny 1024 -nz 1024 --testcase 4
2021-08-26 09:47:34.675575
b'Result (avg): 0.000306937\nResult (max): 0.00119137\n'

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -t 4 -nx 512 -ny 1024 -nz 1024 --testcase 4
2021-08-26 09:48:07.695643
b'Result (avg): 0.000306937\nResult (max): 0.00119137\n'

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -t 4 -nx 512 -ny 1024 -nz 1024 --testcase 4
2021-08-26 09:48:40.852930
b'Result (avg): 3.80882e+09\nResult (max): 8.21198e+11\n'

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -t 4 -nx 512 -ny 1024 -nz 1024 --testcase 4
2021-08-26 09:49:13.823691
b'Result (avg): 0.000306937\nResult (max): 0.00119137\n'

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -t 4 -nx 512 -ny 1024 -nz 1024 --testcase 4
2021-08-26 09:49:47.255779
b'Result (avg): 0.000306937\nResult (max): 0.00119137\n'

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -t 4 -nx 512 -ny 1024 -nz 1024 --testcase 4
2021-08-26 09:50:20.275479
b'Result (avg): 0.000306937\nResult (max): 0.00119137\n'

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -t 4 -nx 512 -ny 1024 -nz 1024 --testcase 4
2021-08-26 09:50:53.383140
b'Result (avg): 0.000306937\nResult (max): 0.00119137\n'

Slab 2D->1D opt1
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
Starting computation for size 128
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 128 -ny 128 -nz 128
2021-08-26 09:51:26.971598
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 128 -ny 128 -nz 128
2021-08-26 09:51:29.551752
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 128 -ny 128 -nz 128
2021-08-26 09:51:32.235678
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 128 -ny 128 -nz 128
2021-08-26 09:51:35.273325
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 128 -ny 128 -nz 128
2021-08-26 09:51:38.041259
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 128 -ny 128 -nz 128
2021-08-26 09:51:40.741230
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 128 -ny 128 -nz 128
2021-08-26 09:51:44.019980
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 128 -ny 128 -nz 128
2021-08-26 09:51:46.696797
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 128 -ny 128 -nz 128
2021-08-26 09:51:49.516577
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 128 -ny 128 -nz 128
2021-08-26 09:51:52.164106
b''

Starting computation for size [128, 128, 256]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 128 -ny 128 -nz 256
2021-08-26 09:51:55.979805
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 128 -ny 128 -nz 256
2021-08-26 09:51:59.655401
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 128 -ny 128 -nz 256
2021-08-26 09:52:03.200112
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 128 -ny 128 -nz 256
2021-08-26 09:52:07.631365
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 128 -ny 128 -nz 256
2021-08-26 09:52:11.147610
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 128 -ny 128 -nz 256
2021-08-26 09:52:14.587481
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 128 -ny 128 -nz 256
2021-08-26 09:52:19.699656
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 128 -ny 128 -nz 256
2021-08-26 09:52:23.220485
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 128 -ny 128 -nz 256
2021-08-26 09:52:32.876888
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 128 -ny 128 -nz 256
2021-08-26 09:52:40.923787
b''

Starting computation for size [128, 256, 256]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 128 -ny 256 -nz 256
2021-08-26 09:52:48.867855
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 128 -ny 256 -nz 256
2021-08-26 09:53:01.671725
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 128 -ny 256 -nz 256
2021-08-26 09:53:13.308667
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 128 -ny 256 -nz 256
2021-08-26 09:53:19.519868
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 128 -ny 256 -nz 256
2021-08-26 09:53:24.852639
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 128 -ny 256 -nz 256
2021-08-26 09:53:30.159544
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 128 -ny 256 -nz 256
2021-08-26 09:53:35.564538
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 128 -ny 256 -nz 256
2021-08-26 09:53:41.191705
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 128 -ny 256 -nz 256
2021-08-26 09:53:46.440244
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 128 -ny 256 -nz 256
2021-08-26 09:53:51.876455
b''

Starting computation for size 256
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 256 -ny 256 -nz 256
2021-08-26 09:53:58.079743
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 256 -ny 256 -nz 256
2021-08-26 09:54:06.711776
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 256 -ny 256 -nz 256
2021-08-26 09:54:15.643812
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 256 -ny 256 -nz 256
2021-08-26 09:54:24.539649
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 256 -ny 256 -nz 256
2021-08-26 09:54:33.103497
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 256 -ny 256 -nz 256
2021-08-26 09:54:41.607743
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 256 -ny 256 -nz 256
2021-08-26 09:54:50.155508
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 256 -ny 256 -nz 256
2021-08-26 09:54:59.048063
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 256 -ny 256 -nz 256
2021-08-26 09:55:07.697015
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 256 -ny 256 -nz 256
2021-08-26 09:55:17.083852
b''

Starting computation for size [256, 256, 512]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 256 -ny 256 -nz 512
2021-08-26 09:55:35.052625
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 256 -ny 256 -nz 512
2021-08-26 09:55:57.779302
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 256 -ny 256 -nz 512
2021-08-26 09:56:26.278051
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 256 -ny 256 -nz 512
2021-08-26 09:56:48.472314
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 256 -ny 256 -nz 512
2021-08-26 09:57:05.571819
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 256 -ny 256 -nz 512
2021-08-26 09:57:21.132624
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 256 -ny 256 -nz 512
2021-08-26 09:57:36.631643
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 256 -ny 256 -nz 512
2021-08-26 09:57:52.571427
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 256 -ny 256 -nz 512
2021-08-26 09:58:08.351635
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 256 -ny 256 -nz 512
2021-08-26 09:58:25.323786
b''

Starting computation for size [256, 512, 512]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 256 -ny 512 -nz 512
2021-08-26 09:58:42.987805
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 256 -ny 512 -nz 512
2021-08-26 09:59:12.027901
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 256 -ny 512 -nz 512
2021-08-26 09:59:40.387706
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 256 -ny 512 -nz 512
2021-08-26 10:00:09.581509
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 256 -ny 512 -nz 512
2021-08-26 10:00:40.191586
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 256 -ny 512 -nz 512
2021-08-26 10:01:10.280585
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 256 -ny 512 -nz 512
2021-08-26 10:01:39.764483
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 256 -ny 512 -nz 512
2021-08-26 10:02:11.244388
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 256 -ny 512 -nz 512
2021-08-26 10:02:41.141115
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 256 -ny 512 -nz 512
2021-08-26 10:03:15.232285
b''

Starting computation for size 512
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 512 -ny 512 -nz 512
2021-08-26 10:03:50.299524
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 512 -ny 512 -nz 512
2021-08-26 10:04:48.851793
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 512 -ny 512 -nz 512
2021-08-26 10:05:47.116444
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 512 -ny 512 -nz 512
2021-08-26 10:06:42.740647
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 512 -ny 512 -nz 512
2021-08-26 10:07:39.039662
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 512 -ny 512 -nz 512
2021-08-26 10:08:44.507358
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 512 -ny 512 -nz 512
2021-08-26 10:10:01.091130
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 512 -ny 512 -nz 512
2021-08-26 10:10:58.571403
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 512 -ny 512 -nz 512
2021-08-26 10:11:53.875139
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 512 -ny 512 -nz 512
2021-08-26 10:12:55.459383
b''

Starting computation for size [512, 512, 1024]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 512 -ny 512 -nz 1024
2021-08-26 10:13:59.307937
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 512 -ny 512 -nz 1024
2021-08-26 10:15:49.604050
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 512 -ny 512 -nz 1024
2021-08-26 10:17:42.407926
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 512 -ny 512 -nz 1024
2021-08-26 10:19:54.451794
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 512 -ny 512 -nz 1024
2021-08-26 10:21:44.788300
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 512 -ny 512 -nz 1024
2021-08-26 10:23:41.008561
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 512 -ny 512 -nz 1024
2021-08-26 10:25:40.623679
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 512 -ny 512 -nz 1024
2021-08-26 10:27:56.079147
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 512 -ny 512 -nz 1024
2021-08-26 10:29:51.775510
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 512 -ny 512 -nz 1024
2021-08-26 10:31:45.719399
b''

Starting computation for size [512, 1024, 1024]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 512 -ny 1024 -nz 1024
2021-08-26 10:33:48.819349
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 512 -ny 1024 -nz 1024
2021-08-26 10:37:31.351267
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 512 -ny 1024 -nz 1024
2021-08-26 10:41:16.531349
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 512 -ny 1024 -nz 1024
2021-08-26 10:46:16.147768
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 512 -ny 1024 -nz 1024
2021-08-26 10:49:57.139164
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 512 -ny 1024 -nz 1024
2021-08-26 10:53:39.083431
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 512 -ny 1024 -nz 1024
2021-08-26 10:57:30.754927
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 512 -ny 1024 -nz 1024
2021-08-26 11:01:56.891796
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 512 -ny 1024 -nz 1024
2021-08-26 11:05:45.251763
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -nx 512 -ny 1024 -nz 1024
2021-08-26 11:09:33.903288
b''

Starting computation for size 128
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-08-26 11:15:22.793128
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-08-26 11:15:25.791010
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-08-26 11:15:27.975018
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-08-26 11:15:30.075241
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-08-26 11:15:32.155094
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-08-26 11:15:34.247118
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-08-26 11:15:36.495059
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-08-26 11:15:38.684012
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-08-26 11:15:40.807064
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-08-26 11:15:43.051225
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

Starting computation for size 256
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-08-26 11:15:51.034756
b'Result (avg): 5.19278e-09\nResult (max): 5.37366e-08\n'

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-08-26 11:15:55.423751
b'Result (avg): 5.19278e-09\nResult (max): 5.37366e-08\n'

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-08-26 11:15:58.823209
b'Result (avg): 5.19278e-09\nResult (max): 5.37366e-08\n'

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-08-26 11:16:02.047257
b'Result (avg): 5.19278e-09\nResult (max): 5.37366e-08\n'

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-08-26 11:16:09.871157
b'Result (avg): 5.19278e-09\nResult (max): 5.37366e-08\n'

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-08-26 11:16:12.883251
b'Result (avg): 5.19278e-09\nResult (max): 5.37366e-08\n'

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-08-26 11:16:15.847084
b'Result (avg): 5.19278e-09\nResult (max): 5.37366e-08\n'

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-08-26 11:16:19.047069
b'Result (avg): 5.19278e-09\nResult (max): 5.37366e-08\n'

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-08-26 11:16:21.975166
b'Result (avg): 5.19278e-09\nResult (max): 5.37366e-08\n'

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-08-26 11:16:24.971212
b'Result (avg): 5.19278e-09\nResult (max): 5.37366e-08\n'

Starting computation for size 512
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-08-26 11:16:28.236249
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-08-26 11:16:40.991005
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-08-26 11:16:50.879154
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-08-26 11:17:02.655397
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-08-26 11:17:12.640308
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-08-26 11:17:23.651111
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-08-26 11:17:33.383396
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-08-26 11:17:43.587315
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-08-26 11:17:54.667537
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-08-26 11:18:05.303246
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

Starting computation for size [512, 1024, 1024]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -t 4 -nx 512 -ny 1024 -nz 1024 --testcase 4
2021-08-26 11:18:15.527244
b'Result (avg): 0.000306937\nResult (max): 0.00119153\n'

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -t 4 -nx 512 -ny 1024 -nz 1024 --testcase 4
2021-08-26 11:18:56.259343
b'Result (avg): 0.000306937\nResult (max): 0.00119153\n'

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -t 4 -nx 512 -ny 1024 -nz 1024 --testcase 4
2021-08-26 11:19:32.519233
b'Result (avg): 0.000306937\nResult (max): 0.00119153\n'

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -t 4 -nx 512 -ny 1024 -nz 1024 --testcase 4
2021-08-26 11:20:13.587257
b'Result (avg): 0.000306937\nResult (max): 0.00119153\n'

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -t 4 -nx 512 -ny 1024 -nz 1024 --testcase 4
2021-08-26 11:20:48.467096
b'Result (avg): 0.000306937\nResult (max): 0.00119153\n'

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -t 4 -nx 512 -ny 1024 -nz 1024 --testcase 4
2021-08-26 11:21:23.135277
b'Result (avg): 0.000306937\nResult (max): 0.00119153\n'

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -t 4 -nx 512 -ny 1024 -nz 1024 --testcase 4
2021-08-26 11:21:56.983137
b'Result (avg): 0.000306937\nResult (max): 0.00119153\n'

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -t 4 -nx 512 -ny 1024 -nz 1024 --testcase 4
2021-08-26 11:22:32.916614
b'Result (avg): 0.000306937\nResult (max): 0.00119153\n'

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -t 4 -nx 512 -ny 1024 -nz 1024 --testcase 4
2021-08-26 11:23:07.047254
b'Result (avg): 0.000306937\nResult (max): 0.00119153\n'

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward --opt 1 -t 4 -nx 512 -ny 1024 -nz 1024 --testcase 4
2021-08-26 11:23:43.275298
b'Result (avg): 0.000306937\nResult (max): 0.00119153\n'

Slab 1D->2D default
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
Starting computation for size 128
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 128 -ny 128 -nz 128
2021-08-26 11:24:20.139482
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 128 -ny 128 -nz 128
2021-08-26 11:24:22.803187
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 128 -ny 128 -nz 128
2021-08-26 11:24:25.255440
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 128 -ny 128 -nz 128
2021-08-26 11:24:29.372811
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 128 -ny 128 -nz 128
2021-08-26 11:24:33.082176
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 128 -ny 128 -nz 128
2021-08-26 11:24:35.790270
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 128 -ny 128 -nz 128
2021-08-26 11:24:39.371127
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 128 -ny 128 -nz 128
2021-08-26 11:24:41.927041
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 128 -ny 128 -nz 128
2021-08-26 11:24:44.375117
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 128 -ny 128 -nz 128
2021-08-26 11:24:46.987200
b''

Starting computation for size [128, 128, 256]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 128 -ny 128 -nz 256
2021-08-26 11:24:51.159400
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 128 -ny 128 -nz 256
2021-08-26 11:24:54.591025
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 128 -ny 128 -nz 256
2021-08-26 11:24:57.979152
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 128 -ny 128 -nz 256
2021-08-26 11:25:02.007971
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 128 -ny 128 -nz 256
2021-08-26 11:25:06.027250
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 128 -ny 128 -nz 256
2021-08-26 11:25:10.090599
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 128 -ny 128 -nz 256
2021-08-26 11:25:15.124837
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 128 -ny 128 -nz 256
2021-08-26 11:25:18.691118
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 128 -ny 128 -nz 256
2021-08-26 11:25:22.035511
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 128 -ny 128 -nz 256
2021-08-26 11:25:25.419217
b''

Starting computation for size [128, 256, 256]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 128 -ny 256 -nz 256
2021-08-26 11:25:30.039394
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 128 -ny 256 -nz 256
2021-08-26 11:25:35.275203
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 128 -ny 256 -nz 256
2021-08-26 11:25:40.399156
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 128 -ny 256 -nz 256
2021-08-26 11:25:46.679060
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 128 -ny 256 -nz 256
2021-08-26 11:25:53.579316
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 128 -ny 256 -nz 256
2021-08-26 11:25:59.183259
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 128 -ny 256 -nz 256
2021-08-26 11:26:05.680841
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 128 -ny 256 -nz 256
2021-08-26 11:26:11.119053
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 128 -ny 256 -nz 256
2021-08-26 11:26:16.359271
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 128 -ny 256 -nz 256
2021-08-26 11:26:21.787239
b''

Starting computation for size 256
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 256 -ny 256 -nz 256
2021-08-26 11:26:29.914527
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 256 -ny 256 -nz 256
2021-08-26 11:26:39.911107
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 256 -ny 256 -nz 256
2021-08-26 11:26:48.536983
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 256 -ny 256 -nz 256
2021-08-26 11:26:57.311255
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 256 -ny 256 -nz 256
2021-08-26 11:27:06.867330
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 256 -ny 256 -nz 256
2021-08-26 11:27:15.890412
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 256 -ny 256 -nz 256
2021-08-26 11:27:28.347287
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 256 -ny 256 -nz 256
2021-08-26 11:27:37.347055
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 256 -ny 256 -nz 256
2021-08-26 11:27:46.047170
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 256 -ny 256 -nz 256
2021-08-26 11:27:55.230600
b''

Starting computation for size [256, 256, 512]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 256 -ny 256 -nz 512
2021-08-26 11:28:10.759260
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 256 -ny 256 -nz 512
2021-08-26 11:28:26.499166
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 256 -ny 256 -nz 512
2021-08-26 11:28:41.602994
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 256 -ny 256 -nz 512
2021-08-26 11:28:58.135186
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 256 -ny 256 -nz 512
2021-08-26 11:29:14.768203
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 256 -ny 256 -nz 512
2021-08-26 11:29:30.147469
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 256 -ny 256 -nz 512
2021-08-26 11:29:46.431257
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 256 -ny 256 -nz 512
2021-08-26 11:30:02.479303
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 256 -ny 256 -nz 512
2021-08-26 11:30:17.815178
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 256 -ny 256 -nz 512
2021-08-26 11:30:34.194669
b''

Starting computation for size [256, 512, 512]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 256 -ny 512 -nz 512
2021-08-26 11:30:54.119134
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 256 -ny 512 -nz 512
2021-08-26 11:31:24.739011
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 256 -ny 512 -nz 512
2021-08-26 11:32:01.731286
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 256 -ny 512 -nz 512
2021-08-26 11:32:52.547355
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 256 -ny 512 -nz 512
2021-08-26 11:33:23.679223
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 256 -ny 512 -nz 512
2021-08-26 11:33:56.645576
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 256 -ny 512 -nz 512
2021-08-26 11:34:36.782721
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 256 -ny 512 -nz 512
2021-08-26 11:35:27.533088
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 256 -ny 512 -nz 512
2021-08-26 11:35:58.459502
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 256 -ny 512 -nz 512
2021-08-26 11:36:30.371287
b''

Starting computation for size 512
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 512 -ny 512 -nz 512
2021-08-26 11:37:06.907262
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 512 -ny 512 -nz 512
2021-08-26 11:38:03.572275
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 512 -ny 512 -nz 512
2021-08-26 11:38:59.831584
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 512 -ny 512 -nz 512
2021-08-26 11:40:13.571184
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 512 -ny 512 -nz 512
2021-08-26 11:41:12.503386
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 512 -ny 512 -nz 512
2021-08-26 11:42:11.871198
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 512 -ny 512 -nz 512
2021-08-26 11:43:10.787017
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 512 -ny 512 -nz 512
2021-08-26 11:44:10.745590
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 512 -ny 512 -nz 512
2021-08-26 11:45:08.518898
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 512 -ny 512 -nz 512
2021-08-26 11:46:07.599242
b''

Starting computation for size [512, 512, 1024]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 512 -ny 512 -nz 1024
2021-08-26 11:47:21.447066
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 512 -ny 512 -nz 1024
2021-08-26 11:49:45.691062
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 512 -ny 512 -nz 1024
2021-08-26 11:51:46.704174
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 512 -ny 512 -nz 1024
2021-08-26 11:54:24.559481
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 512 -ny 512 -nz 1024
2021-08-26 11:56:16.296185
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 512 -ny 512 -nz 1024
2021-08-26 11:58:13.323100
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 512 -ny 512 -nz 1024
2021-08-26 12:00:12.263246
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 512 -ny 512 -nz 1024
2021-08-26 12:02:09.152230
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 512 -ny 512 -nz 1024
2021-08-26 12:04:06.212950
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 512 -ny 512 -nz 1024
2021-08-26 12:06:13.227247
b''

Starting computation for size [512, 1024, 1024]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 512 -ny 1024 -nz 1024
2021-08-26 12:08:43.798998
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 512 -ny 1024 -nz 1024
2021-08-26 12:12:50.383048
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 512 -ny 1024 -nz 1024
2021-08-26 12:16:34.464132
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 512 -ny 1024 -nz 1024
2021-08-26 12:21:30.750802
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 512 -ny 1024 -nz 1024
2021-08-26 12:25:42.259502
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 512 -ny 1024 -nz 1024
2021-08-26 12:29:30.803276
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 512 -ny 1024 -nz 1024
2021-08-26 12:33:24.563326
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 512 -ny 1024 -nz 1024
2021-08-26 12:37:13.311279
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 512 -ny 1024 -nz 1024
2021-08-26 12:41:20.733836
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -nx 512 -ny 1024 -nz 1024
2021-08-26 12:45:12.255908
b''

Starting computation for size 128
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-08-26 12:49:30.997991
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-08-26 12:49:33.658970
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-08-26 12:49:37.693463
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-08-26 12:49:40.910653
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-08-26 12:49:43.155482
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-08-26 12:49:45.435363
b'Result (avg): 1.91723e-05\nResult (max): 7.43547e-05\n'

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-08-26 12:49:47.771364
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-08-26 12:49:50.099225
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-08-26 12:49:54.081234
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-08-26 12:49:56.979246
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

Starting computation for size 256
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-08-26 12:49:59.792490
b'Result (avg): 5.01733e-09\nResult (max): 5.42241e-08\n'

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-08-26 12:50:03.016392
b'Result (avg): 5.01733e-09\nResult (max): 5.42241e-08\n'

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-08-26 12:50:06.231767
b'Result (avg): 5.01733e-09\nResult (max): 5.42241e-08\n'

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-08-26 12:50:10.646730
b'Result (avg): 5.01733e-09\nResult (max): 5.42241e-08\n'

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-08-26 12:50:15.079862
b'Result (avg): 5.01733e-09\nResult (max): 5.42241e-08\n'

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-08-26 12:50:18.527177
b'Result (avg): 188400\nResult (max): 2.16212e+07\n'

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-08-26 12:50:21.820078
b'Result (avg): 5.01733e-09\nResult (max): 5.42241e-08\n'

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-08-26 12:50:24.955611
b'Result (avg): 5.01733e-09\nResult (max): 5.42241e-08\n'

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-08-26 12:50:30.028434
b'Result (avg): 5.01733e-09\nResult (max): 5.42241e-08\n'

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-08-26 12:50:33.907238
b'Result (avg): 5.01733e-09\nResult (max): 5.42241e-08\n'

Starting computation for size 512
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-08-26 12:50:37.479524
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-08-26 12:50:48.110078
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-08-26 12:50:59.020622
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-08-26 12:51:12.807711
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-08-26 12:51:23.123140
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-08-26 12:51:34.380685
b'Result (avg): 1.08658e+06\nResult (max): 1.27469e+08\n'

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-08-26 12:51:49.928417
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-08-26 12:52:04.378027
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-08-26 12:52:15.792711
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-08-26 12:52:27.019285
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

Starting computation for size [512, 1024, 1024]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -t 4 -nx 512 -ny 1024 -nz 1024 --testcase 4
2021-08-26 12:52:39.911898
b'Result (avg): 0.000306937\nResult (max): 0.00119165\n'

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -t 4 -nx 512 -ny 1024 -nz 1024 --testcase 4
2021-08-26 12:53:17.499516
b'Result (avg): 0.000306937\nResult (max): 0.00119165\n'

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -t 4 -nx 512 -ny 1024 -nz 1024 --testcase 4
2021-08-26 12:53:54.747351
b'Result (avg): 0.000306937\nResult (max): 0.00119165\n'

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -t 4 -nx 512 -ny 1024 -nz 1024 --testcase 4
2021-08-26 12:54:40.119232
b'Result (avg): 0.000306937\nResult (max): 0.00119165\n'

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -t 4 -nx 512 -ny 1024 -nz 1024 --testcase 4
2021-08-26 12:55:17.355730
b'Result (avg): 0.000306937\nResult (max): 0.00119165\n'

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -t 4 -nx 512 -ny 1024 -nz 1024 --testcase 4
2021-08-26 12:55:55.531283
b'Result (avg): 2.18319e+06\nResult (max): 1.26238e+09\n'

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -t 4 -nx 512 -ny 1024 -nz 1024 --testcase 4
2021-08-26 12:56:37.822195
b'Result (avg): 0.000306937\nResult (max): 0.00119165\n'

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -t 4 -nx 512 -ny 1024 -nz 1024 --testcase 4
2021-08-26 12:57:22.143006
b'Result (avg): 0.000306937\nResult (max): 0.00119165\n'

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -t 4 -nx 512 -ny 1024 -nz 1024 --testcase 4
2021-08-26 12:57:59.207643
b'Result (avg): 0.000306937\nResult (max): 0.00119165\n'

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX -t 4 -nx 512 -ny 1024 -nz 1024 --testcase 4
2021-08-26 12:58:36.632068
b'Result (avg): 0.000306937\nResult (max): 0.00119165\n'

Slab 1D->2D opt1
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
Starting computation for size 128
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128
2021-08-26 12:59:14.429426
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128
2021-08-26 12:59:17.235663
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128
2021-08-26 12:59:19.943125
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128
2021-08-26 12:59:23.103159
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128
2021-08-26 12:59:26.725663
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128
2021-08-26 12:59:29.801039
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128
2021-08-26 12:59:32.503437
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128
2021-08-26 12:59:35.175927
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128
2021-08-26 12:59:37.851854
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128
2021-08-26 12:59:40.913654
b''

Starting computation for size [128, 128, 256]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256
2021-08-26 12:59:44.316014
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256
2021-08-26 12:59:48.260510
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256
2021-08-26 12:59:51.843316
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256
2021-08-26 12:59:56.016641
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256
2021-08-26 12:59:59.907296
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256
2021-08-26 13:00:04.000709
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256
2021-08-26 13:00:07.976236
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256
2021-08-26 13:00:11.516611
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256
2021-08-26 13:00:15.147746
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256
2021-08-26 13:00:20.220871
b''

Starting computation for size [128, 256, 256]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256
2021-08-26 13:00:24.168219
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256
2021-08-26 13:00:29.556805
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256
2021-08-26 13:00:35.775002
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256
2021-08-26 13:00:42.996281
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256
2021-08-26 13:00:48.512460
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256
2021-08-26 13:00:54.104116
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256
2021-08-26 13:00:59.884074
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256
2021-08-26 13:01:05.317516
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256
2021-08-26 13:01:11.649189
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256
2021-08-26 13:01:17.752953
b''

Starting computation for size 256
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256
2021-08-26 13:01:23.400061
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256
2021-08-26 13:01:33.060435
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256
2021-08-26 13:01:42.236596
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256
2021-08-26 13:01:52.249167
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256
2021-08-26 13:02:01.556041
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256
2021-08-26 13:02:11.034839
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256
2021-08-26 13:02:19.859230
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256
2021-08-26 13:02:30.079339
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256
2021-08-26 13:02:39.501425
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256
2021-08-26 13:02:49.960808
b''

Starting computation for size [256, 256, 512]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512
2021-08-26 13:02:59.528341
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512
2021-08-26 13:03:15.415564
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512
2021-08-26 13:03:31.923015
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512
2021-08-26 13:03:50.906879
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512
2021-08-26 13:04:08.545969
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512
2021-08-26 13:04:26.493499
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512
2021-08-26 13:04:43.059885
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512
2021-08-26 13:05:00.241023
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512
2021-08-26 13:05:16.380649
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512
2021-08-26 13:05:33.000625
b''

Starting computation for size [256, 512, 512]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512
2021-08-26 13:05:49.876943
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512
2021-08-26 13:06:21.355044
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512
2021-08-26 13:06:51.909223
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512
2021-08-26 13:07:27.039496
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512
2021-08-26 13:07:59.636181
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512
2021-08-26 13:08:32.829281
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512
2021-08-26 13:09:05.248203
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512
2021-08-26 13:09:36.243508
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512
2021-08-26 13:10:06.358768
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512
2021-08-26 13:10:37.115358
b''

Starting computation for size 512
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512
2021-08-26 13:11:07.817563
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512
2021-08-26 13:12:06.141302
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512
2021-08-26 13:13:12.551044
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512
2021-08-26 13:14:33.240899
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512
2021-08-26 13:15:29.823081
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512
2021-08-26 13:16:28.164455
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512
2021-08-26 13:17:24.929759
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512
2021-08-26 13:18:22.333616
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512
2021-08-26 13:19:20.621596
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512
2021-08-26 13:20:20.111173
b''

Starting computation for size [512, 512, 1024]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024
2021-08-26 13:21:18.620219
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024
2021-08-26 13:23:15.755652
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024
2021-08-26 13:25:09.709015
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024
2021-08-26 13:27:28.371705
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024
2021-08-26 13:29:22.608803
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024
2021-08-26 13:31:43.824221
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024
2021-08-26 13:33:38.496447
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024
2021-08-26 13:35:36.085124
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024
2021-08-26 13:37:32.023626
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024
2021-08-26 13:39:30.268295
b''

Starting computation for size [512, 1024, 1024]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024
2021-08-26 13:41:27.520306
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024
2021-08-26 13:45:15.235801
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024
2021-08-26 13:49:25.095036
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024
2021-08-26 13:53:49.719408
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024
2021-08-26 13:57:34.695049
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024
2021-08-26 14:01:28.159893
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024
2021-08-26 14:05:44.052424
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024
2021-08-26 14:09:35.503720
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024
2021-08-26 14:13:19.372370
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024
2021-08-26 14:17:04.620037
b''

Starting computation for size 128
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-08-26 14:21:24.463234
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-08-26 14:21:27.112589
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-08-26 14:21:30.108956
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-08-26 14:21:33.081572
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-08-26 14:21:36.143480
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-08-26 14:21:39.265755
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-08-26 14:21:42.460573
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-08-26 14:21:45.573347
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-08-26 14:21:47.835816
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-08-26 14:21:50.073468
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

Starting computation for size 256
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-08-26 14:21:52.376204
b'Result (avg): 5.19268e-09\nResult (max): 5.08699e-08\n'

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-08-26 14:21:55.562758
b'Result (avg): 5.19268e-09\nResult (max): 5.08699e-08\n'

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-08-26 14:21:58.739714
b'Result (avg): 5.19268e-09\nResult (max): 5.08699e-08\n'

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-08-26 14:22:02.049212
b'Result (avg): 5.19268e-09\nResult (max): 5.08699e-08\n'

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-08-26 14:22:05.160849
b'Result (avg): 5.19268e-09\nResult (max): 5.08699e-08\n'

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-08-26 14:22:08.364082
b'Result (avg): 5.19268e-09\nResult (max): 5.08699e-08\n'

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-08-26 14:22:11.747717
b'Result (avg): 5.19268e-09\nResult (max): 5.08699e-08\n'

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-08-26 14:22:15.116608
b'Result (avg): 5.19268e-09\nResult (max): 5.08699e-08\n'

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-08-26 14:22:18.344812
b'Result (avg): 5.19268e-09\nResult (max): 5.08699e-08\n'

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-08-26 14:22:21.585200
b'Result (avg): 5.19268e-09\nResult (max): 5.08699e-08\n'

Starting computation for size 512
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-08-26 14:22:24.765551
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-08-26 14:22:35.276588
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-08-26 14:22:45.367859
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-08-26 14:22:57.697351
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-08-26 14:23:07.837629
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-08-26 14:23:18.037608
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-08-26 14:23:28.527935
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-08-26 14:23:39.501353
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-08-26 14:23:49.677015
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-08-26 14:24:00.407465
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

Starting computation for size [512, 1024, 1024]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -t 4 -nx 512 -ny 1024 -nz 1024 --testcase 4
2021-08-26 14:24:10.753002
b'Result (avg): 0.000306938\nResult (max): 0.00119158\n'

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -t 4 -nx 512 -ny 1024 -nz 1024 --testcase 4
2021-08-26 14:24:46.563728
b'Result (avg): 0.000306938\nResult (max): 0.00119158\n'

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -t 4 -nx 512 -ny 1024 -nz 1024 --testcase 4
2021-08-26 14:25:21.679632
b'Result (avg): 0.000306938\nResult (max): 0.00119158\n'

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -t 4 -nx 512 -ny 1024 -nz 1024 --testcase 4
2021-08-26 14:26:02.968024
b'Result (avg): 0.000306938\nResult (max): 0.00119158\n'

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -t 4 -nx 512 -ny 1024 -nz 1024 --testcase 4
2021-08-26 14:26:37.128290
b'Result (avg): 0.000306938\nResult (max): 0.00119158\n'

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -t 4 -nx 512 -ny 1024 -nz 1024 --testcase 4
2021-08-26 14:27:12.479731
b'Result (avg): 0.000306938\nResult (max): 0.00119158\n'

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -t 4 -nx 512 -ny 1024 -nz 1024 --testcase 4
2021-08-26 14:27:50.732911
b'Result (avg): 0.000306938\nResult (max): 0.00119158\n'

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -t 4 -nx 512 -ny 1024 -nz 1024 --testcase 4
2021-08-26 14:28:25.917062
b'Result (avg): 0.000306938\nResult (max): 0.00119158\n'

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -t 4 -nx 512 -ny 1024 -nz 1024 --testcase 4
2021-08-26 14:29:00.521063
b'Result (avg): 0.000306938\nResult (max): 0.00119158\n'

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/pcsgs/forward -s Z_Then_YX --opt 1 -t 4 -nx 512 -ny 1024 -nz 1024 --testcase 4
2021-08-26 14:29:35.544749
b'Result (avg): 0.000306938\nResult (max): 0.00119158\n'

Slab 2D->1D default (inverse)
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
Starting computation for size 128
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 128 -ny 128 -nz 128 --testcase 2
2021-08-26 14:30:09.349551
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 128 -ny 128 -nz 128 --testcase 2
2021-08-26 14:30:11.968015
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 128 -ny 128 -nz 128 --testcase 2
2021-08-26 14:30:14.543681
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 128 -ny 128 -nz 128 --testcase 2
2021-08-26 14:30:17.515897
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 128 -ny 128 -nz 128 --testcase 2
2021-08-26 14:30:20.008351
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 128 -ny 128 -nz 128 --testcase 2
2021-08-26 14:30:22.491851
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 128 -ny 128 -nz 128 --testcase 2
2021-08-26 14:30:25.067875
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 128 -ny 128 -nz 128 --testcase 2
2021-08-26 14:30:27.603879
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 128 -ny 128 -nz 128 --testcase 2
2021-08-26 14:30:30.159761
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 128 -ny 128 -nz 128 --testcase 2
2021-08-26 14:30:33.267786
b''

Starting computation for size [128, 128, 256]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 128 -ny 128 -nz 256 --testcase 2
2021-08-26 14:30:35.795727
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 128 -ny 128 -nz 256 --testcase 2
2021-08-26 14:30:39.359452
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 128 -ny 128 -nz 256 --testcase 2
2021-08-26 14:30:42.735726
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 128 -ny 128 -nz 256 --testcase 2
2021-08-26 14:30:46.699799
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 128 -ny 128 -nz 256 --testcase 2
2021-08-26 14:30:50.163743
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 128 -ny 128 -nz 256 --testcase 2
2021-08-26 14:30:53.652589
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 128 -ny 128 -nz 256 --testcase 2
2021-08-26 14:30:57.159809
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 128 -ny 128 -nz 256 --testcase 2
2021-08-26 14:31:00.567739
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 128 -ny 128 -nz 256 --testcase 2
2021-08-26 14:31:04.004154
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 128 -ny 128 -nz 256 --testcase 2
2021-08-26 14:31:07.539933
b''

Starting computation for size [128, 256, 256]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 128 -ny 256 -nz 256 --testcase 2
2021-08-26 14:31:11.071796
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 128 -ny 256 -nz 256 --testcase 2
2021-08-26 14:31:16.323510
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 128 -ny 256 -nz 256 --testcase 2
2021-08-26 14:31:21.499805
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 128 -ny 256 -nz 256 --testcase 2
2021-08-26 14:31:27.259704
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 128 -ny 256 -nz 256 --testcase 2
2021-08-26 14:31:32.519816
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 128 -ny 256 -nz 256 --testcase 2
2021-08-26 14:31:37.984254
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 128 -ny 256 -nz 256 --testcase 2
2021-08-26 14:31:43.512164
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 128 -ny 256 -nz 256 --testcase 2
2021-08-26 14:31:48.939766
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 128 -ny 256 -nz 256 --testcase 2
2021-08-26 14:31:54.155809
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 128 -ny 256 -nz 256 --testcase 2
2021-08-26 14:31:59.431778
b''

Starting computation for size 256
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 256 -ny 256 -nz 256 --testcase 2
2021-08-26 14:32:04.923906
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 256 -ny 256 -nz 256 --testcase 2
2021-08-26 14:32:13.551615
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 256 -ny 256 -nz 256 --testcase 2
2021-08-26 14:32:22.124072
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 256 -ny 256 -nz 256 --testcase 2
2021-08-26 14:32:30.684093
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 256 -ny 256 -nz 256 --testcase 2
2021-08-26 14:32:39.467581
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 256 -ny 256 -nz 256 --testcase 2
2021-08-26 14:32:48.603518
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 256 -ny 256 -nz 256 --testcase 2
2021-08-26 14:32:57.130177
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 256 -ny 256 -nz 256 --testcase 2
2021-08-26 14:33:05.699924
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 256 -ny 256 -nz 256 --testcase 2
2021-08-26 14:33:14.283704
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 256 -ny 256 -nz 256 --testcase 2
2021-08-26 14:33:23.255845
b''

Starting computation for size [256, 256, 512]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 256 -ny 256 -nz 512 --testcase 2
2021-08-26 14:33:32.547613
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 256 -ny 256 -nz 512 --testcase 2
2021-08-26 14:33:48.239725
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 256 -ny 256 -nz 512 --testcase 2
2021-08-26 14:34:03.944413
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 256 -ny 256 -nz 512 --testcase 2
2021-08-26 14:34:20.099803
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 256 -ny 256 -nz 512 --testcase 2
2021-08-26 14:34:38.344066
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 256 -ny 256 -nz 512 --testcase 2
2021-08-26 14:34:53.831592
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 256 -ny 256 -nz 512 --testcase 2
2021-08-26 14:35:10.127797
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 256 -ny 256 -nz 512 --testcase 2
2021-08-26 14:35:26.094193
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 256 -ny 256 -nz 512 --testcase 2
2021-08-26 14:35:57.587486
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 256 -ny 256 -nz 512 --testcase 2
2021-08-26 14:36:24.802349
b''

Starting computation for size [256, 512, 512]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 256 -ny 512 -nz 512 --testcase 2
2021-08-26 14:36:45.187693
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 256 -ny 512 -nz 512 --testcase 2
2021-08-26 14:37:15.591444
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 256 -ny 512 -nz 512 --testcase 2
2021-08-26 14:37:44.659648
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 256 -ny 512 -nz 512 --testcase 2
2021-08-26 14:38:13.447507
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 256 -ny 512 -nz 512 --testcase 2
2021-08-26 14:38:42.071659
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 256 -ny 512 -nz 512 --testcase 2
2021-08-26 14:39:13.099881
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 256 -ny 512 -nz 512 --testcase 2
2021-08-26 14:39:41.467681
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 256 -ny 512 -nz 512 --testcase 2
2021-08-26 14:40:12.663399
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 256 -ny 512 -nz 512 --testcase 2
2021-08-26 14:40:42.559732
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 256 -ny 512 -nz 512 --testcase 2
2021-08-26 14:41:13.511418
b''

Starting computation for size 512
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 512 -ny 512 -nz 512 --testcase 2
2021-08-26 14:41:43.274716
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 512 -ny 512 -nz 512 --testcase 2
2021-08-26 14:42:40.751462
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 512 -ny 512 -nz 512 --testcase 2
2021-08-26 14:43:36.839647
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 512 -ny 512 -nz 512 --testcase 2
2021-08-26 14:44:33.568014
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 512 -ny 512 -nz 512 --testcase 2
2021-08-26 14:45:30.791770
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 512 -ny 512 -nz 512 --testcase 2
2021-08-26 14:46:28.806361
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 512 -ny 512 -nz 512 --testcase 2
2021-08-26 14:47:24.571698
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 512 -ny 512 -nz 512 --testcase 2
2021-08-26 14:48:21.819466
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 512 -ny 512 -nz 512 --testcase 2
2021-08-26 14:49:16.635790
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 512 -ny 512 -nz 512 --testcase 2
2021-08-26 14:50:15.199686
b''

Starting computation for size [512, 512, 1024]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 512 -ny 512 -nz 1024 --testcase 2
2021-08-26 14:51:13.975549
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 512 -ny 512 -nz 1024 --testcase 2
2021-08-26 14:53:35.972994
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 512 -ny 512 -nz 1024 --testcase 2
2021-08-26 14:55:31.870623
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 512 -ny 512 -nz 1024 --testcase 2
2021-08-26 14:57:25.588235
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 512 -ny 512 -nz 1024 --testcase 2
2021-08-26 14:59:18.342816
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 512 -ny 512 -nz 1024 --testcase 2
2021-08-26 15:01:13.379954
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 512 -ny 512 -nz 1024 --testcase 2
2021-08-26 15:03:01.779474
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 512 -ny 512 -nz 1024 --testcase 2
2021-08-26 15:04:56.200613
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 512 -ny 512 -nz 1024 --testcase 2
2021-08-26 15:06:46.751324
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 512 -ny 512 -nz 1024 --testcase 2
2021-08-26 15:08:46.299359
b''

Starting computation for size [512, 1024, 1024]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-08-26 15:11:00.739460
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-08-26 15:14:48.119399
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-08-26 15:18:27.553108
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-08-26 15:22:38.831516
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-08-26 15:26:44.216507
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-08-26 15:30:36.695335
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-08-26 15:34:14.255268
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-08-26 15:38:04.255883
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-08-26 15:41:48.428070
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-08-26 15:45:51.391819
b''

Slab 2D->1D opt1 (inverse)
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
Starting computation for size 128
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-08-26 15:49:38.612466
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-08-26 15:49:41.363951
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-08-26 15:49:43.925628
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-08-26 15:49:46.805240
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-08-26 15:49:49.989340
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-08-26 15:49:52.589313
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-08-26 15:49:55.541357
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-08-26 15:49:57.936056
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-08-26 15:50:00.297729
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-08-26 15:50:02.929750
b''

Starting computation for size [128, 128, 256]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-08-26 15:50:06.137534
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-08-26 15:50:09.607994
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-08-26 15:50:12.959711
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-08-26 15:50:16.872293
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-08-26 15:50:20.632045
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-08-26 15:50:24.032156
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-08-26 15:50:28.225783
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-08-26 15:50:31.580094
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-08-26 15:50:35.011997
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-08-26 15:50:38.405668
b''

Starting computation for size [128, 256, 256]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-08-26 15:50:42.941768
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-08-26 15:50:48.208115
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-08-26 15:50:53.360057
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-08-26 15:50:59.321555
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-08-26 15:51:06.024054
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-08-26 15:51:11.249475
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-08-26 15:51:17.912016
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-08-26 15:51:23.001628
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-08-26 15:51:28.160108
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-08-26 15:51:33.340096
b''

Starting computation for size 256
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-08-26 15:51:40.756144
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-08-26 15:51:49.676086
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-08-26 15:51:58.752156
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-08-26 15:52:07.652109
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-08-26 15:52:17.164108
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-08-26 15:52:26.084066
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-08-26 15:52:35.288118
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-08-26 15:52:44.120050
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-08-26 15:52:52.620094
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-08-26 15:53:01.440162
b''

Starting computation for size [256, 256, 512]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-08-26 15:53:12.363705
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-08-26 15:53:27.997533
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-08-26 15:53:44.075866
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-08-26 15:54:00.873454
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-08-26 15:54:17.584013
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-08-26 15:54:33.525666
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-08-26 15:54:49.276204
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-08-26 15:55:05.077736
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-08-26 15:55:20.456111
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-08-26 15:55:36.520147
b''

Starting computation for size [256, 512, 512]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-08-26 15:55:54.997715
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-08-26 15:56:23.817801
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-08-26 15:56:53.413772
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-08-26 15:57:23.700164
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-08-26 15:57:53.761676
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-08-26 15:58:23.704199
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-08-26 15:59:01.196339
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-08-26 15:59:52.340338
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-08-26 16:00:24.161765
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-08-26 16:00:54.497722
b''

Starting computation for size 512
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-08-26 16:01:28.584290
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-08-26 16:02:25.972213
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-08-26 16:03:23.501895
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-08-26 16:04:26.057827
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-08-26 16:05:23.472278
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-08-26 16:06:21.526008
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-08-26 16:07:16.752306
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-08-26 16:08:15.493601
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-08-26 16:09:11.104257
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-08-26 16:10:08.260376
b''

Starting computation for size [512, 512, 1024]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-08-26 16:11:11.672301
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-08-26 16:13:05.276267
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-08-26 16:15:05.916184
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-08-26 16:17:19.370266
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-08-26 16:19:10.074200
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-08-26 16:21:00.672296
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-08-26 16:22:48.416300
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-08-26 16:24:45.582256
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-08-26 16:26:35.367465
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-08-26 16:28:29.523812
b''

Starting computation for size [512, 1024, 1024]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-08-26 16:30:32.499768
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-08-26 16:34:45.188106
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-08-26 16:38:25.779213
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-08-26 16:42:21.415787
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-08-26 16:45:59.456218
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-08-26 16:50:13.424168
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-08-26 16:53:57.594480
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-08-26 16:57:49.508220
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-08-26 17:01:33.422418
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-08-26 17:05:55.263113
b''

Slab 1D->2D default (inverse)
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
No protocol specified
Starting computation for size 128
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 2
2021-08-26 17:10:05.955659
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 2
2021-08-26 17:10:08.628284
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 2
2021-08-26 17:10:11.010709
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 2
2021-08-26 17:10:13.834608
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 2
2021-08-26 17:10:16.262660
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 2
2021-08-26 17:10:18.706333
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 2
2021-08-26 17:10:21.856154
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 2
2021-08-26 17:10:24.302540
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 2
2021-08-26 17:10:26.712239
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 2
2021-08-26 17:10:29.158631
b''

Starting computation for size [128, 128, 256]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 256 --testcase 2
2021-08-26 17:10:32.878639
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 256 --testcase 2
2021-08-26 17:10:36.380202
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 256 --testcase 2
2021-08-26 17:10:39.690647
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 256 --testcase 2
2021-08-26 17:10:43.524181
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 256 --testcase 2
2021-08-26 17:10:46.824190
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 256 --testcase 2
2021-08-26 17:10:50.228085
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 256 --testcase 2
2021-08-26 17:10:53.574538
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 256 --testcase 2
2021-08-26 17:10:56.982568
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 256 --testcase 2
2021-08-26 17:11:00.338576
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 256 --testcase 2
2021-08-26 17:11:03.666408
b''

Starting computation for size [128, 256, 256]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 128 -ny 256 -nz 256 --testcase 2
2021-08-26 17:11:07.576116
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 128 -ny 256 -nz 256 --testcase 2
2021-08-26 17:11:12.920059
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 128 -ny 256 -nz 256 --testcase 2
2021-08-26 17:11:18.082653
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 128 -ny 256 -nz 256 --testcase 2
2021-08-26 17:11:23.894674
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 128 -ny 256 -nz 256 --testcase 2
2021-08-26 17:11:29.050514
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 128 -ny 256 -nz 256 --testcase 2
2021-08-26 17:11:34.308139
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 128 -ny 256 -nz 256 --testcase 2
2021-08-26 17:11:39.512009
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 128 -ny 256 -nz 256 --testcase 2
2021-08-26 17:11:44.692060
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 128 -ny 256 -nz 256 --testcase 2
2021-08-26 17:11:50.050669
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 128 -ny 256 -nz 256 --testcase 2
2021-08-26 17:11:55.322547
b''

Starting computation for size 256
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 2
2021-08-26 17:12:01.510631
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 2
2021-08-26 17:12:10.444068
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 2
2021-08-26 17:12:19.640166
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 2
2021-08-26 17:12:28.148130
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 2
2021-08-26 17:12:36.779925
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 2
2021-08-26 17:12:45.720187
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 2
2021-08-26 17:12:54.664102
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 2
2021-08-26 17:13:03.538574
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 2
2021-08-26 17:13:12.478585
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 2
2021-08-26 17:13:21.678558
b''

Starting computation for size [256, 256, 512]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 512 --testcase 2
2021-08-26 17:13:32.494639
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 512 --testcase 2
2021-08-26 17:13:48.150356
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 512 --testcase 2
2021-08-26 17:14:03.604093
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 512 --testcase 2
2021-08-26 17:14:22.500175
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 512 --testcase 2
2021-08-26 17:14:38.368214
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 512 --testcase 2
2021-08-26 17:14:54.706446
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 512 --testcase 2
2021-08-26 17:15:10.358583
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 512 --testcase 2
2021-08-26 17:15:26.020132
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 512 --testcase 2
2021-08-26 17:15:41.300216
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 512 --testcase 2
2021-08-26 17:15:56.618640
b''

Starting computation for size [256, 512, 512]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 256 -ny 512 -nz 512 --testcase 2
2021-08-26 17:16:14.420141
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 256 -ny 512 -nz 512 --testcase 2
2021-08-26 17:16:43.855859
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 256 -ny 512 -nz 512 --testcase 2
2021-08-26 17:17:13.463956
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 256 -ny 512 -nz 512 --testcase 2
2021-08-26 17:17:50.559999
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 256 -ny 512 -nz 512 --testcase 2
2021-08-26 17:18:21.551231
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 256 -ny 512 -nz 512 --testcase 2
2021-08-26 17:18:52.843245
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 256 -ny 512 -nz 512 --testcase 2
2021-08-26 17:19:22.798982
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 256 -ny 512 -nz 512 --testcase 2
2021-08-26 17:19:52.023269
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 256 -ny 512 -nz 512 --testcase 2
2021-08-26 17:20:20.619173
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 256 -ny 512 -nz 512 --testcase 2
2021-08-26 17:20:57.691497
b''

Starting computation for size 512
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 2
2021-08-26 17:21:49.672352
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 2
2021-08-26 17:22:47.252163
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 2
2021-08-26 17:23:49.996179
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 2
2021-08-26 17:25:03.324342
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 2
2021-08-26 17:26:02.160224
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 2
2021-08-26 17:27:01.674662
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 2
2021-08-26 17:27:58.942775
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 2
2021-08-26 17:28:57.884221
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 2
2021-08-26 17:29:55.628271
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 2
2021-08-26 17:30:57.231452
b''

Starting computation for size [512, 512, 1024]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 1024 --testcase 2
2021-08-26 17:32:03.042747
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 1024 --testcase 2
2021-08-26 17:33:56.580194
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 1024 --testcase 2
2021-08-26 17:35:50.132259
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 1024 --testcase 2
2021-08-26 17:37:55.056325
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 1024 --testcase 2
2021-08-26 17:40:09.068257
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 1024 --testcase 2
2021-08-26 17:42:04.552153
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 1024 --testcase 2
2021-08-26 17:43:52.012130
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 1024 --testcase 2
2021-08-26 17:45:44.120105
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 1024 --testcase 2
2021-08-26 17:47:36.242639
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 1024 --testcase 2
2021-08-26 17:49:32.563739
b''

Starting computation for size [512, 1024, 1024]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-08-26 17:51:36.614654
b''

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-08-26 17:55:57.654934
b''

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-08-26 17:59:36.852188
b''

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-08-26 18:04:29.932131
b''

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-08-26 18:08:17.622786
b''

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-08-26 18:12:30.778509
b''

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-08-26 18:16:15.355101
b''

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-08-26 18:20:06.731294
b''

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-08-26 18:23:51.823544
b''

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 4 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-08-26 18:27:55.698086
b''

Slab 1D->2D opt1 (inverse)
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
No protocol specified
No protocol specified
No protocol specified
No protocol specified
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  slab

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
Starting computation for size 128
-> Executing test 0
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-08-26 18:32:11.345895
b''

-> Executing test 1
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-08-26 18:32:12.415732
b''

-> Executing test 2
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-08-26 18:32:13.443911
b''

-> Executing test 3
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-08-26 18:32:14.335947
b''

-> Executing test 4
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-08-26 18:32:15.179688
b''

-> Executing test 5
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-08-26 18:32:16.020192
b''

-> Executing test 6
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-08-26 18:32:16.875741
b''

-> Executing test 7
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-08-26 18:32:17.720262
b''

-> Executing test 8
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-08-26 18:32:18.560513
b''

-> Executing test 9
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-08-26 18:32:19.407754
b''

Starting computation for size [128, 128, 256]
-> Executing test 0
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-08-26 18:32:20.263856
b''

-> Executing test 1
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-08-26 18:32:21.095780
b''

-> Executing test 2
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-08-26 18:32:21.943760
b''

-> Executing test 3
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-08-26 18:32:22.779759
b''

-> Executing test 4
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-08-26 18:32:23.640261
b''

-> Executing test 5
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-08-26 18:32:24.507715
b''

-> Executing test 6
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-08-26 18:32:25.344209
b''

-> Executing test 7
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-08-26 18:32:26.188218
b''

-> Executing test 8
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-08-26 18:32:27.064385
b''

-> Executing test 9
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-08-26 18:32:27.912612
b''

Starting computation for size [128, 256, 256]
-> Executing test 0
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-08-26 18:32:28.787931
b''

-> Executing test 1
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-08-26 18:32:29.623544
b''

-> Executing test 2
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-08-26 18:32:30.483757
b''

-> Executing test 3
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-08-26 18:32:31.355709
b''

-> Executing test 4
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-08-26 18:32:32.199760
b''

-> Executing test 5
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-08-26 18:32:33.055783
b''

-> Executing test 6
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-08-26 18:32:33.983934
b''

-> Executing test 7
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-08-26 18:32:34.864445
b''

-> Executing test 8
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-08-26 18:32:35.712247
b''

-> Executing test 9
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-08-26 18:32:36.572125
b''

Starting computation for size 256
-> Executing test 0
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-08-26 18:32:37.448081
b''

-> Executing test 1
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-08-26 18:32:38.296357
b''

-> Executing test 2
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-08-26 18:32:39.135848
b''

-> Executing test 3
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-08-26 18:32:39.979707
b''

-> Executing test 4
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-08-26 18:32:40.803943
b''

-> Executing test 5
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-08-26 18:32:41.687795
b''

-> Executing test 6
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-08-26 18:32:42.503701
b''

-> Executing test 7
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-08-26 18:32:43.435702
b''

-> Executing test 8
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-08-26 18:32:44.256090
b''

-> Executing test 9
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-08-26 18:32:45.115592
b''

Starting computation for size [256, 256, 512]
-> Executing test 0
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-08-26 18:32:45.947623
b''

-> Executing test 1
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-08-26 18:32:46.787704
b''

-> Executing test 2
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-08-26 18:32:47.635642
b''

-> Executing test 3
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-08-26 18:32:48.475963
b''

-> Executing test 4
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-08-26 18:32:49.327752
b''

-> Executing test 5
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-08-26 18:32:50.171697
b''

-> Executing test 6
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-08-26 18:32:51.036072
b''

-> Executing test 7
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-08-26 18:32:51.871952
b''

-> Executing test 8
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-08-26 18:32:52.735926
b''

-> Executing test 9
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-08-26 18:32:53.563816
b''

Starting computation for size [256, 512, 512]
-> Executing test 0
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-08-26 18:32:54.423769
b''

-> Executing test 1
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-08-26 18:32:55.311675
b''

-> Executing test 2
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-08-26 18:32:56.180405
b''

-> Executing test 3
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-08-26 18:32:57.036235
b''

-> Executing test 4
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-08-26 18:32:57.851773
b''

-> Executing test 5
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-08-26 18:32:58.699779
b''

-> Executing test 6
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-08-26 18:32:59.551613
b''

-> Executing test 7
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-08-26 18:33:00.372528
b''

-> Executing test 8
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-08-26 18:33:01.199690
b''

-> Executing test 9
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-08-26 18:33:02.063714
b''

Starting computation for size 512
-> Executing test 0
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-08-26 18:33:02.911761
b''

-> Executing test 1
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-08-26 18:33:03.731767
b''

-> Executing test 2
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-08-26 18:33:04.543711
b''

-> Executing test 3
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-08-26 18:33:05.432214
b''

-> Executing test 4
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-08-26 18:33:06.271479
b''

-> Executing test 5
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-08-26 18:33:07.087727
b''

-> Executing test 6
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-08-26 18:33:07.975904
b''

-> Executing test 7
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-08-26 18:33:08.851894
b''

-> Executing test 8
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-08-26 18:33:09.711692
b''

-> Executing test 9
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-08-26 18:33:10.591852
b''

Starting computation for size [512, 512, 1024]
-> Executing test 0
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-08-26 18:33:11.420372
b''

-> Executing test 1
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-08-26 18:33:12.476778
b''

-> Executing test 2
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-08-26 18:33:13.360529
b''

-> Executing test 3
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-08-26 18:33:14.180610
b''

-> Executing test 4
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-08-26 18:33:15.016330
b''

-> Executing test 5
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-08-26 18:33:15.887750
b''

-> Executing test 6
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-08-26 18:33:16.740421
b''

-> Executing test 7
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-08-26 18:33:17.599522
b''

-> Executing test 8
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-08-26 18:33:18.427824
b''

-> Executing test 9
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-08-26 18:33:19.263780
b''

Starting computation for size [512, 1024, 1024]
-> Executing test 0
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-08-26 18:33:20.079594
b''

-> Executing test 1
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-08-26 18:33:20.919728
b''

-> Executing test 2
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-08-26 18:33:21.771646
b''

-> Executing test 3
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-08-26 18:33:22.603878
b''

-> Executing test 4
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-08-26 18:33:23.439910
b''

-> Executing test 5
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-08-26 18:33:24.283898
b''

-> Executing test 6
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-08-26 18:33:25.115728
b''

-> Executing test 7
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-08-26 18:33:25.987827
b''

-> Executing test 8
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-08-26 18:33:26.831758
b''

-> Executing test 9
mpiexec -n 24 --hostfile ../mpi/hostfile_pcsgs slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -t 2 -p 24 -b ../benchmarks/pcsgs/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-08-26 18:33:27.707811
b''

all done
