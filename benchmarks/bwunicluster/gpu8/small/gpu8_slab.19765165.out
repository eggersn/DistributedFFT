Modules loaded
uc2n515
uc2n515
uc2n515
uc2n515
uc2n515
uc2n515
uc2n515
uc2n515
uc2n517
uc2n517
uc2n517
uc2n517
uc2n517
uc2n517
uc2n517
uc2n517
uc2n515 uc2n515 uc2n515 uc2n515 uc2n515 uc2n515 uc2n515 uc2n515 uc2n517 uc2n517 uc2n517 uc2n517 uc2n517 uc2n517 uc2n517 uc2n517
uc2n515 uc2n515 uc2n515 uc2n515 uc2n515 uc2n515 uc2n515 uc2n515
uc2n517 uc2n517 uc2n517 uc2n517 uc2n517 uc2n517 uc2n517 uc2n517
start building
-- The CUDA compiler identification is NVIDIA 11.0.194
-- The CXX compiler identification is GNU 8.3.1
-- Detecting CUDA compiler ABI info
-- Detecting CUDA compiler ABI info - done
-- Check for working CUDA compiler: /opt/bwhpc/common/devel/cuda/11.0/bin/nvcc - skipped
-- Detecting CUDA compile features
-- Detecting CUDA compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/g++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found MPI_CXX: /pfs/data5/software_uc2/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libmpi.so (found version "3.1") 
-- Found MPI: TRUE (found version "3.1")  
-- Found CUDAToolkit: /opt/bwhpc/common/devel/cuda/11.0/include (found version "11.0.194") 
-- Looking for C++ include pthread.h
-- Looking for C++ include pthread.h - found
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed
-- Looking for pthread_create in pthreads
-- Looking for pthread_create in pthreads - not found
-- Looking for pthread_create in pthread
-- Looking for pthread_create in pthread - found
-- Found Threads: TRUE  
-- Configuring done
-- Generating done
-- Build files have been written to: /home/st/st_us-051200/st_st160727/DistributedFFT/build
Scanning dependencies of target mpicufft
[  3%] Building CXX object CMakeFiles/mpicufft.dir/src/mpicufft.cpp.o
[  6%] Linking CXX shared library libmpicufft.so
[  6%] Built target mpicufft
Scanning dependencies of target timer
[  9%] Building CXX object CMakeFiles/timer.dir/src/timer.cpp.o
[ 12%] Linking CXX shared library libtimer.so
[ 12%] Built target timer
Scanning dependencies of target pencil_decomp
[ 15%] Building CXX object CMakeFiles/pencil_decomp.dir/src/pencil/mpicufft_pencil.cpp.o
[ 18%] Building CXX object CMakeFiles/pencil_decomp.dir/src/pencil/mpicufft_pencil_opt1.cpp.o
[ 21%] Linking CXX shared library libpencil_decomp.so
[ 21%] Built target pencil_decomp
Scanning dependencies of target test_base
[ 24%] Building CUDA object CMakeFiles/test_base.dir/tests/src/base.cu.o
nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 27%] Linking CUDA shared library libtest_base.so
[ 27%] Built target test_base
Scanning dependencies of target pencil_tests
[ 30%] Building CUDA object CMakeFiles/pencil_tests.dir/tests/src/pencil/base.cu.o
nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 33%] Building CUDA object CMakeFiles/pencil_tests.dir/tests/src/pencil/random_dist_1D.cu.o
nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 36%] Building CUDA object CMakeFiles/pencil_tests.dir/tests/src/pencil/random_dist_2D.cu.o
nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 39%] Building CUDA object CMakeFiles/pencil_tests.dir/tests/src/pencil/random_dist_3D.cu.o
nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 42%] Linking CUDA shared library libpencil_tests.so
[ 42%] Built target pencil_tests
Scanning dependencies of target pencil
[ 45%] Building CXX object CMakeFiles/pencil.dir/tests/src/pencil/main.cpp.o
[ 48%] Linking CXX executable pencil
[ 48%] Built target pencil
Scanning dependencies of target slab_decomp
[ 51%] Building CXX object CMakeFiles/slab_decomp.dir/src/slab/default/mpicufft_slab.cpp.o
[ 54%] Building CXX object CMakeFiles/slab_decomp.dir/src/slab/default/mpicufft_slab_opt1.cpp.o
[ 57%] Building CXX object CMakeFiles/slab_decomp.dir/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp.o
[ 60%] Building CXX object CMakeFiles/slab_decomp.dir/src/slab/z_then_yx/mpicufft_slab_z_then_yx_opt1.cpp.o
[ 63%] Building CXX object CMakeFiles/slab_decomp.dir/src/slab/y_then_zx/mpicufft_slab_y_then_zx.cpp.o
[ 66%] Linking CXX shared library libslab_decomp.so
[ 66%] Built target slab_decomp
Scanning dependencies of target slab_tests
[ 69%] Building CUDA object CMakeFiles/slab_tests.dir/tests/src/slab/base.cu.o
nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 72%] Building CUDA object CMakeFiles/slab_tests.dir/tests/src/slab/random_dist_default.cu.o
nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 75%] Building CUDA object CMakeFiles/slab_tests.dir/tests/src/slab/random_dist_y_then_zx.cu.o
nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 78%] Building CUDA object CMakeFiles/slab_tests.dir/tests/src/slab/random_dist_z_then_yx.cu.o
nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 81%] Linking CUDA shared library libslab_tests.so
[ 81%] Built target slab_tests
Scanning dependencies of target slab
[ 84%] Building CXX object CMakeFiles/slab.dir/tests/src/slab/main.cpp.o
[ 87%] Linking CXX executable slab
[ 87%] Built target slab
Scanning dependencies of target reference_tests
[ 90%] Building CUDA object CMakeFiles/reference_tests.dir/tests/src/reference/reference.cu.o
nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 93%] Linking CUDA shared library libreference_tests.so
[ 93%] Built target reference_tests
Scanning dependencies of target reference
[ 96%] Building CXX object CMakeFiles/reference.dir/tests/src/reference/main.cpp.o
[100%] Linking CXX executable reference
[100%] Built target reference
finished building
start python script
Starting on HOST16
-----------------------------------------------------------------------------
Slab 2D->1D default
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1120139] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1120139] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1120399] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1120399] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1120666] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1120666] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1121194] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1121194] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1121711] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1121711] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1121977] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1121977] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1122246] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1122246] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1122524] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1122524] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1122791] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1122791] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1123058] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1123058] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1123330] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1123330] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1123602] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1123602] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1123871] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1123871] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1124386] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1124386] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1124912] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1124912] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1125175] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1125175] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1125444] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1125444] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1125711] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1125711] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1125994] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1125994] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1126258] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1126258] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1126522] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1126522] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1126788] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1126788] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1127071] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1127071] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1127587] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1127587] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1128101] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1128101] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1128368] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1128368] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1128647] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1128647] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1128914] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1128914] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1129180] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1129180] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1129456] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1129456] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1129721] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1129721] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1129990] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1129990] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1130255] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1130255] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1130786] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1130786] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1131301] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1131301] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1131567] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1131567] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1131833] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1131833] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1132112] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1132112] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1132381] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1132381] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1132649] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1132649] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1132914] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1132914] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1133189] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1133189] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1133460] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1133460] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1133973] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1133973] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1134506] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1134506] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1134769] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1134769] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1135038] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1135038] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1135303] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1135303] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1135583] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1135583] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1135852] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1135852] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1136118] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1136118] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1136388] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1136388] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1136663] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1136663] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1137180] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1137180] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1137696] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1137696] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1137969] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1137969] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1138238] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1138238] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1138508] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1138508] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1138775] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1138775] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1139050] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1139050] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1139317] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1139317] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1139582] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1139582] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1139864] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1139864] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1140381] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1140381] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1140895] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1140895] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1141178] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1141178] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1141445] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1141445] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1141713] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1141713] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1141980] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1141980] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1142257] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1142257] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1142526] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1142526] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1142794] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1142794] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1143071] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1143071] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1143588] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1143588] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1144103] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1144103] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1144386] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1144386] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1144653] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1144653] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1144920] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1144920] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1145196] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1145196] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1145463] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1145463] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1145730] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1145730] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1146010] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1146010] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1146282] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1146282] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1146811] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1146811] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1147328] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1147328] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1147605] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1147605] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1147878] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1147878] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1148146] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1148146] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1148423] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1148423] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1148691] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1148691] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1148976] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1148976] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1149248] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1149248] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1149524] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1149524] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1150044] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1150044] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1150575] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1150575] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1150846] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1150846] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1151128] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1151128] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1151408] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1151408] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1151675] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1151675] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1151959] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1151959] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1152227] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1152227] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1152525] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1152525] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1152806] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1152806] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1153339] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1153339] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1153875] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1153875] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1154165] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1154165] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1154447] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1154447] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1154736] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1154736] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1155010] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1155010] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1155310] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1155310] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1155584] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1155584] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1155893] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1155893] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1156181] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1156181] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1156745] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1156745] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1157282] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1157282] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1157599] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1157599] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1157887] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1157887] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1158201] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1158201] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1158494] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1158494] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1158822] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1158822] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1159118] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1159118] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1159460] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1159460] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1159773] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1159773] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1160377] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1160377] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1160935] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1160935] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1161281] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1161281] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1161600] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1161600] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n517:796056:0:796056] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x1507c6380000)
[uc2n517:796054:0:796054] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14c288300000)
[uc2n517:796052:0:796052] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x147e4c280000)
[uc2n517:796050:0:796050] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x148d86200000)
[uc2n517:796055:0:796055] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x152bfc340000)
[uc2n517:796053:0:796053] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x148a802c0000)
[uc2n517:796057:0:796057] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x1482963c0000)
[uc2n517:796051:0:796051] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x149824240000)
==== backtrace (tid: 796057) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001069f MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
 9 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
10 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
11 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 796054) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001069f MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
 9 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
10 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
11 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 796055) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001069f MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
 9 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
10 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
11 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 796056) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001069f MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
 9 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
10 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
11 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 796050) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001069f MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
 9 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
10 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
11 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 796053) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001069f MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
 9 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
10 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
11 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 796052) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001069f MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
 9 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
10 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
11 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 796051) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001069f MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
 9 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
10 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
11 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec noticed that process rank 9 with PID 796051 on node uc2n517 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1161878] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1161878] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n517:796408:0:796408] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x153138240010)
[uc2n517:796414:0:796414] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x1502ea3c0010)
[uc2n517:796410:0:796410] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14fb362c0010)
[uc2n517:796411:0:796411] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14f07a300010)
[uc2n517:796412:0:796412] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14a05e340010)
[uc2n517:796409:0:796409] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14d07e280010)
[uc2n517:796413:0:796413] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14bd8a380010)
[uc2n517:796407:0:796407] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x145da8200010)
==== backtrace (tid: 796408) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x00000000001d5e98 cuMemGetAttribute()  ???:0
 3 0x00000000002ffe53 cudbgGetAPI()  ???:0
 4 0x00000000003009e8 cudbgGetAPI()  ???:0
 5 0x0000000000435d90 cudbgApiInit()  ???:0
 6 0x000000000018d730 ???()  /lib64/libcuda.so.1:0
 7 0x000000000018df04 ???()  /lib64/libcuda.so.1:0
 8 0x00000000001904d9 ???()  /lib64/libcuda.so.1:0
 9 0x00000000001fe77e cuGraphAddMemAllocNode()  ???:0
10 0x00000000000a2944 mca_common_cuda_cu_memcpy()  common_cuda.c:0
11 0x000000000007c563 opal_cuda_memcpy_sync()  ???:0
12 0x0000000000077119 non_overlap_cuda_copy_content_same_ddt()  opal_datatype_copy.c:0
13 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
14 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
15 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
16 0x000000000009042b PMPI_Alltoallv()  ???:0
17 0x000000000001069f MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
18 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
19 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
20 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
21 0x000000000040331f main()  ???:0
22 0x00000000000236a3 __libc_start_main()  ???:0
23 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 796414) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x00000000001d5e98 cuMemGetAttribute()  ???:0
 3 0x00000000002ffe53 cudbgGetAPI()  ???:0
 4 0x00000000003009e8 cudbgGetAPI()  ???:0
 5 0x0000000000435d90 cudbgApiInit()  ???:0
 6 0x000000000018d730 ???()  /lib64/libcuda.so.1:0
 7 0x000000000018df04 ???()  /lib64/libcuda.so.1:0
 8 0x00000000001904d9 ???()  /lib64/libcuda.so.1:0
 9 0x00000000001fe77e cuGraphAddMemAllocNode()  ???:0
10 0x00000000000a2944 mca_common_cuda_cu_memcpy()  common_cuda.c:0
11 0x000000000007c563 opal_cuda_memcpy_sync()  ???:0
12 0x0000000000077119 non_overlap_cuda_copy_content_same_ddt()  opal_datatype_copy.c:0
13 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
14 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
15 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
16 0x000000000009042b PMPI_Alltoallv()  ???:0
17 0x000000000001069f MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
18 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
19 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
20 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
21 0x000000000040331f main()  ???:0
22 0x00000000000236a3 __libc_start_main()  ???:0
23 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 796410) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x00000000001d5e98 cuMemGetAttribute()  ???:0
 3 0x00000000002ffe53 cudbgGetAPI()  ???:0
 4 0x00000000003009e8 cudbgGetAPI()  ???:0
 5 0x0000000000435d90 cudbgApiInit()  ???:0
 6 0x000000000018d730 ???()  /lib64/libcuda.so.1:0
 7 0x000000000018df04 ???()  /lib64/libcuda.so.1:0
 8 0x00000000001904d9 ???()  /lib64/libcuda.so.1:0
 9 0x00000000001fe77e cuGraphAddMemAllocNode()  ???:0
10 0x00000000000a2944 mca_common_cuda_cu_memcpy()  common_cuda.c:0
11 0x000000000007c563 opal_cuda_memcpy_sync()  ???:0
12 0x0000000000077119 non_overlap_cuda_copy_content_same_ddt()  opal_datatype_copy.c:0
13 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
14 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
15 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
16 0x000000000009042b PMPI_Alltoallv()  ???:0
17 0x000000000001069f MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
18 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
19 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
20 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
21 0x000000000040331f main()  ???:0
22 0x00000000000236a3 __libc_start_main()  ???:0
23 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 796413) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x00000000001d5e98 cuMemGetAttribute()  ???:0
 3 0x00000000002ffe53 cudbgGetAPI()  ???:0
 4 0x00000000003009e8 cudbgGetAPI()  ???:0
 5 0x0000000000435d90 cudbgApiInit()  ???:0
 6 0x000000000018d730 ???()  /lib64/libcuda.so.1:0
 7 0x000000000018df04 ???()  /lib64/libcuda.so.1:0
 8 0x00000000001904d9 ???()  /lib64/libcuda.so.1:0
 9 0x00000000001fe77e cuGraphAddMemAllocNode()  ???:0
10 0x00000000000a2944 mca_common_cuda_cu_memcpy()  common_cuda.c:0
11 0x000000000007c563 opal_cuda_memcpy_sync()  ???:0
12 0x0000000000077119 non_overlap_cuda_copy_content_same_ddt()  opal_datatype_copy.c:0
13 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
14 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
15 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
16 0x000000000009042b PMPI_Alltoallv()  ???:0
17 0x000000000001069f MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
18 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
19 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
20 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
21 0x000000000040331f main()  ???:0
22 0x00000000000236a3 __libc_start_main()  ???:0
23 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 796411) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x00000000001d5e98 cuMemGetAttribute()  ???:0
 3 0x00000000002ffe53 cudbgGetAPI()  ???:0
 4 0x00000000003009e8 cudbgGetAPI()  ???:0
 5 0x0000000000435d90 cudbgApiInit()  ???:0
 6 0x000000000018d730 ???()  /lib64/libcuda.so.1:0
 7 0x000000000018df04 ???()  /lib64/libcuda.so.1:0
 8 0x00000000001904d9 ???()  /lib64/libcuda.so.1:0
 9 0x00000000001fe77e cuGraphAddMemAllocNode()  ???:0
10 0x00000000000a2944 mca_common_cuda_cu_memcpy()  common_cuda.c:0
11 0x000000000007c563 opal_cuda_memcpy_sync()  ???:0
12 0x0000000000077119 non_overlap_cuda_copy_content_same_ddt()  opal_datatype_copy.c:0
13 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
14 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
15 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
16 0x000000000009042b PMPI_Alltoallv()  ???:0
17 0x000000000001069f MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
18 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
19 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
20 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
21 0x000000000040331f main()  ???:0
22 0x00000000000236a3 __libc_start_main()  ???:0
23 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 796412) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x00000000001d5e98 cuMemGetAttribute()  ???:0
 3 0x00000000002ffe53 cudbgGetAPI()  ???:0
 4 0x00000000003009e8 cudbgGetAPI()  ???:0
 5 0x0000000000435d90 cudbgApiInit()  ???:0
 6 0x000000000018d730 ???()  /lib64/libcuda.so.1:0
 7 0x000000000018df04 ???()  /lib64/libcuda.so.1:0
 8 0x00000000001904d9 ???()  /lib64/libcuda.so.1:0
 9 0x00000000001fe77e cuGraphAddMemAllocNode()  ???:0
10 0x00000000000a2944 mca_common_cuda_cu_memcpy()  common_cuda.c:0
11 0x000000000007c563 opal_cuda_memcpy_sync()  ???:0
12 0x0000000000077119 non_overlap_cuda_copy_content_same_ddt()  opal_datatype_copy.c:0
13 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
14 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
15 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
16 0x000000000009042b PMPI_Alltoallv()  ???:0
17 0x000000000001069f MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
18 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
19 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
20 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
21 0x000000000040331f main()  ???:0
22 0x00000000000236a3 __libc_start_main()  ???:0
23 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 796409) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x00000000001d5e98 cuMemGetAttribute()  ???:0
 3 0x00000000002ffe53 cudbgGetAPI()  ???:0
 4 0x00000000003009e8 cudbgGetAPI()  ???:0
 5 0x0000000000435d90 cudbgApiInit()  ???:0
 6 0x000000000018d730 ???()  /lib64/libcuda.so.1:0
 7 0x000000000018df04 ???()  /lib64/libcuda.so.1:0
 8 0x00000000001904d9 ???()  /lib64/libcuda.so.1:0
 9 0x00000000001fe77e cuGraphAddMemAllocNode()  ???:0
10 0x00000000000a2944 mca_common_cuda_cu_memcpy()  common_cuda.c:0
11 0x000000000007c563 opal_cuda_memcpy_sync()  ???:0
12 0x0000000000077119 non_overlap_cuda_copy_content_same_ddt()  opal_datatype_copy.c:0
13 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
14 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
15 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
16 0x000000000009042b PMPI_Alltoallv()  ???:0
17 0x000000000001069f MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
18 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
19 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
20 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
21 0x000000000040331f main()  ???:0
22 0x00000000000236a3 __libc_start_main()  ???:0
23 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 796407) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x00000000001d5e98 cuMemGetAttribute()  ???:0
 3 0x00000000002ffe53 cudbgGetAPI()  ???:0
 4 0x00000000003009e8 cudbgGetAPI()  ???:0
 5 0x0000000000435d90 cudbgApiInit()  ???:0
 6 0x000000000018d730 ???()  /lib64/libcuda.so.1:0
 7 0x000000000018df04 ???()  /lib64/libcuda.so.1:0
 8 0x00000000001904d9 ???()  /lib64/libcuda.so.1:0
 9 0x00000000001fe77e cuGraphAddMemAllocNode()  ???:0
10 0x00000000000a2944 mca_common_cuda_cu_memcpy()  common_cuda.c:0
11 0x000000000007c563 opal_cuda_memcpy_sync()  ???:0
12 0x0000000000077119 non_overlap_cuda_copy_content_same_ddt()  opal_datatype_copy.c:0
13 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
14 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
15 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
16 0x000000000009042b PMPI_Alltoallv()  ???:0
17 0x000000000001069f MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
18 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
19 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
20 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
21 0x000000000040331f main()  ???:0
22 0x00000000000236a3 __libc_start_main()  ???:0
23 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec noticed that process rank 8 with PID 796407 on node uc2n517 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1162153] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1162153] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n517:796749:0:796749] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14bb40240000)
[uc2n517:796748:0:796748] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14fff4200000)
[uc2n517:796754:0:796754] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14f348380000)
[uc2n517:796755:0:796755] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x148a4c3c0000)
[uc2n517:796752:0:796752] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x145fe8300000)
[uc2n517:796753:0:796753] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x1470dc340000)
[uc2n517:796751:0:796751] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x1503082c0000)
[uc2n517:796750:0:796750] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x146d4c280000)
==== backtrace (tid: 796753) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x0000000000010d69 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
 8 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
 9 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
10 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 796754) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x0000000000010d69 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
 8 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
 9 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
10 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 796755) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x0000000000010d69 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
 8 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
 9 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
10 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 796752) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x0000000000010d69 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
 8 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
 9 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
10 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 796748) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x0000000000010d69 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
 8 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
 9 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
10 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 796749) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x0000000000010d69 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
 8 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
 9 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
10 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 796751) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x0000000000010d69 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
 8 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
 9 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
10 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 796750) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x0000000000010d69 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
 8 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
 9 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
10 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec noticed that process rank 14 with PID 796754 on node uc2n517 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n517:797108:0:797108] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14e462340000)
[uc2n517:797107:0:797107] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14bd7a300000)
[uc2n517:797110:0:797110] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x150bd83c0000)
[uc2n517:797103:0:797103] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x15167e200000)
[uc2n517:797105:0:797105] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x148a3a280000)
[uc2n517:797109:0:797109] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14f3a6380000)
[uc2n517:797106:0:797106] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x1453062c0000)
[uc2n517:797104:0:797104] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14cac6240000)
==== backtrace (tid: 797107) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x0000000000010d69 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
 8 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
 9 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
10 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 797108) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x0000000000010d69 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
 8 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
 9 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
10 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 797110) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x0000000000010d69 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
 8 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
 9 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
10 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 797109) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x0000000000010d69 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
 8 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
 9 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
10 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 797105) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x0000000000010d69 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
 8 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
 9 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
10 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 797106) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x0000000000010d69 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
 8 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
 9 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
10 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 797103) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x0000000000010d69 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
 8 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
 9 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
10 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 797104) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x0000000000010d69 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
 8 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
 9 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
10 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
[uc2n515.localdomain:1162432] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1162432] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec noticed that process rank 8 with PID 797103 on node uc2n517 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1162709] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1162709] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1162986] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1162986] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1163253] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1163253] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1163542] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1163542] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1163825] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1163825] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1164106] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1164106] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1164373] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1164373] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1164637] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1164637] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1164902] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1164902] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1165183] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1165183] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1165453] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1165453] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1165720] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1165720] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1165986] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1165986] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1166285] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1166285] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1166568] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1166568] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1166832] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1166832] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1167109] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1167109] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1167373] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1167373] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1167642] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1167642] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1167905] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1167905] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1168188] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1168188] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1168457] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1168457] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1168726] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1168726] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1169027] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1169027] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1169312] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1169312] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1169579] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1169579] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1169846] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1169846] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1170126] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1170126] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1170393] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1170393] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1170660] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1170660] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1170941] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1170941] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1171218] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1171218] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1171503] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1171503] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1171809] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1171809] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1172095] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1172095] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1172383] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1172383] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1172662] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1172662] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1172932] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1172932] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1173216] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1173216] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1173490] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1173490] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1173772] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1173772] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1174119] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1174119] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[16846,1],7]
  Exit code:    1
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1174444] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1174444] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1174825] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1174825] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[18048,1],14]
  Exit code:    1
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1175145] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1175145] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1175511] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1175511] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1175852] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1175852] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n517:810829:0:810829] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x151c3c380000)
==== backtrace (tid: 810829) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001069f MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
 9 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
10 0x0000000000019625 Tests_Slab_Random_Default<double>::testcase4()  ???:0
11 0x0000000000016c8d Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
[uc2n517:810830:0:810830] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14f5d03c0000)
==== backtrace (tid: 810830) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001069f MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
 9 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
10 0x0000000000019625 Tests_Slab_Random_Default<double>::testcase4()  ???:0
11 0x0000000000016c8d Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
[uc2n517:810824:0:810824] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x148ed4240000)
[uc2n517:810823:0:810823] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x147824200000)
==== backtrace (tid: 810824) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001069f MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
 9 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
10 0x0000000000019625 Tests_Slab_Random_Default<double>::testcase4()  ???:0
11 0x0000000000016c8d Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 810823) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001069f MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
 9 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
10 0x0000000000019625 Tests_Slab_Random_Default<double>::testcase4()  ???:0
11 0x0000000000016c8d Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec noticed that process rank 14 with PID 810829 on node uc2n517 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1176178] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1176178] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[23003,1],7]
  Exit code:    1
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1176518] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1176518] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n517:811544:0:811544] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x151cd6380000)
==== backtrace (tid: 811544) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x0000000000010d69 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
 8 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
 9 0x0000000000019625 Tests_Slab_Random_Default<double>::testcase4()  ???:0
10 0x0000000000016c8d Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
[uc2n517:811545:0:811545] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14ce9c3c0000)
==== backtrace (tid: 811545) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x0000000000010d69 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
 8 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
 9 0x0000000000019625 Tests_Slab_Random_Default<double>::testcase4()  ???:0
10 0x0000000000016c8d Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
[uc2n517:811539:0:811539] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x144db6240000)
[uc2n517:811538:0:811538] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x145cf4200000)
==== backtrace (tid: 811539) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x0000000000010d69 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
 8 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
 9 0x0000000000019625 Tests_Slab_Random_Default<double>::testcase4()  ???:0
10 0x0000000000016c8d Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 811538) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x0000000000010d69 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
 8 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
 9 0x0000000000019625 Tests_Slab_Random_Default<double>::testcase4()  ???:0
10 0x0000000000016c8d Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec noticed that process rank 14 with PID 811544 on node uc2n517 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1176902] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1176902] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n517:811971:0:811971] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x147e56380000)
==== backtrace (tid: 811971) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x0000000000010d69 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
 8 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
 9 0x0000000000019625 Tests_Slab_Random_Default<double>::testcase4()  ???:0
10 0x0000000000016c8d Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
[uc2n517:811972:0:811972] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x147ad23c0000)
==== backtrace (tid: 811972) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x0000000000010d69 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
 8 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
 9 0x0000000000019625 Tests_Slab_Random_Default<double>::testcase4()  ???:0
10 0x0000000000016c8d Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
[uc2n517:811966:0:811966] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14c932240000)
[uc2n517:811965:0:811965] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x145aba200000)
==== backtrace (tid: 811966) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x0000000000010d69 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
 8 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
 9 0x0000000000019625 Tests_Slab_Random_Default<double>::testcase4()  ???:0
10 0x0000000000016c8d Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 811965) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x0000000000010d69 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
 8 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
 9 0x0000000000019625 Tests_Slab_Random_Default<double>::testcase4()  ???:0
10 0x0000000000016c8d Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
mpiexec noticed that process rank 14 with PID 811971 on node uc2n517 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
Starting computation for size 128
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 128
2021-09-19 02:02:01.372784
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 128
2021-09-19 02:02:12.490551
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 128
2021-09-19 02:02:20.665044
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 128
2021-09-19 02:02:28.153048
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 128
2021-09-19 02:02:36.288751
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 128
2021-09-19 02:02:44.150490
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 128
2021-09-19 02:02:52.415731
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 128
2021-09-19 02:02:59.837493
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 128
2021-09-19 02:03:07.867977
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 128
2021-09-19 02:03:15.298900
b''

Starting computation for size [128, 128, 256]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 256
2021-09-19 02:03:23.550050
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 256
2021-09-19 02:03:31.003166
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 256
2021-09-19 02:03:39.095376
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 256
2021-09-19 02:03:46.535025
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 256
2021-09-19 02:03:54.687466
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 256
2021-09-19 02:04:02.180256
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 256
2021-09-19 02:04:10.191022
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 256
2021-09-19 02:04:17.721880
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 256
2021-09-19 02:04:26.344749
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 256
2021-09-19 02:04:33.808517
b''

Starting computation for size [128, 256, 256]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 256 -nz 256
2021-09-19 02:04:42.034706
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 256 -nz 256
2021-09-19 02:04:49.484112
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 256 -nz 256
2021-09-19 02:04:58.142061
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 256 -nz 256
2021-09-19 02:05:05.666418
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 256 -nz 256
2021-09-19 02:05:13.852113
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 256 -nz 256
2021-09-19 02:05:21.452242
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 256 -nz 256
2021-09-19 02:05:29.674826
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 256 -nz 256
2021-09-19 02:05:37.255999
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 256 -nz 256
2021-09-19 02:05:46.314649
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 256 -nz 256
2021-09-19 02:05:54.084607
b''

Starting computation for size 256
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 256
2021-09-19 02:06:02.610405
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 256
2021-09-19 02:06:10.212394
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 256
2021-09-19 02:06:18.762995
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 256
2021-09-19 02:06:26.279433
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 256
2021-09-19 02:06:34.538031
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 256
2021-09-19 02:06:42.148942
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 256
2021-09-19 02:06:50.378907
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 256
2021-09-19 02:06:58.005287
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 256
2021-09-19 02:07:06.129333
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 256
2021-09-19 02:07:13.757066
b''

Starting computation for size [256, 256, 512]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 512
2021-09-19 02:07:22.268025
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 512
2021-09-19 02:07:30.038036
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 512
2021-09-19 02:07:38.546862
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 512
2021-09-19 02:07:46.178713
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 512
2021-09-19 02:07:54.554256
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 512
2021-09-19 02:08:02.333924
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 512
2021-09-19 02:08:10.687246
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 512
2021-09-19 02:08:18.406233
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 512
2021-09-19 02:08:26.601445
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 512
2021-09-19 02:08:34.484628
b''

Starting computation for size [256, 512, 512]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 512 -nz 512
2021-09-19 02:08:42.869656
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 512 -nz 512
2021-09-19 02:08:50.970376
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 512 -nz 512
2021-09-19 02:08:59.488377
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 512 -nz 512
2021-09-19 02:09:07.494722
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 512 -nz 512
2021-09-19 02:09:15.962046
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 512 -nz 512
2021-09-19 02:09:24.118209
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 512 -nz 512
2021-09-19 02:09:32.726958
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 512 -nz 512
2021-09-19 02:09:44.338914
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 512 -nz 512
2021-09-19 02:09:52.842326
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 512 -nz 512
2021-09-19 02:10:01.510528
b''

Starting computation for size 512
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 512
2021-09-19 02:10:10.000270
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 512
2021-09-19 02:10:18.941317
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 512
2021-09-19 02:10:27.834284
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 512
2021-09-19 02:10:36.829437
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 512
2021-09-19 02:10:45.743989
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 512
2021-09-19 02:10:54.693564
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 512
2021-09-19 02:11:03.678741
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 512
2021-09-19 02:11:12.459690
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 512
2021-09-19 02:11:21.414247
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 512
2021-09-19 02:11:30.544074
b''

Starting computation for size [512, 512, 1024]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 1024
2021-09-19 02:11:39.555961
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 1024
2021-09-19 02:11:50.342509
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 1024
2021-09-19 02:12:00.232885
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 1024
2021-09-19 02:12:10.396897
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 1024
2021-09-19 02:12:20.015876
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 1024
2021-09-19 02:12:30.517863
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 1024
2021-09-19 02:12:40.232465
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 1024
2021-09-19 02:12:50.589210
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 1024
2021-09-19 02:13:00.319405
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 1024
2021-09-19 02:13:11.288062
b''

Starting computation for size [512, 1024, 1024]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 1024 -nz 1024
2021-09-19 02:13:21.158108
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 1024 -nz 1024
2021-09-19 02:13:34.636262
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 1024 -nz 1024
2021-09-19 02:13:45.939675
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 1024 -nz 1024
2021-09-19 02:13:59.040043
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 1024 -nz 1024
2021-09-19 02:14:10.389608
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 1024 -nz 1024
2021-09-19 02:14:24.162139
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 1024 -nz 1024
2021-09-19 02:14:35.489200
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 1024 -nz 1024
2021-09-19 02:14:49.113060
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 1024 -nz 1024
2021-09-19 02:15:00.402364
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 1024 -nz 1024
2021-09-19 02:15:15.173819
b''

Starting computation for size 1024
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 1024
2021-09-19 02:15:26.830135
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 1024
2021-09-19 02:15:46.094348
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 1024
2021-09-19 02:16:00.511800
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 1024
2021-09-19 02:16:19.380778
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 1024
2021-09-19 02:16:33.881529
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 1024
2021-09-19 02:16:54.152603
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 1024
2021-09-19 02:17:08.393174
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 1024
2021-09-19 02:17:28.091418
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 1024
2021-09-19 02:17:42.600513
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 1024
2021-09-19 02:18:04.640735
b''

Starting computation for size [1024, 1024, 2048]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 2048
2021-09-19 02:18:19.714953
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 2048
2021-09-19 02:18:51.068456
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 2048
2021-09-19 02:19:12.530315
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 2048
2021-09-19 02:19:43.178463
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 2048
2021-09-19 02:20:04.186050
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 2048
2021-09-19 02:20:37.184556
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 2048
2021-09-19 02:20:58.344668
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 2048
2021-09-19 02:21:30.936138
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 2048
2021-09-19 02:21:52.510060
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 2048
2021-09-19 02:22:28.613232
b''

Starting computation for size [1024, 2048, 2048]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 2048 -nz 2048
2021-09-19 02:22:51.100275
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 2048 -nz 2048
2021-09-19 02:23:46.703398
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 2048 -nz 2048
2021-09-19 02:24:20.568038
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 2048 -nz 2048
2021-09-19 02:25:14.628180
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 2048 -nz 2048
2021-09-19 02:25:48.952088
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 2048 -nz 2048
2021-09-19 02:26:46.088716
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 2048 -nz 2048
2021-09-19 02:27:20.423910
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 2048 -nz 2048
2021-09-19 02:28:17.860524
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 2048 -nz 2048
2021-09-19 02:28:52.312261
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 2048 -nz 2048
2021-09-19 02:29:56.563432
b''

Starting computation for size 2048
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 2048 -ny 2048 -nz 2048
2021-09-19 02:30:34.319419
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 2048 -ny 2048 -nz 2048
2021-09-19 02:32:17.595589
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 2048 -ny 2048 -nz 2048
2021-09-19 02:33:17.571697
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 2048 -ny 2048 -nz 2048
2021-09-19 02:34:58.521259
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 2048 -ny 2048 -nz 2048
2021-09-19 02:35:57.980607
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 2048 -ny 2048 -nz 2048
2021-09-19 02:37:43.118034
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 2048 -ny 2048 -nz 2048
2021-09-19 02:38:42.035832
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 2048 -ny 2048 -nz 2048
2021-09-19 02:38:58.264021
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 2048 -ny 2048 -nz 2048
2021-09-19 02:39:14.349587
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 2048 -ny 2048 -nz 2048
2021-09-19 02:39:30.554339
b''

Starting computation for size 128
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 02:39:47.335994
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 02:39:56.697699
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 02:40:05.124764
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 02:40:12.841554
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 02:40:20.906064
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 02:40:28.763625
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 02:40:37.229344
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 02:40:45.050899
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 02:40:53.222530
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 02:41:01.028486
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

Starting computation for size 256
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 02:41:09.308028
b'Result (avg): 5.01735e-09\nResult (max): 5.42241e-08\n'

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 02:41:17.249989
b'Result (avg): 5.01735e-09\nResult (max): 5.42241e-08\n'

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 02:41:25.618989
b'Result (avg): 5.01735e-09\nResult (max): 5.42241e-08\n'

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 02:41:33.478337
b'Result (avg): 5.01735e-09\nResult (max): 5.42241e-08\n'

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 02:41:41.651253
b'Result (avg): 5.01735e-09\nResult (max): 5.42241e-08\n'

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 02:41:49.651975
b'Result (avg): 5.03363e-09\nResult (max): 6.42158e-08\n'

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 02:41:57.941866
b'Result (avg): 5.01735e-09\nResult (max): 5.42241e-08\n'

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 02:42:05.899020
b'Result (avg): 5.01735e-09\nResult (max): 5.42241e-08\n'

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 02:42:14.026867
b'Result (avg): 5.01735e-09\nResult (max): 5.42241e-08\n'

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 02:42:21.998922
b'Result (avg): 5.01735e-09\nResult (max): 5.42241e-08\n'

Starting computation for size 512
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 02:42:30.222249
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 02:42:39.173599
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 02:42:48.362309
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 02:42:57.390121
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 02:43:06.659056
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 02:43:15.923066
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 02:43:25.138805
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 02:43:34.219162
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 02:43:43.893543
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 02:43:53.023440
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

Starting computation for size 1024
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 02:44:02.250850
b'Result (avg): 7.38396e-07\nResult (max): 9.23959e-06\n'

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 02:44:21.690218
b'Result (avg): 7.38396e-07\nResult (max): 9.23959e-06\n'

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 02:44:40.555413
b'Result (avg): 7.38396e-07\nResult (max): 9.23959e-06\n'

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 02:44:59.673474
b'Result (avg): 7.38396e-07\nResult (max): 9.23959e-06\n'

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 02:45:18.635576
b'Result (avg): 7.38396e-07\nResult (max): 9.23959e-06\n'

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 02:45:38.161459
b'Result (avg): 2.41896e-06\nResult (max): 0.000158774\n'

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 02:45:56.894695
b'Result (avg): 7.38396e-07\nResult (max): 9.23959e-06\n'

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 02:46:16.197743
b'Result (avg): 7.38396e-07\nResult (max): 9.23959e-06\n'

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 02:46:34.819891
b'Result (avg): 7.38396e-07\nResult (max): 9.23959e-06\n'

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 02:46:54.431372
b'Result (avg): 7.38396e-07\nResult (max): 9.23959e-06\n'

Starting computation for size 2048
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 02:47:13.095016
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 02:48:54.023992
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 02:50:09.337064
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 02:51:56.651242
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 02:53:11.834834
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 02:55:09.737681
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 02:56:45.261173
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 02:58:07.511115
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 02:59:33.425463
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 03:01:02.849037
b''

Slab 2D->1D opt1
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1177246] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1177246] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1177515] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1177515] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1177785] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1177785] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1178306] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1178306] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1178575] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1178575] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1178842] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1178842] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1179124] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1179124] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1179390] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1179390] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1179657] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1179657] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1179920] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1179920] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1180197] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1180197] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1180466] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1180466] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1180732] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1180732] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1181249] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1181249] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1181524] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1181524] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1181791] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1181791] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1182058] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1182058] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1182341] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1182341] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1182607] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1182607] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1182873] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1182873] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1183150] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1183150] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1183417] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1183417] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1183686] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1183686] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1184198] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1184198] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1184475] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1184475] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1184741] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1184741] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1185008] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1185008] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1185291] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1185291] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1185558] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1185558] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1185824] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1185824] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1186105] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1186105] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1186374] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1186374] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1186639] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1186639] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1187165] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1187165] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1187430] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1187430] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1187699] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1187699] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1187977] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1187977] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1188247] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1188247] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1188516] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1188516] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1188782] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1188782] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1189065] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1189065] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1189335] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1189335] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1189598] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1189598] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1190125] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1190125] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1190393] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1190393] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1190660] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1190660] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1190947] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1190947] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1191216] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1191216] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1191483] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1191483] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1191760] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1191760] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1192032] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1192032] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1192308] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1192308] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1192578] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1192578] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1193094] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1193094] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1193361] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1193361] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1193640] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1193640] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1193931] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1193931] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1194198] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1194198] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1194467] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1194467] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1194744] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1194744] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1195041] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1195041] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1195308] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1195308] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1195589] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1195589] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1196104] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1196104] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1196373] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1196373] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1196643] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1196643] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1196944] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1196944] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1197212] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1197212] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1197481] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1197481] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1197762] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1197762] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1198052] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1198052] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1198320] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1198320] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1198589] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1198589] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1199118] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1199118] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1199387] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1199387] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1199665] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1199665] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1199981] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1199981] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1200253] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1200253] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1200520] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1200520] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1200799] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1200799] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1201117] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1201117] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1201387] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1201387] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1201667] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1201667] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1202180] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1202180] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1202459] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1202459] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1202730] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1202730] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1203096] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1203096] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1203365] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1203365] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1203646] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1203646] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1203913] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1203913] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1204277] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1204277] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1204568] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1204568] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1204850] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1204850] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1205369] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1205369] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1205649] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1205649] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1205923] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1205923] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1206289] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1206289] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1206569] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1206569] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1206852] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1206852] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1207128] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1207128] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1207501] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1207501] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1207794] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1207794] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1208079] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1208079] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1208619] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1208619] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1208897] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1208897] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1209199] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1209199] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1209639] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1209639] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1209930] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1209930] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1210218] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1210218] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1210522] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1210522] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1210987] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1210987] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1211302] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1211302] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1211596] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1211596] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1212247] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1212247] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1212537] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1212537] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1212867] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1212867] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1213495] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1213495] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1213810] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1213810] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1214115] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1214115] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1214435] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1214435] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1215133] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1215133] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1215499] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1215499] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1215818] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1215818] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1216410] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1216410] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1216739] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1216739] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1217103] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1217103] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1217770] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1217770] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n517:853477:0:853477] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x1462ac3c0000)
[uc2n517:853476:0:853476] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x147720380000)
[uc2n517:853471:0:853471] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x15322a240000)
[uc2n517:853473:0:853473] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x151f202c0000)
[uc2n517:853472:0:853472] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x152080280000)
[uc2n517:853474:0:853474] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x145544300000)
[uc2n517:853470:0:853470] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x1459dc200000)
[uc2n517:853475:0:853475] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14a4c0340000)
[uc2n515:1217791:0:1217791] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14ae4c200000)
[uc2n515:1217785:0:1217785] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14f880200000)
[uc2n515:1217787:0:1217787] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x1490f4200000)
[uc2n515:1217786:0:1217786] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x1534fe200000)
[uc2n515:1217788:0:1217788] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x153e4c200000)
[uc2n515:1217789:0:1217789] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x154590200000)
[uc2n515:1217790:0:1217790] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14dd26200000)
[uc2n515:1217784:0:1217784] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14d12c200000)
==== backtrace (tid:1217788) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x000000000001eb28 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
11 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
12 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
13 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
14 0x000000000040331f main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1217789) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x000000000001eb28 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
11 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
12 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
13 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
14 0x000000000040331f main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1217790) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x000000000001eb28 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
11 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
12 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
13 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
14 0x000000000040331f main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 853476) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001eb28 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
 9 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
10 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
11 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 853477) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001eb28 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
 9 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
10 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
11 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1217791) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x000000000001eb28 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
11 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
12 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
13 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
14 0x000000000040331f main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 853474) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001eb28 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
 9 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
10 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
11 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 853473) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001eb28 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
 9 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
10 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
11 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 853471) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001eb28 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
 9 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
10 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
11 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 853472) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001eb28 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
 9 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
10 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
11 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 853470) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001eb28 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
 9 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
10 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
11 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 853475) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001eb28 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
 9 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
10 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
11 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1217786) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x000000000001eb28 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
11 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
12 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
13 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
14 0x000000000040331f main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1217784) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x000000000001eb28 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
11 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
12 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
13 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
14 0x000000000040331f main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1217785) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x000000000001eb28 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
11 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
12 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
13 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
14 0x000000000040331f main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1217787) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x000000000001eb28 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
11 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
12 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
13 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
14 0x000000000040331f main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec noticed that process rank 2 with PID 0 on node uc2n515 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1218108] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1218108] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
The call to cuMemcpyAsync failed. This is a unrecoverable error and will
cause the program to abort.
  cuMemcpyAsync(0x15484b000000, 0x154a52c00000, 131072) returned value 1
Check the cuda.h file for what the return value means.
--------------------------------------------------------------------------
[uc2n517.localdomain:853837] CUDA: Error in cuMemcpy: res=-1, dest=0x15484b000000, src=0x154a52c00000, size=131072
[uc2n517:853837] *** Process received signal ***
[uc2n517:853837] Signal: Aborted (6)
[uc2n517:853837] Signal code:  (-6)
[uc2n517:853837] [ 0] /usr/lib64/libpthread.so.0(+0x12dd0)[0x154cc2b64dd0]
[uc2n517:853837] [ 1] /usr/lib64/libc.so.6(gsignal+0x10f)[0x154cc27c770f]
[uc2n517:853837] [ 2] /usr/lib64/libc.so.6(abort+0x127)[0x154cc27b1b25]
[uc2n517:853837] [ 3] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libopen-pal.so.40(+0x7c58f)[0x154cc173158f]
[uc2n517:853837] [ 4] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libopen-pal.so.40(+0x77119)[0x154cc172c119]
[uc2n517:853837] [ 5] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libmpi.so.40(ompi_datatype_sndrcv+0x50a)[0x154cce56ab2a]
[uc2n517:853837] [ 6] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libmpi.so.40(ompi_coll_base_alltoallv_intra_basic_linear+0x26e)[0x154cce5bf80e]
[uc2n517:853837] [ 7] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libmpi.so.40(ompi_coll_tuned_alltoallv_intra_dec_fixed+0x42)[0x154cce5cb702]
[uc2n517:853837] [ 8] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libmpi.so.40(MPI_Alltoallv+0x1ab)[0x154cce56d42b]
[uc2n517:853837] [ 9] /home/st/st_us-051200/st_st160727/DistributedFFT/build/libslab_decomp.so(_ZN18MPIcuFFT_Slab_Opt1IdE12All2All_SyncEPvb+0xf8)[0x154cd8cbab28]
[uc2n517:853837] [10] /home/st/st_us-051200/st_st160727/DistributedFFT/build/libslab_decomp.so(_ZN18MPIcuFFT_Slab_Opt1IdE7execR2CEPvPKv+0x122)[0x154cd8cba352]
[uc2n517:853837] [11] /home/st/st_us-051200/st_st160727/DistributedFFT/build/libslab_tests.so(_ZN25Tests_Slab_Random_DefaultIdE9testcase0Eii+0x518)[0x154cd8f071ae]
[uc2n517:853837] [12] /home/st/st_us-051200/st_st160727/DistributedFFT/build/libslab_tests.so(_ZN25Tests_Slab_Random_DefaultIdE3runEiii+0x2f)[0x154cd8f06c1d]
[uc2n517:853837] [13] slab[0x40331f]
[uc2n517:853837] [14] /usr/lib64/libc.so.6(__libc_start_main+0xf3)[0x154cc27b36a3]
[uc2n517:853837] [15] slab[0x4039ee]
[uc2n517:853837] *** End of error message ***
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec noticed that process rank 15 with PID 853837 on node uc2n517 exited on signal 6 (Aborted).
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1218380] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1218380] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n517:854121:0:854121] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14e9a83c0000)
[uc2n517:854120:0:854120] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x150a8c380000)
[uc2n517:854114:0:854114] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14584c200000)
[uc2n517:854115:0:854115] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x153284240000)
[uc2n517:854118:0:854118] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14833c300000)
[uc2n517:854119:0:854119] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14cb2e340000)
[uc2n517:854117:0:854117] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14783c2c0000)
[uc2n517:854116:0:854116] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14f286280000)
==== backtrace (tid: 854121) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x000000000008d803 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000001f2c9 MPIcuFFT_Slab_Opt1<double>::All2All_MPIType()  ???:0
 8 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
 9 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
10 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 854120) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x000000000008d803 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000001f2c9 MPIcuFFT_Slab_Opt1<double>::All2All_MPIType()  ???:0
 8 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
 9 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
10 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 854114) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x000000000008d803 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000001f2c9 MPIcuFFT_Slab_Opt1<double>::All2All_MPIType()  ???:0
 8 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
 9 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
10 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 854115) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x000000000008d803 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000001f2c9 MPIcuFFT_Slab_Opt1<double>::All2All_MPIType()  ???:0
 8 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
 9 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
10 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 854119) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x000000000008d803 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000001f2c9 MPIcuFFT_Slab_Opt1<double>::All2All_MPIType()  ???:0
 8 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
 9 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
10 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 854118) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x000000000008d803 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000001f2c9 MPIcuFFT_Slab_Opt1<double>::All2All_MPIType()  ???:0
 8 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
 9 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
10 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 854117) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x000000000008d803 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000001f2c9 MPIcuFFT_Slab_Opt1<double>::All2All_MPIType()  ???:0
 8 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
 9 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
10 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 854116) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x000000000008d803 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000001f2c9 MPIcuFFT_Slab_Opt1<double>::All2All_MPIType()  ???:0
 8 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
 9 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
10 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
[uc2n515:1218400:0:1218400] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14ac6c200000)
[uc2n515:1218393:0:1218393] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14a7f4200000)
[uc2n515:1218399:0:1218399] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x1477ac200000)
[uc2n515:1218394:0:1218394] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x145d6c200000)
[uc2n515:1218397:0:1218397] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14c5cc200000)
[uc2n515:1218398:0:1218398] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x1540bc200000)
[uc2n515:1218395:0:1218395] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14634c200000)
[uc2n515:1218396:0:1218396] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14738c200000)
==== backtrace (tid:1218400) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x0000000000121b20 mca_coll_basic_alltoallw_intra()  ???:0
 8 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 9 0x000000000001f2c9 MPIcuFFT_Slab_Opt1<double>::All2All_MPIType()  ???:0
10 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
11 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
12 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
13 0x000000000040331f main()  ???:0
14 0x00000000000236a3 __libc_start_main()  ???:0
15 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1218393) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x0000000000121b20 mca_coll_basic_alltoallw_intra()  ???:0
 8 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 9 0x000000000001f2c9 MPIcuFFT_Slab_Opt1<double>::All2All_MPIType()  ???:0
10 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
11 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
12 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
13 0x000000000040331f main()  ???:0
14 0x00000000000236a3 __libc_start_main()  ???:0
15 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1218399) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x0000000000121b20 mca_coll_basic_alltoallw_intra()  ???:0
 8 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 9 0x000000000001f2c9 MPIcuFFT_Slab_Opt1<double>::All2All_MPIType()  ???:0
10 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
11 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
12 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
13 0x000000000040331f main()  ???:0
14 0x00000000000236a3 __libc_start_main()  ???:0
15 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1218397) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x0000000000121b20 mca_coll_basic_alltoallw_intra()  ???:0
 8 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 9 0x000000000001f2c9 MPIcuFFT_Slab_Opt1<double>::All2All_MPIType()  ???:0
10 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
11 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
12 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
13 0x000000000040331f main()  ???:0
14 0x00000000000236a3 __libc_start_main()  ???:0
15 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1218398) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x0000000000121b20 mca_coll_basic_alltoallw_intra()  ???:0
 8 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 9 0x000000000001f2c9 MPIcuFFT_Slab_Opt1<double>::All2All_MPIType()  ???:0
10 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
11 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
12 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
13 0x000000000040331f main()  ???:0
14 0x00000000000236a3 __libc_start_main()  ???:0
15 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1218394) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x0000000000121b20 mca_coll_basic_alltoallw_intra()  ???:0
 8 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 9 0x000000000001f2c9 MPIcuFFT_Slab_Opt1<double>::All2All_MPIType()  ???:0
10 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
11 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
12 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
13 0x000000000040331f main()  ???:0
14 0x00000000000236a3 __libc_start_main()  ???:0
15 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1218395) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x0000000000121b20 mca_coll_basic_alltoallw_intra()  ???:0
 8 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 9 0x000000000001f2c9 MPIcuFFT_Slab_Opt1<double>::All2All_MPIType()  ???:0
10 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
11 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
12 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
13 0x000000000040331f main()  ???:0
14 0x00000000000236a3 __libc_start_main()  ???:0
15 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1218396) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x0000000000121b20 mca_coll_basic_alltoallw_intra()  ???:0
 8 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 9 0x000000000001f2c9 MPIcuFFT_Slab_Opt1<double>::All2All_MPIType()  ???:0
10 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
11 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
12 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
13 0x000000000040331f main()  ???:0
14 0x00000000000236a3 __libc_start_main()  ???:0
15 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec noticed that process rank 15 with PID 854121 on node uc2n517 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1218728] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1218728] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
The call to cuMemcpyAsync failed. This is a unrecoverable error and will
cause the program to abort.
  cuMemcpyAsync(0x8853830, 0x14cbe2800000, 65536) returned value 1
Check the cuda.h file for what the return value means.
--------------------------------------------------------------------------
[uc2n517.localdomain:854473] CUDA: Error in cuMemcpy: res=-1, dest=0x8853830, src=0x14cbe2800000, size=65536
[uc2n517:854473] *** Process received signal ***
[uc2n517:854473] Signal: Aborted (6)
[uc2n517:854473] Signal code:  (-6)
[uc2n517:854473] [ 0] /usr/lib64/libpthread.so.0(+0x12dd0)[0x14ce5614ddd0]
[uc2n517:854473] [ 1] /usr/lib64/libc.so.6(gsignal+0x10f)[0x14ce55db070f]
[uc2n517:854473] [ 2] /usr/lib64/libc.so.6(abort+0x127)[0x14ce55d9ab25]
[uc2n517:854473] [ 3] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libopen-pal.so.40(+0x7c375)[0x14ce54d1a375]
[uc2n517:854473] [ 4] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libopen-pal.so.40(opal_convertor_pack+0xd8)[0x14ce54d111e8]
[uc2n517:854473] [ 5] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libmpi.so.40(ompi_datatype_sndrcv+0x1e3)[0x14ce61b53803]
[uc2n517:854473] [ 6] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libmpi.so.40(mca_coll_basic_alltoallw_intra+0xa4)[0x14ce61be7984]
[uc2n517:854473] [ 7] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libmpi.so.40(MPI_Alltoallw+0x1eb)[0x14ce61b56ccb]
[uc2n517:854473] [ 8] /home/st/st_us-051200/st_st160727/DistributedFFT/build/libslab_decomp.so(_ZN18MPIcuFFT_Slab_Opt1IdE15All2All_MPITypeEPvb+0x149)[0x14ce6c2a42c9]
[uc2n517:854473] [ 9] /home/st/st_us-051200/st_st160727/DistributedFFT/build/libslab_decomp.so(_ZN18MPIcuFFT_Slab_Opt1IdE7execR2CEPvPKv+0x122)[0x14ce6c2a3352]
[uc2n517:854473] [10] /home/st/st_us-051200/st_st160727/DistributedFFT/build/libslab_tests.so(_ZN25Tests_Slab_Random_DefaultIdE9testcase0Eii+0x518)[0x14ce6c4f01ae]
[uc2n517:854473] [11] /home/st/st_us-051200/st_st160727/DistributedFFT/build/libslab_tests.so(_ZN25Tests_Slab_Random_DefaultIdE3runEiii+0x2f)[0x14ce6c4efc1d]
[uc2n517:854473] [12] slab[0x40331f]
[uc2n517:854473] [13] /usr/lib64/libc.so.6(__libc_start_main+0xf3)[0x14ce55d9c6a3]
[uc2n517:854473] [14] slab[0x4039ee]
[uc2n517:854473] *** End of error message ***
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec noticed that process rank 15 with PID 854473 on node uc2n517 exited on signal 6 (Aborted).
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1218997] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1218997] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1219264] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1219264] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1219543] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1219543] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1219834] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1219834] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1220114] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1220114] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1220393] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1220393] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1220659] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1220659] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1220928] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1220928] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1221195] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1221195] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1221471] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1221471] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1221736] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1221736] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1222005] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1222005] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1222272] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1222272] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1222581] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1222581] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1222864] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1222864] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1223130] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1223130] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1223401] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1223401] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1223678] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1223678] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1223944] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1223944] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1224211] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1224211] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1224489] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1224489] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1224754] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1224754] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1225023] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1225023] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1225314] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1225314] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1225611] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1225611] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1225882] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1225882] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1226151] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1226151] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1226430] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1226430] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1226701] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1226701] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1226972] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1226972] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1227251] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1227251] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1227523] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1227523] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1227810] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1227810] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1228109] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1228109] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1228396] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1228396] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1228682] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1228682] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1228968] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1228968] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1229241] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1229241] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1229522] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1229522] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1229802] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1229802] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1230074] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1230074] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1230432] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1230432] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[28106,1],7]
  Exit code:    1
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1230754] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1230754] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1231138] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1231138] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[25224,1],14]
  Exit code:    1
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1231444] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1231444] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1231808] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1231808] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1232172] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1232172] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n517:868166:0:868166] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x147d68380000)
[uc2n515:1232192:0:1232192] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14a150200000)
==== backtrace (tid:1232192) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x000000000001eb28 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
11 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
12 0x0000000000019625 Tests_Slab_Random_Default<double>::testcase4()  ???:0
13 0x0000000000016c8d Tests_Slab_Random_Default<double>::run()  ???:0
14 0x000000000040331f main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 868166) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001eb28 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
 9 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
10 0x0000000000019625 Tests_Slab_Random_Default<double>::testcase4()  ???:0
11 0x0000000000016c8d Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
[uc2n517:868167:0:868167] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x1526883c0000)
==== backtrace (tid: 868167) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001eb28 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
 9 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
10 0x0000000000019625 Tests_Slab_Random_Default<double>::testcase4()  ???:0
11 0x0000000000016c8d Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
[uc2n515:1232191:0:1232191] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x15258c200000)
==== backtrace (tid:1232191) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x000000000001eb28 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
11 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
12 0x0000000000019625 Tests_Slab_Random_Default<double>::testcase4()  ???:0
13 0x0000000000016c8d Tests_Slab_Random_Default<double>::run()  ???:0
14 0x000000000040331f main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
[uc2n517:868161:0:868161] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14b584240000)
[uc2n517:868160:0:868160] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x15483c200000)
==== backtrace (tid: 868161) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001eb28 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
 9 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
10 0x0000000000019625 Tests_Slab_Random_Default<double>::testcase4()  ???:0
11 0x0000000000016c8d Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 868160) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001eb28 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
 9 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
10 0x0000000000019625 Tests_Slab_Random_Default<double>::testcase4()  ???:0
11 0x0000000000016c8d Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
[uc2n515:1232185:0:1232185] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14d906200000)
==== backtrace (tid:1232185) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x000000000001eb28 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
11 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
12 0x0000000000019625 Tests_Slab_Random_Default<double>::testcase4()  ???:0
13 0x0000000000016c8d Tests_Slab_Random_Default<double>::run()  ???:0
14 0x000000000040331f main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
mpiexec noticed that process rank 7 with PID 0 on node uc2n515 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1232531] [[25913,0],0] ORTE_ERROR_LOG: Data unpack would read past end of buffer in file util/show_help.c at line 501
[uc2n515.localdomain:1232531] 14 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1232531] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[25913,1],7]
  Exit code:    1
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1232867] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1232867] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n517:868887:0:868887] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14e6a4380000)
[uc2n515:1232894:0:1232894] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14d844200000)
==== backtrace (tid: 868887) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x000000000008d803 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000001f2c9 MPIcuFFT_Slab_Opt1<double>::All2All_MPIType()  ???:0
 8 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
 9 0x0000000000019625 Tests_Slab_Random_Default<double>::testcase4()  ???:0
10 0x0000000000016c8d Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1232894) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x0000000000121b20 mca_coll_basic_alltoallw_intra()  ???:0
 8 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 9 0x000000000001f2c9 MPIcuFFT_Slab_Opt1<double>::All2All_MPIType()  ???:0
10 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
11 0x0000000000019625 Tests_Slab_Random_Default<double>::testcase4()  ???:0
12 0x0000000000016c8d Tests_Slab_Random_Default<double>::run()  ???:0
13 0x000000000040331f main()  ???:0
14 0x00000000000236a3 __libc_start_main()  ???:0
15 0x00000000004039ee _start()  ???:0
=================================
[uc2n517:868888:0:868888] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14bf583c0000)
==== backtrace (tid: 868888) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x000000000008d803 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000001f2c9 MPIcuFFT_Slab_Opt1<double>::All2All_MPIType()  ???:0
 8 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
 9 0x0000000000019625 Tests_Slab_Random_Default<double>::testcase4()  ???:0
10 0x0000000000016c8d Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
[uc2n517:868881:0:868881] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x147dd8200000)
[uc2n517:868882:0:868882] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x148e9c240000)
==== backtrace (tid: 868881) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x000000000008d803 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000001f2c9 MPIcuFFT_Slab_Opt1<double>::All2All_MPIType()  ???:0
 8 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
 9 0x0000000000019625 Tests_Slab_Random_Default<double>::testcase4()  ???:0
10 0x0000000000016c8d Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 868882) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x000000000008d803 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000001f2c9 MPIcuFFT_Slab_Opt1<double>::All2All_MPIType()  ???:0
 8 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
 9 0x0000000000019625 Tests_Slab_Random_Default<double>::testcase4()  ???:0
10 0x0000000000016c8d Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
[uc2n515:1232893:0:1232893] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14cc66200000)
==== backtrace (tid:1232893) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x0000000000121b20 mca_coll_basic_alltoallw_intra()  ???:0
 8 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 9 0x000000000001f2c9 MPIcuFFT_Slab_Opt1<double>::All2All_MPIType()  ???:0
10 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
11 0x0000000000019625 Tests_Slab_Random_Default<double>::testcase4()  ???:0
12 0x0000000000016c8d Tests_Slab_Random_Default<double>::run()  ???:0
13 0x000000000040331f main()  ???:0
14 0x00000000000236a3 __libc_start_main()  ???:0
15 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec noticed that process rank 14 with PID 868887 on node uc2n517 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1233221] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1233221] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
The call to cuMemcpyAsync failed. This is a unrecoverable error and will
cause the program to abort.
  cuMemcpyAsync(0x149f04609178, 0x149ae1ff03c0, 131040) returned value 1
Check the cuda.h file for what the return value means.
--------------------------------------------------------------------------
[uc2n517.localdomain:869272] CUDA: Error in cuMemcpy: res=-1, dest=0x149f04609178, src=0x149ae1ff03c0, size=131040
[uc2n517:869272] *** Process received signal ***
[uc2n517:869272] Signal: Aborted (6)
[uc2n517:869272] Signal code:  (-6)
[uc2n517:869272] [ 0] /usr/lib64/libpthread.so.0(+0x12dd0)[0x149f47bcedd0]
[uc2n517:869272] [ 1] /usr/lib64/libc.so.6(gsignal+0x10f)[0x149f4783170f]
[uc2n517:869272] [ 2] /usr/lib64/libc.so.6(abort+0x127)[0x149f4781bb25]
[uc2n517:869272] [ 3] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libopen-pal.so.40(+0x7c375)[0x149f4679b375]
[uc2n517:869272] [ 4] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libopen-pal.so.40(opal_convertor_pack+0xd8)[0x149f467921e8]
[uc2n517:869272] [ 5] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libopen-pal.so.40(mca_btl_smcuda_prepare_src+0x126)[0x149f467ea0a6]
[uc2n517:869272] [ 6] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libmpi.so.40(mca_pml_ob1_send_request_schedule_once+0x1bb)[0x149f5374d22b]
[uc2n517:869272] [ 7] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libmpi.so.40(+0x20927c)[0x149f5375027c]
[uc2n517:869272] [ 8] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libopen-pal.so.40(mca_btl_smcuda_component_progress+0x507)[0x149f467ec4c7]
[uc2n517:869272] [ 9] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libopen-pal.so.40(opal_progress+0x2b)[0x149f46781a1b]
[uc2n517:869272] [10] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libopen-pal.so.40(ompi_sync_wait_mt+0xb5)[0x149f46787ef5]
[uc2n517:869272] [11] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libmpi.so.40(ompi_request_default_wait_all+0x3da)[0x149f535c48ea]
[uc2n517:869272] [12] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libmpi.so.40(mca_coll_basic_alltoallw_intra+0x252)[0x149f53668b32]
[uc2n517:869272] [13] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libmpi.so.40(MPI_Alltoallw+0x1eb)[0x149f535d7ccb]
[uc2n517:869272] [14] /home/st/st_us-051200/st_st160727/DistributedFFT/build/libslab_decomp.so(_ZN18MPIcuFFT_Slab_Opt1IdE15All2All_MPITypeEPvb+0x149)[0x149f5dd252c9]
[uc2n517:869272] [15] /home/st/st_us-051200/st_st160727/DistributedFFT/build/libslab_decomp.so(_ZN18MPIcuFFT_Slab_Opt1IdE7execR2CEPvPKv+0x122)[0x149f5dd24352]
[uc2n517:869272] [16] /home/st/st_us-051200/st_st160727/DistributedFFT/build/libslab_tests.so(_ZN25Tests_Slab_Random_DefaultIdE9testcase4Eii+0xed9)[0x149f5df73625]
[uc2n517:869272] [17] /home/st/st_us-051200/st_st160727/DistributedFFT/build/libslab_tests.so(_ZN25Tests_Slab_Random_DefaultIdE3runEiii+0x9f)[0x149f5df70c8d]
[uc2n517:869272] [18] slab[0x40331f]
[uc2n517:869272] [19] /usr/lib64/libc.so.6(__libc_start_main+0xf3)[0x149f4781d6a3]
[uc2n517:869272] [20] slab[0x4039ee]
[uc2n517:869272] *** End of error message ***
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
[uc2n515.localdomain:1233261] CUDA: Error in cuMemcpy: res=1, dest=0x149edff48171, src=0x149abdff03c0, size=131040
[uc2n515:1233261] *** Process received signal ***
[uc2n515:1233261] Signal: Aborted (6)
[uc2n515:1233261] Signal code:  (-6)
[uc2n515:1233261] [ 0] /usr/lib64/libpthread.so.0(+0x12dd0)[0x149f25af0dd0]
[uc2n515:1233261] [ 1] /usr/lib64/libc.so.6(gsignal+0x10f)[0x149f2575370f]
[uc2n515:1233261] [ 2] /usr/lib64/libc.so.6(abort+0x127)[0x149f2573db25]
[uc2n515:1233261] [ 3] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libopen-pal.so.40(+0x7c375)[0x149f246bd375]
[uc2n515:1233261] [ 4] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libopen-pal.so.40(opal_convertor_pack+0xd8)[0x149f246b41e8]
[uc2n515:1233261] [ 5] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libopen-pal.so.40(mca_btl_openib_prepare_src+0x81)[0x149f2470f941]
[uc2n515:1233261] [ 6] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libmpi.so.40(mca_pml_ob1_send_request_schedule_once+0x1bb)[0x149f3166f22b]
[uc2n515:1233261] [ 7] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libmpi.so.40(+0x20927c)[0x149f3167227c]
[uc2n515:1233261] [ 8] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libopen-pal.so.40(+0xda31f)[0x149f2471b31f]
[uc2n515:1233261] [ 9] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libopen-pal.so.40(+0xdb1a6)[0x149f2471c1a6]
[uc2n515:1233261] [10] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libopen-pal.so.40(opal_progress+0x2b)[0x149f246a3a1b]
[uc2n515:1233261] [11] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libopen-pal.so.40(ompi_sync_wait_mt+0xb5)[0x149f246a9ef5]
[uc2n515:1233261] [12] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libmpi.so.40(ompi_request_default_wait_all+0x3da)[0x149f314e68ea]
[uc2n515:1233261] [13] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libmpi.so.40(mca_coll_basic_alltoallw_intra+0x252)[0x149f3158ab32]
[uc2n515:1233261] [14] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libmpi.so.40(MPI_Alltoallw+0x1eb)[0x149f314f9ccb]
[uc2n515:1233261] [15] /home/st/st_us-051200/st_st160727/DistributedFFT/build/libslab_decomp.so(_ZN18MPIcuFFT_Slab_Opt1IdE15All2All_MPITypeEPvb+0x149)[0x149f3bc472c9]
[uc2n515:1233261] [16] /home/st/st_us-051200/st_st160727/DistributedFFT/build/libslab_decomp.so(_ZN18MPIcuFFT_Slab_Opt1IdE7execR2CEPvPKv+0x122)[0x149f3bc46352]
[uc2n515:1233261] [17] /home/st/st_us-051200/st_st160727/DistributedFFT/build/libslab_tests.so(_ZN25Tests_Slab_Random_DefaultIdE9testcase4Eii+0xed9)[0x149f3be95625]
[uc2n515:1233261] [18] /home/st/st_us-051200/st_st160727/DistributedFFT/build/libslab_tests.so(_ZN25Tests_Slab_Random_DefaultIdE3runEiii+0x9f)[0x149f3be92c8d]
[uc2n515:1233261] [19] slab[0x40331f]
[uc2n515:1233261] [20] /usr/lib64/libc.so.6(__libc_start_main+0xf3)[0x149f2573f6a3]
[uc2n515:1233261] [21] slab[0x4039ee]
[uc2n515:1233261] *** End of error message ***
[uc2n515.localdomain:1233262] CUDA: Error in cuMemcpy: res=1, dest=0x145a4f4dd131, src=0x14562dff03c0, size=131040
[uc2n515:1233262] *** Process received signal ***
[uc2n515:1233262] Signal: Aborted (6)
[uc2n515:1233262] Signal code:  (-6)
[uc2n515:1233262] [ 0] /usr/lib64/libpthread.so.0(+0x12dd0)[0x145a95047dd0]
[uc2n515:1233262] [ 1] /usr/lib64/libc.so.6(gsignal+0x10f)[0x145a94caa70f]
[uc2n515:1233262] [ 2] /usr/lib64/libc.so.6(abort+0x127)[0x145a94c94b25]
[uc2n515:1233262] [ 3] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libopen-pal.so.40(+0x7c375)[0x145a93c14375]
[uc2n515:1233262] [ 4] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libopen-pal.so.40(opal_convertor_pack+0xd8)[0x145a93c0b1e8]
[uc2n515:1233262] [ 5] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libopen-pal.so.40(mca_btl_openib_prepare_src+0x81)[0x145a93c66941]
[uc2n515:1233262] [ 6] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libmpi.so.40(mca_pml_ob1_send_request_schedule_once+0x1bb)[0x145aa0bc622b]
[uc2n515:1233262] [ 7] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libmpi.so.40(+0x20927c)[0x145aa0bc927c]
[uc2n515:1233262] [ 8] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libopen-pal.so.40(+0xda31f)[0x145a93c7231f]
[uc2n515:1233262] [ 9] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libopen-pal.so.40(+0xdb1a6)[0x145a93c731a6]
[uc2n515:1233262] [10] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libopen-pal.so.40(opal_progress+0x2b)[0x145a93bfaa1b]
[uc2n515:1233262] [11] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libopen-pal.so.40(ompi_sync_wait_mt+0xb5)[0x145a93c00ef5]
[uc2n515:1233262] [12] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libmpi.so.40(ompi_request_default_wait_all+0x3da)[0x145aa0a3d8ea]
[uc2n515:1233262] [13] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libmpi.so.40(mca_coll_basic_alltoallw_intra+0x252)[0x145aa0ae1b32]
[uc2n515:1233262] [14] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libmpi.so.40(MPI_Alltoallw+0x1eb)[0x145aa0a50ccb]
[uc2n515:1233262] [15] /home/st/st_us-051200/st_st160727/DistributedFFT/build/libslab_decomp.so(_ZN18MPIcuFFT_Slab_Opt1IdE15All2All_MPITypeEPvb+0x149)[0x145aab19e2c9]
[uc2n515:1233262] [16] /home/st/st_us-051200/st_st160727/DistributedFFT/build/libslab_decomp.so(_ZN18MPIcuFFT_Slab_Opt1IdE7execR2CEPvPKv+0x122)[0x145aab19d352]
[uc2n515:1233262] [17] /home/st/st_us-051200/st_st160727/DistributedFFT/build/libslab_tests.so(_ZN25Tests_Slab_Random_DefaultIdE9testcase4Eii+0xed9)[0x145aab3ec625]
[uc2n515:1233262] [18] /home/st/st_us-051200/st_st160727/DistributedFFT/build/libslab_tests.so(_ZN25Tests_Slab_Random_DefaultIdE3runEiii+0x9f)[0x145aab3e9c8d]
[uc2n515:1233262] [19] slab[0x40331f]
[uc2n515:1233262] [20] /usr/lib64/libc.so.6(__libc_start_main+0xf3)[0x145a94c966a3]
[uc2n515:1233262] [21] slab[0x4039ee]
[uc2n515:1233262] *** End of error message ***
--------------------------------------------------------------------------
mpiexec noticed that process rank 14 with PID 869272 on node uc2n517 exited on signal 6 (Aborted).
--------------------------------------------------------------------------
[uc2n515.localdomain:1233221] 2 more processes have sent help message help-mpi-common-cuda.txt / cuMemcpyAsync failed
Starting computation for size 128
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 128
2021-09-19 03:02:30.968538
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 128
2021-09-19 03:02:47.286340
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 128
2021-09-19 03:02:55.559043
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 128
2021-09-19 03:03:03.092787
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 128
2021-09-19 03:03:11.194439
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 128
2021-09-19 03:03:18.655997
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 128
2021-09-19 03:03:28.373177
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 128
2021-09-19 03:03:35.822011
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 128
2021-09-19 03:03:43.994837
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 128
2021-09-19 03:03:51.468125
b''

Starting computation for size [128, 128, 256]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 256
2021-09-19 03:04:01.464847
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 256
2021-09-19 03:04:08.921996
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 256
2021-09-19 03:04:17.131477
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 256
2021-09-19 03:04:24.609443
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 256
2021-09-19 03:04:33.311434
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 256
2021-09-19 03:04:40.917609
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 256
2021-09-19 03:04:52.995853
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 256
2021-09-19 03:05:00.404413
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 256
2021-09-19 03:05:08.619852
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 256
2021-09-19 03:05:16.025705
b''

Starting computation for size [128, 256, 256]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 256 -nz 256
2021-09-19 03:05:27.698327
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 256 -nz 256
2021-09-19 03:05:35.271047
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 256 -nz 256
2021-09-19 03:05:43.536856
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 256 -nz 256
2021-09-19 03:05:51.398833
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 256 -nz 256
2021-09-19 03:05:59.933019
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 256 -nz 256
2021-09-19 03:06:07.460002
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 256 -nz 256
2021-09-19 03:06:22.388342
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 256 -nz 256
2021-09-19 03:06:30.151477
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 256 -nz 256
2021-09-19 03:06:38.375483
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 256 -nz 256
2021-09-19 03:06:45.936699
b''

Starting computation for size 256
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 256
2021-09-19 03:07:01.230081
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 256
2021-09-19 03:07:11.024181
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 256
2021-09-19 03:07:20.010448
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 256
2021-09-19 03:07:27.569688
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 256
2021-09-19 03:07:35.832025
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 256
2021-09-19 03:07:43.494141
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 256
2021-09-19 03:07:57.839493
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 256
2021-09-19 03:08:05.377738
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 256
2021-09-19 03:08:13.626793
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 256
2021-09-19 03:08:21.330923
b''

Starting computation for size [256, 256, 512]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 512
2021-09-19 03:08:36.363478
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 512
2021-09-19 03:08:44.113954
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 512
2021-09-19 03:08:52.424572
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 512
2021-09-19 03:09:00.134261
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 512
2021-09-19 03:09:08.454997
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 512
2021-09-19 03:09:16.297232
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 512
2021-09-19 03:09:37.639659
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 512
2021-09-19 03:09:45.444073
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 512
2021-09-19 03:09:53.717944
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 512
2021-09-19 03:10:01.781446
b''

Starting computation for size [256, 512, 512]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 512 -nz 512
2021-09-19 03:10:23.846430
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 512 -nz 512
2021-09-19 03:10:31.960707
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 512 -nz 512
2021-09-19 03:10:40.522593
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 512 -nz 512
2021-09-19 03:10:48.584036
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 512 -nz 512
2021-09-19 03:10:57.051324
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 512 -nz 512
2021-09-19 03:11:05.911587
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 512 -nz 512
2021-09-19 03:11:40.369255
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 512 -nz 512
2021-09-19 03:11:48.631084
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 512 -nz 512
2021-09-19 03:11:57.165168
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 512 -nz 512
2021-09-19 03:12:05.694653
b''

Starting computation for size 512
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 512
2021-09-19 03:12:41.593326
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 512
2021-09-19 03:12:51.167314
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 512
2021-09-19 03:13:00.125611
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 512
2021-09-19 03:13:08.916607
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 512
2021-09-19 03:13:17.931636
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 512
2021-09-19 03:13:27.389705
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 512
2021-09-19 03:14:03.324395
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 512
2021-09-19 03:14:12.375567
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 512
2021-09-19 03:14:21.326490
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 512
2021-09-19 03:14:30.777815
b''

Starting computation for size [512, 512, 1024]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-19 03:15:06.673169
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-19 03:15:17.413844
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-19 03:15:27.262600
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-19 03:15:37.575635
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-19 03:15:47.402164
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-19 03:15:59.977707
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-19 03:17:01.882523
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-19 03:17:12.950780
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-19 03:17:22.829801
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-19 03:17:34.542374
b''

Starting computation for size [512, 1024, 1024]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-19 03:18:39.644191
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-19 03:18:53.751683
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-19 03:19:05.372956
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-19 03:19:18.784505
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-19 03:19:30.302387
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-19 03:19:46.309265
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-19 03:21:40.102963
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-19 03:21:54.214029
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-19 03:22:06.008882
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-19 03:22:22.153881
b''

Starting computation for size 1024
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-19 03:24:21.903969
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-19 03:24:43.149275
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-19 03:24:58.578272
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-19 03:25:18.532310
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-19 03:25:34.010713
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-19 03:25:57.901214
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-19 03:27:56.410384
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-19 03:28:18.101119
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-19 03:28:33.607122
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-19 03:28:57.852849
b''

Starting computation for size [1024, 1024, 2048]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-19 03:31:02.810592
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-19 03:31:38.184756
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-19 03:32:01.464827
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-19 03:32:34.389048
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-19 03:32:58.037304
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-19 03:33:38.132552
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-19 03:37:22.407540
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-19 03:37:57.377901
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-19 03:38:21.010220
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-19 03:39:02.699919
b''

Starting computation for size [1024, 2048, 2048]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-19 03:43:00.165768
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-19 03:44:03.371531
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-19 03:44:41.866853
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-19 03:45:40.707956
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-19 03:46:19.130980
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-19 03:47:31.499022
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-19 03:54:52.599577
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-19 03:55:54.841952
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-19 03:56:33.341817
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-19 03:57:47.346340
b''

Starting computation for size 2048
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-19 04:05:31.508370
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-19 04:07:32.497289
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-19 04:08:39.620269
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-19 04:10:29.358159
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-19 04:11:36.753756
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-19 04:13:50.117832
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-19 04:21:43.802889
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-19 04:22:01.145527
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-19 04:22:17.159389
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-19 04:22:33.912378
b''

Starting computation for size 128
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 04:22:50.168134
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 04:22:59.493031
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 04:23:07.556523
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 04:23:15.349445
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 04:23:23.610143
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 04:23:31.453111
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 04:23:39.731096
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 04:23:47.600973
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 04:23:55.825772
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 04:24:03.655153
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

Starting computation for size 256
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 04:24:12.006575
b'Result (avg): 5.19278e-09\nResult (max): 5.37366e-08\n'

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 04:24:19.937093
b'Result (avg): 5.19278e-09\nResult (max): 5.37366e-08\n'

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 04:24:28.182799
b'Result (avg): 5.19278e-09\nResult (max): 5.37366e-08\n'

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 04:24:36.103862
b'Result (avg): 5.19278e-09\nResult (max): 5.37366e-08\n'

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 04:24:44.382264
b'Result (avg): 5.19278e-09\nResult (max): 5.37366e-08\n'

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 04:24:52.303940
b'Result (avg): 5.19278e-09\nResult (max): 5.37366e-08\n'

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 04:25:00.783882
b'Result (avg): 5.19278e-09\nResult (max): 5.37366e-08\n'

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 04:25:08.673077
b'Result (avg): 5.19278e-09\nResult (max): 5.37366e-08\n'

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 04:25:17.130910
b'Result (avg): 5.19278e-09\nResult (max): 5.37366e-08\n'

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 04:25:25.117273
b'Result (avg): 5.19278e-09\nResult (max): 5.37366e-08\n'

Starting computation for size 512
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 04:25:33.815382
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 04:25:42.753149
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 04:25:52.007781
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 04:26:00.976963
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 04:26:10.356538
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 04:26:19.581654
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 04:26:30.197619
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 04:26:39.209055
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 04:26:48.399295
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 04:26:57.402882
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

Starting computation for size 1024
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 04:27:07.914026
b'Result (avg): 7.69766e-07\nResult (max): 9.22813e-06\n'

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 04:27:27.253298
b'Result (avg): 7.69766e-07\nResult (max): 9.22813e-06\n'

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 04:27:45.890961
b'Result (avg): 7.69766e-07\nResult (max): 9.22813e-06\n'

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 04:28:05.077845
b'Result (avg): 7.69766e-07\nResult (max): 9.22813e-06\n'

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 04:28:23.868463
b'Result (avg): 7.69766e-07\nResult (max): 9.22813e-06\n'

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 04:28:43.443864
b'Result (avg): 7.69766e-07\nResult (max): 9.22813e-06\n'

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 04:29:07.335507
b'Result (avg): 7.69766e-07\nResult (max): 9.22813e-06\n'

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 04:29:26.678344
b'Result (avg): 7.69766e-07\nResult (max): 9.22813e-06\n'

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 04:29:45.361220
b'Result (avg): 7.69766e-07\nResult (max): 9.22813e-06\n'

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 04:30:05.120932
b'Result (avg): 7.69766e-07\nResult (max): 9.22813e-06\n'

Starting computation for size 2048
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 04:30:29.607679
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 04:32:11.265633
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 04:33:26.885147
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 04:35:15.268727
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 04:36:30.058886
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 04:38:28.501092
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 04:40:24.754484
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 04:41:46.716074
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 04:43:19.114982
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 04:44:46.227598
b''

Slab 1D->2D default
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1233572] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1233572] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1233864] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1233864] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1234130] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1234130] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1234645] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1234645] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1235177] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1235177] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1235444] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1235444] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1235713] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1235713] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1235987] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1235987] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1236252] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1236252] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1236519] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1236519] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1236786] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1236786] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1237063] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1237063] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1237328] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1237328] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1237842] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1237842] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1238355] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1238355] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1238640] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1238640] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1238909] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1238909] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1239174] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1239174] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1239448] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1239448] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1239718] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1239718] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1239986] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1239986] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1240249] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1240249] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1240527] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1240527] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1241042] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1241042] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1241560] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1241560] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1241826] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1241826] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1242107] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1242107] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1242375] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1242375] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1242642] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1242642] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1242923] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1242923] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1243192] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1243192] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1243459] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1243459] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1243737] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1243737] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1244252] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1244252] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1244770] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1244770] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1245037] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1245037] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1245322] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1245322] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1245587] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1245587] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1245851] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1245851] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1246129] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1246129] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1246399] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1246399] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1246663] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1246663] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1246940] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1246940] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1247455] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1247455] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1247969] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1247969] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1248252] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1248252] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1248519] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1248519] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1248784] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1248784] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1249061] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1249061] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1249326] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1249326] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1249595] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1249595] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1249872] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1249872] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1250141] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1250141] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1250657] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1250657] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1251188] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1251188] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1251453] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1251453] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1251739] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1251739] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1252008] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1252008] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1252274] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1252274] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1252553] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1252553] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1252824] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1252824] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1253105] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1253105] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1253377] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1253377] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1253894] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1253894] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1254423] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1254423] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1254688] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1254688] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1254979] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1254979] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1255262] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1255262] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1255526] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1255526] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1255793] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1255793] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1256103] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1256103] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1256371] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1256371] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1256638] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1256638] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1257167] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1257167] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1257687] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1257687] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1257952] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1257952] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1258255] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1258255] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1258526] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1258526] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1258793] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1258793] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1259075] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1259075] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1259411] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1259411] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1259691] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1259691] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1259960] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1259960] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1260490] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1260490] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1261009] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1261009] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1261278] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1261278] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1261607] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1261607] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1261875] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1261875] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1262152] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1262152] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1262422] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1262422] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1262757] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1262757] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1263040] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1263040] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1263307] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1263307] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1263842] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1263842] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1264362] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1264362] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1264646] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1264646] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1265030] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1265030] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1265302] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1265302] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1265583] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1265583] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1265869] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1265869] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1266256] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1266256] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1266544] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1266544] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1266830] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1266830] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1267367] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1267367] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1267902] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1267902] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1268198] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1268198] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1268578] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1268578] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1268875] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1268875] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1269157] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1269157] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1269447] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1269447] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1269838] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1269838] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1270154] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1270154] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1270457] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1270457] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1271018] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1271018] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1271559] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1271559] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1271892] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1271892] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1272387] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1272387] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1272705] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1272705] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1273013] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1273013] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1273334] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1273334] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1273864] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1273864] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1274213] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1274213] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1274547] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1274547] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1275151] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1275151] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1275736] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1275736] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1276114] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1276114] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1276847] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1276847] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n517:913504:0:913504] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x1545cc000000)
[uc2n517:913508:0:913508] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x1508d4000000)
[uc2n517:913510:0:913510] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14ff1c000000)
[uc2n517:913505:0:913505] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14cc00000000)
[uc2n517:913509:0:913509] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14b7f4000000)
[uc2n517:913511:0:913511] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14b6ee000000)
[uc2n517:913507:0:913507] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14acb4000000)
[uc2n517:913506:0:913506] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x15468c000000)
==== backtrace (tid: 913509) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000002cd18 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
 9 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
10 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
11 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
12 0x0000000000403591 main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 913508) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000002cd18 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
 9 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
10 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
11 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
==== backtrace (tid: 913511) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000002cd18 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
 9 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
10 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
11 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
12 0x0000000000403591 main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 913507) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000002cd18 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
 9 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
10 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
11 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
12 0x0000000000403591 main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
12 0x0000000000403591 main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 913505) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
=================================
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000002cd18 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
 9 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
10 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
11 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
12 0x0000000000403591 main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 913506) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000002cd18 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
 9 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
10 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
11 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
12 0x0000000000403591 main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 913510) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000002cd18 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
 9 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
10 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
11 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
12 0x0000000000403591 main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 913504) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000002cd18 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
 9 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
10 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
11 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
12 0x0000000000403591 main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec noticed that process rank 11 with PID 913507 on node uc2n517 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1277115] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1277115] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n517:913855:0:913855] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x152b2e000010)
[uc2n517:913852:0:913852] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14dc06000010)
[uc2n517:913853:0:913853] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14c0f8000010)
[uc2n517:913854:0:913854] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x1463aa000010)
[uc2n517:913857:0:913857] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x152f30000010)
[uc2n517:913851:0:913851] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x145a26000010)
[uc2n517:913856:0:913856] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x147370000010)
[uc2n517:913858:0:913858] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14a0e6000010)
==== backtrace (tid: 913855) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x00000000001d5e98 cuMemGetAttribute()  ???:0
 3 0x00000000002ffe53 cudbgGetAPI()  ???:0
 4 0x00000000003009e8 cudbgGetAPI()  ???:0
 5 0x0000000000435d90 cudbgApiInit()  ???:0
 6 0x000000000018d730 ???()  /lib64/libcuda.so.1:0
 7 0x000000000018df04 ???()  /lib64/libcuda.so.1:0
 8 0x00000000001904d9 ???()  /lib64/libcuda.so.1:0
 9 0x00000000001fe77e cuGraphAddMemAllocNode()  ???:0
10 0x00000000000a2944 mca_common_cuda_cu_memcpy()  common_cuda.c:0
11 0x000000000007c563 opal_cuda_memcpy_sync()  ???:0
12 0x0000000000077119 non_overlap_cuda_copy_content_same_ddt()  opal_datatype_copy.c:0
13 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
14 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
15 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
16 0x000000000009042b PMPI_Alltoallv()  ???:0
17 0x000000000002cd18 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
18 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
19 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
20 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
21 0x0000000000403591 main()  ???:0
22 0x00000000000236a3 __libc_start_main()  ???:0
23 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 913857) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x00000000001d5e98 cuMemGetAttribute()  ???:0
 3 0x00000000002ffe53 cudbgGetAPI()  ???:0
 4 0x00000000003009e8 cudbgGetAPI()  ???:0
 5 0x0000000000435d90 cudbgApiInit()  ???:0
 6 0x000000000018d730 ???()  /lib64/libcuda.so.1:0
 7 0x000000000018df04 ???()  /lib64/libcuda.so.1:0
 8 0x00000000001904d9 ???()  /lib64/libcuda.so.1:0
 9 0x00000000001fe77e cuGraphAddMemAllocNode()  ???:0
10 0x00000000000a2944 mca_common_cuda_cu_memcpy()  common_cuda.c:0
11 0x000000000007c563 opal_cuda_memcpy_sync()  ???:0
12 0x0000000000077119 non_overlap_cuda_copy_content_same_ddt()  opal_datatype_copy.c:0
13 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
14 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
15 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
16 0x000000000009042b PMPI_Alltoallv()  ???:0
17 0x000000000002cd18 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
18 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
19 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
20 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
21 0x0000000000403591 main()  ???:0
22 0x00000000000236a3 __libc_start_main()  ???:0
23 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 913856) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x00000000001d5e98 cuMemGetAttribute()  ???:0
 3 0x00000000002ffe53 cudbgGetAPI()  ???:0
 4 0x00000000003009e8 cudbgGetAPI()  ???:0
 5 0x0000000000435d90 cudbgApiInit()  ???:0
 6 0x000000000018d730 ???()  /lib64/libcuda.so.1:0
 7 0x000000000018df04 ???()  /lib64/libcuda.so.1:0
 8 0x00000000001904d9 ???()  /lib64/libcuda.so.1:0
 9 0x00000000001fe77e cuGraphAddMemAllocNode()  ???:0
10 0x00000000000a2944 mca_common_cuda_cu_memcpy()  common_cuda.c:0
11 0x000000000007c563 opal_cuda_memcpy_sync()  ???:0
12 0x0000000000077119 non_overlap_cuda_copy_content_same_ddt()  opal_datatype_copy.c:0
13 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
14 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
15 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
16 0x000000000009042b PMPI_Alltoallv()  ???:0
17 0x000000000002cd18 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
18 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
19 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
20 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
21 0x0000000000403591 main()  ???:0
22 0x00000000000236a3 __libc_start_main()  ???:0
23 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 913858) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x00000000001d5e98 cuMemGetAttribute()  ???:0
 3 0x00000000002ffe53 cudbgGetAPI()  ???:0
 4 0x00000000003009e8 cudbgGetAPI()  ???:0
 5 0x0000000000435d90 cudbgApiInit()  ???:0
 6 0x000000000018d730 ???()  /lib64/libcuda.so.1:0
 7 0x000000000018df04 ???()  /lib64/libcuda.so.1:0
 8 0x00000000001904d9 ???()  /lib64/libcuda.so.1:0
 9 0x00000000001fe77e cuGraphAddMemAllocNode()  ???:0
10 0x00000000000a2944 mca_common_cuda_cu_memcpy()  common_cuda.c:0
11 0x000000000007c563 opal_cuda_memcpy_sync()  ???:0
12 0x0000000000077119 non_overlap_cuda_copy_content_same_ddt()  opal_datatype_copy.c:0
13 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
14 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
15 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
16 0x000000000009042b PMPI_Alltoallv()  ???:0
17 0x000000000002cd18 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
18 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
19 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
20 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
21 0x0000000000403591 main()  ???:0
22 0x00000000000236a3 __libc_start_main()  ???:0
23 0x00000000004039ee _start()  ???:0
==== backtrace (tid: 913851) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x00000000001d5e98 cuMemGetAttribute()  ???:0
 3 0x00000000002ffe53 cudbgGetAPI()  ???:0
 4 0x00000000003009e8 cudbgGetAPI()  ???:0
=================================
==== backtrace (tid: 913852) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x00000000001d5e98 cuMemGetAttribute()  ???:0
 3 0x00000000002ffe53 cudbgGetAPI()  ???:0
 4 0x00000000003009e8 cudbgGetAPI()  ???:0
 5 0x0000000000435d90 cudbgApiInit()  ???:0
 6 0x000000000018d730 ???()  /lib64/libcuda.so.1:0
 7 0x000000000018df04 ???()  /lib64/libcuda.so.1:0
 8 0x00000000001904d9 ???()  /lib64/libcuda.so.1:0
 9 0x00000000001fe77e cuGraphAddMemAllocNode()  ???:0
10 0x00000000000a2944 mca_common_cuda_cu_memcpy()  common_cuda.c:0
11 0x000000000007c563 opal_cuda_memcpy_sync()  ???:0
12 0x0000000000077119 non_overlap_cuda_copy_content_same_ddt()  opal_datatype_copy.c:0
13 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
14 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
15 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
16 0x000000000009042b PMPI_Alltoallv()  ???:0
17 0x000000000002cd18 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
18 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
19 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
20 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
21 0x0000000000403591 main()  ???:0
22 0x00000000000236a3 __libc_start_main()  ???:0
==== backtrace (tid: 913853) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x00000000001d5e98 cuMemGetAttribute()  ???:0
 3 0x00000000002ffe53 cudbgGetAPI()  ???:0
 4 0x00000000003009e8 cudbgGetAPI()  ???:0
 5 0x0000000000435d90 cudbgApiInit()  ???:0
 6 0x000000000018d730 ???()  /lib64/libcuda.so.1:0
 7 0x000000000018df04 ???()  /lib64/libcuda.so.1:0
 8 0x00000000001904d9 ???()  /lib64/libcuda.so.1:0
 9 0x00000000001fe77e cuGraphAddMemAllocNode()  ???:0
10 0x00000000000a2944 mca_common_cuda_cu_memcpy()  common_cuda.c:0
11 0x000000000007c563 opal_cuda_memcpy_sync()  ???:0
12 0x0000000000077119 non_overlap_cuda_copy_content_same_ddt()  opal_datatype_copy.c:0
13 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
14 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
15 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
16 0x000000000009042b PMPI_Alltoallv()  ???:0
17 0x000000000002cd18 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
18 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
19 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
20 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
21 0x0000000000403591 main()  ???:0
22 0x00000000000236a3 __libc_start_main()  ???:0
23 0x00000000004039ee _start()  ???:0
=================================
23 0x00000000004039ee _start()  ???:0
=================================
 5 0x0000000000435d90 cudbgApiInit()  ???:0
 6 0x000000000018d730 ???()  /lib64/libcuda.so.1:0
 7 0x000000000018df04 ???()  /lib64/libcuda.so.1:0
 8 0x00000000001904d9 ???()  /lib64/libcuda.so.1:0
 9 0x00000000001fe77e cuGraphAddMemAllocNode()  ???:0
10 0x00000000000a2944 mca_common_cuda_cu_memcpy()  common_cuda.c:0
11 0x000000000007c563 opal_cuda_memcpy_sync()  ???:0
12 0x0000000000077119 non_overlap_cuda_copy_content_same_ddt()  opal_datatype_copy.c:0
13 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
14 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
15 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
16 0x000000000009042b PMPI_Alltoallv()  ???:0
17 0x000000000002cd18 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
18 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
19 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
20 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
21 0x0000000000403591 main()  ???:0
22 0x00000000000236a3 __libc_start_main()  ???:0
23 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 913854) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x00000000001d5e98 cuMemGetAttribute()  ???:0
 3 0x00000000002ffe53 cudbgGetAPI()  ???:0
 4 0x00000000003009e8 cudbgGetAPI()  ???:0
 5 0x0000000000435d90 cudbgApiInit()  ???:0
 6 0x000000000018d730 ???()  /lib64/libcuda.so.1:0
 7 0x000000000018df04 ???()  /lib64/libcuda.so.1:0
 8 0x00000000001904d9 ???()  /lib64/libcuda.so.1:0
 9 0x00000000001fe77e cuGraphAddMemAllocNode()  ???:0
10 0x00000000000a2944 mca_common_cuda_cu_memcpy()  common_cuda.c:0
11 0x000000000007c563 opal_cuda_memcpy_sync()  ???:0
12 0x0000000000077119 non_overlap_cuda_copy_content_same_ddt()  opal_datatype_copy.c:0
13 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
14 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
15 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
16 0x000000000009042b PMPI_Alltoallv()  ???:0
17 0x000000000002cd18 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
18 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
19 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
20 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
21 0x0000000000403591 main()  ???:0
22 0x00000000000236a3 __libc_start_main()  ???:0
23 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec noticed that process rank 12 with PID 913855 on node uc2n517 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1277403] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1277403] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n517:914202:0:914202] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14ab68000000)
[uc2n517:914205:0:914205] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14dc0c000000)
[uc2n517:914206:0:914206] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x153ba6000000)
[uc2n517:914207:0:914207] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x1458b0000000)
[uc2n517:914203:0:914203] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x154a16000000)
[uc2n517:914204:0:914204] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x1507be000000)
[uc2n517:914208:0:914208] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x148b20000000)
[uc2n517:914209:0:914209] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x147718000000)
==== backtrace (tid: 914202) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000002d3d9 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_MPIType()  ???:0
 8 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
 9 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
10 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 914205) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000002d3d9 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_MPIType()  ???:0
 8 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
 9 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
10 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 914206) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000002d3d9 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_MPIType()  ???:0
 8 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
 9 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
10 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 914207) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000002d3d9 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_MPIType()  ???:0
 8 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
 9 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
10 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 914203) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000002d3d9 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_MPIType()  ???:0
 8 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
 9 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
10 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 914204) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000002d3d9 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_MPIType()  ???:0
 8 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
 9 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
10 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 914208) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000002d3d9 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_MPIType()  ???:0
 8 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
 9 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
10 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 914209) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000002d3d9 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_MPIType()  ???:0
 8 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
 9 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
10 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec noticed that process rank 8 with PID 914202 on node uc2n517 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1277670] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1277670] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n517:914562:0:914562] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14a636000000)
[uc2n517:914555:0:914555] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14c566000000)
[uc2n517:914560:0:914560] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x1474f2000000)
[uc2n517:914557:0:914557] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14990e000000)
[uc2n517:914556:0:914556] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14caae000000)
[uc2n517:914559:0:914559] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x154c06000000)
[uc2n517:914558:0:914558] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14d282000000)
[uc2n517:914561:0:914561] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14c6d4000000)
==== backtrace (tid: 914555) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000002d3d9 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_MPIType()  ???:0
 8 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
 9 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
10 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 914562) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000002d3d9 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_MPIType()  ???:0
 8 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
 9 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
10 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 914557) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000002d3d9 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_MPIType()  ???:0
 8 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
 9 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
10 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 914556) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000002d3d9 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_MPIType()  ???:0
 8 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
 9 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
10 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 914559) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000002d3d9 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_MPIType()  ???:0
 8 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
 9 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
10 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 914561) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000002d3d9 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_MPIType()  ???:0
 8 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
 9 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
10 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 914560) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000002d3d9 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_MPIType()  ???:0
 8 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
 9 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
10 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 914558) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000002d3d9 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_MPIType()  ???:0
 8 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
 9 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
10 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec noticed that process rank 14 with PID 914561 on node uc2n517 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1277959] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1277959] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1278226] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1278226] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1278503] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1278503] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1278796] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1278796] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1279081] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1279081] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1279344] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1279344] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1279623] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1279623] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1279891] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1279891] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1280157] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1280157] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1280422] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1280422] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1280705] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1280705] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1280974] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1280974] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1281237] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1281237] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1281582] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1281582] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1281866] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1281866] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1282134] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1282134] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1282399] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1282399] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1282678] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1282678] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1282945] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1282945] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1283211] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1283211] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1283477] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1283477] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1283758] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1283758] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1284027] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1284027] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1284317] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1284317] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1284614] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1284614] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1284883] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1284883] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1285153] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1285153] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1285424] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1285424] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1285693] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1285693] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1285962] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1285962] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1286246] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1286246] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1286518] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1286518] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1286800] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1286800] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1287109] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1287109] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1287399] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1287399] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1287682] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1287682] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1287969] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1287969] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1288250] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1288250] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1288540] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1288540] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1288813] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1288813] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1289098] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1289098] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1289458] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1289458] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[1881,1],14]
  Exit code:    1
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1289767] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1289767] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1290164] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1290164] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[1055,1],14]
  Exit code:    1
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1290478] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1290478] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1290850] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1290850] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1291233] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1291233] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n517:928390:0:928390] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14c79c000000)
==== backtrace (tid: 928390) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000002cd18 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
 9 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
10 0x000000000002b234 Tests_Slab_Random_Z_Then_YX<double>::testcase4()  ???:0
11 0x0000000000028bd1 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
12 0x0000000000403591 main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
[uc2n517:928391:0:928391] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14743e000000)
==== backtrace (tid: 928391) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000002cd18 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
 9 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
10 0x000000000002b234 Tests_Slab_Random_Z_Then_YX<double>::testcase4()  ???:0
11 0x0000000000028bd1 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
12 0x0000000000403591 main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
[uc2n517:928384:0:928384] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x1469ac000000)
[uc2n517:928385:0:928385] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x145c0c000000)
==== backtrace (tid: 928384) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000002cd18 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
 9 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
10 0x000000000002b234 Tests_Slab_Random_Z_Then_YX<double>::testcase4()  ???:0
11 0x0000000000028bd1 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
12 0x0000000000403591 main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 928385) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000002cd18 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
 9 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
10 0x000000000002b234 Tests_Slab_Random_Z_Then_YX<double>::testcase4()  ???:0
11 0x0000000000028bd1 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
12 0x0000000000403591 main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec noticed that process rank 14 with PID 928390 on node uc2n517 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1291567] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1291567] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[7812,1],7]
  Exit code:    1
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1291912] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1291912] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n517:929137:0:929137] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x15367c000000)
==== backtrace (tid: 929137) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000002d3d9 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_MPIType()  ???:0
 8 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
 9 0x000000000002b234 Tests_Slab_Random_Z_Then_YX<double>::testcase4()  ???:0
10 0x0000000000028bd1 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
[uc2n517:929138:0:929138] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x150fe0000000)
==== backtrace (tid: 929138) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000002d3d9 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_MPIType()  ???:0
 8 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
 9 0x000000000002b234 Tests_Slab_Random_Z_Then_YX<double>::testcase4()  ???:0
10 0x0000000000028bd1 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
[uc2n517:929132:0:929132] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x145410000000)
==== backtrace (tid: 929132) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000002d3d9 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_MPIType()  ???:0
 8 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
 9 0x000000000002b234 Tests_Slab_Random_Z_Then_YX<double>::testcase4()  ???:0
10 0x0000000000028bd1 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
[uc2n517:929131:0:929131] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14f6e4000000)
==== backtrace (tid: 929131) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000002d3d9 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_MPIType()  ???:0
 8 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
 9 0x000000000002b234 Tests_Slab_Random_Z_Then_YX<double>::testcase4()  ???:0
10 0x0000000000028bd1 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec noticed that process rank 14 with PID 929137 on node uc2n517 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1292255] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1292255] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n517:929518:0:929518] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x1466e0000000)
==== backtrace (tid: 929518) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000002d3d9 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_MPIType()  ???:0
 8 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
 9 0x000000000002b234 Tests_Slab_Random_Z_Then_YX<double>::testcase4()  ???:0
10 0x0000000000028bd1 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
[uc2n517:929519:0:929519] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x152bee000000)
==== backtrace (tid: 929519) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000002d3d9 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_MPIType()  ???:0
 8 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
 9 0x000000000002b234 Tests_Slab_Random_Z_Then_YX<double>::testcase4()  ???:0
10 0x0000000000028bd1 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
[uc2n517:929513:0:929513] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x144976000000)
==== backtrace (tid: 929513) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000002d3d9 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_MPIType()  ???:0
 8 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
 9 0x000000000002b234 Tests_Slab_Random_Z_Then_YX<double>::testcase4()  ???:0
10 0x0000000000028bd1 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
[uc2n517:929512:0:929512] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x149cc8000000)
==== backtrace (tid: 929512) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000002d3d9 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_MPIType()  ???:0
 8 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
 9 0x000000000002b234 Tests_Slab_Random_Z_Then_YX<double>::testcase4()  ???:0
10 0x0000000000028bd1 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
mpiexec noticed that process rank 14 with PID 929518 on node uc2n517 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
Starting computation for size 128
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 128 -ny 128 -nz 128
2021-09-19 04:46:22.030767
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 128 -ny 128 -nz 128
2021-09-19 04:46:44.752465
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 128 -ny 128 -nz 128
2021-09-19 04:46:52.790081
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 128 -ny 128 -nz 128
2021-09-19 04:47:00.130975
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 128 -ny 128 -nz 128
2021-09-19 04:47:08.189363
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 128 -ny 128 -nz 128
2021-09-19 04:47:15.558969
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 128 -ny 128 -nz 128
2021-09-19 04:47:25.653318
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 128 -ny 128 -nz 128
2021-09-19 04:47:33.122677
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 128 -ny 128 -nz 128
2021-09-19 04:47:41.116904
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 128 -ny 128 -nz 128
2021-09-19 04:47:48.502342
b''

Starting computation for size [128, 128, 256]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 128 -ny 128 -nz 256
2021-09-19 04:47:58.900855
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 128 -ny 128 -nz 256
2021-09-19 04:48:06.323737
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 128 -ny 128 -nz 256
2021-09-19 04:48:14.505797
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 128 -ny 128 -nz 256
2021-09-19 04:48:21.977072
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 128 -ny 128 -nz 256
2021-09-19 04:48:30.134421
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 128 -ny 128 -nz 256
2021-09-19 04:48:37.554863
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 128 -ny 128 -nz 256
2021-09-19 04:48:47.632343
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 128 -ny 128 -nz 256
2021-09-19 04:48:55.007039
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 128 -ny 128 -nz 256
2021-09-19 04:49:03.136784
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 128 -ny 128 -nz 256
2021-09-19 04:49:10.519711
b''

Starting computation for size [128, 256, 256]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 128 -ny 256 -nz 256
2021-09-19 04:49:20.797393
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 128 -ny 256 -nz 256
2021-09-19 04:49:28.301126
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 128 -ny 256 -nz 256
2021-09-19 04:49:36.524323
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 128 -ny 256 -nz 256
2021-09-19 04:49:45.305302
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 128 -ny 256 -nz 256
2021-09-19 04:49:53.539990
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 128 -ny 256 -nz 256
2021-09-19 04:50:01.068140
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 128 -ny 256 -nz 256
2021-09-19 04:50:13.016517
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 128 -ny 256 -nz 256
2021-09-19 04:50:20.485100
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 128 -ny 256 -nz 256
2021-09-19 04:50:28.625826
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 128 -ny 256 -nz 256
2021-09-19 04:50:36.074814
b''

Starting computation for size 256
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 256 -ny 256 -nz 256
2021-09-19 04:50:48.717127
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 256 -ny 256 -nz 256
2021-09-19 04:50:56.264986
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 256 -ny 256 -nz 256
2021-09-19 04:51:04.705395
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 256 -ny 256 -nz 256
2021-09-19 04:51:12.292543
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 256 -ny 256 -nz 256
2021-09-19 04:51:20.515569
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 256 -ny 256 -nz 256
2021-09-19 04:51:28.168670
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 256 -ny 256 -nz 256
2021-09-19 04:51:43.993800
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 256 -ny 256 -nz 256
2021-09-19 04:51:51.588701
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 256 -ny 256 -nz 256
2021-09-19 04:51:59.839335
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 256 -ny 256 -nz 256
2021-09-19 04:52:07.691354
b''

Starting computation for size [256, 256, 512]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 256 -ny 256 -nz 512
2021-09-19 04:52:24.865380
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 256 -ny 256 -nz 512
2021-09-19 04:52:32.647647
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 256 -ny 256 -nz 512
2021-09-19 04:52:40.974850
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 256 -ny 256 -nz 512
2021-09-19 04:52:48.727093
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 256 -ny 256 -nz 512
2021-09-19 04:52:57.071994
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 256 -ny 256 -nz 512
2021-09-19 04:53:04.946283
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 256 -ny 256 -nz 512
2021-09-19 04:53:20.952172
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 256 -ny 256 -nz 512
2021-09-19 04:53:28.776159
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 256 -ny 256 -nz 512
2021-09-19 04:53:37.089501
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 256 -ny 256 -nz 512
2021-09-19 04:53:45.062715
b''

Starting computation for size [256, 512, 512]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 256 -ny 512 -nz 512
2021-09-19 04:54:01.963453
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 256 -ny 512 -nz 512
2021-09-19 04:54:10.564289
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 256 -ny 512 -nz 512
2021-09-19 04:54:19.077614
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 256 -ny 512 -nz 512
2021-09-19 04:54:27.192143
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 256 -ny 512 -nz 512
2021-09-19 04:54:35.797761
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 256 -ny 512 -nz 512
2021-09-19 04:54:44.919052
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 256 -ny 512 -nz 512
2021-09-19 04:55:08.762340
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 256 -ny 512 -nz 512
2021-09-19 04:55:16.936956
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 256 -ny 512 -nz 512
2021-09-19 04:55:26.095560
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 256 -ny 512 -nz 512
2021-09-19 04:55:34.612027
b''

Starting computation for size 512
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 512 -ny 512 -nz 512
2021-09-19 04:56:00.425461
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 512 -ny 512 -nz 512
2021-09-19 04:56:09.321070
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 512 -ny 512 -nz 512
2021-09-19 04:56:18.381018
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 512 -ny 512 -nz 512
2021-09-19 04:56:27.083847
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 512 -ny 512 -nz 512
2021-09-19 04:56:36.122898
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 512 -ny 512 -nz 512
2021-09-19 04:56:45.547818
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 512 -ny 512 -nz 512
2021-09-19 04:57:24.960961
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 512 -ny 512 -nz 512
2021-09-19 04:57:33.828079
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 512 -ny 512 -nz 512
2021-09-19 04:57:42.870153
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 512 -ny 512 -nz 512
2021-09-19 04:57:52.527868
b''

Starting computation for size [512, 512, 1024]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 512 -ny 512 -nz 1024
2021-09-19 04:58:36.085011
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 512 -ny 512 -nz 1024
2021-09-19 04:58:46.660868
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 512 -ny 512 -nz 1024
2021-09-19 04:58:56.830124
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 512 -ny 512 -nz 1024
2021-09-19 04:59:07.131403
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 512 -ny 512 -nz 1024
2021-09-19 04:59:17.096169
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 512 -ny 512 -nz 1024
2021-09-19 04:59:28.524660
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 512 -ny 512 -nz 1024
2021-09-19 05:00:08.706165
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 512 -ny 512 -nz 1024
2021-09-19 05:00:19.287531
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 512 -ny 512 -nz 1024
2021-09-19 05:00:29.336592
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 512 -ny 512 -nz 1024
2021-09-19 05:00:41.177939
b''

Starting computation for size [512, 1024, 1024]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 512 -ny 1024 -nz 1024
2021-09-19 05:01:25.266661
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 512 -ny 1024 -nz 1024
2021-09-19 05:01:39.135587
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 512 -ny 1024 -nz 1024
2021-09-19 05:01:51.161464
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 512 -ny 1024 -nz 1024
2021-09-19 05:02:04.592770
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 512 -ny 1024 -nz 1024
2021-09-19 05:02:16.610467
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 512 -ny 1024 -nz 1024
2021-09-19 05:02:32.318618
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 512 -ny 1024 -nz 1024
2021-09-19 05:03:44.569964
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 512 -ny 1024 -nz 1024
2021-09-19 05:03:58.459097
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 512 -ny 1024 -nz 1024
2021-09-19 05:04:10.066372
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 512 -ny 1024 -nz 1024
2021-09-19 05:04:26.465585
b''

Starting computation for size 1024
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024
2021-09-19 05:05:47.101113
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024
2021-09-19 05:06:07.195926
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024
2021-09-19 05:06:23.211872
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024
2021-09-19 05:06:42.782775
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024
2021-09-19 05:06:58.600149
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024
2021-09-19 05:07:22.555174
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024
2021-09-19 05:09:39.091536
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024
2021-09-19 05:09:59.703468
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024
2021-09-19 05:10:15.205085
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024
2021-09-19 05:10:40.614521
b''

Starting computation for size [1024, 1024, 2048]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 2048
2021-09-19 05:13:12.697736
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 2048
2021-09-19 05:13:46.646578
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 2048
2021-09-19 05:14:11.692319
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 2048
2021-09-19 05:14:44.360267
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 2048
2021-09-19 05:15:09.532501
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 2048
2021-09-19 05:15:48.818589
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 2048
2021-09-19 05:18:11.154847
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 2048
2021-09-19 05:18:46.344973
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 2048
2021-09-19 05:19:09.832576
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 2048
2021-09-19 05:19:53.120995
b''

Starting computation for size [1024, 2048, 2048]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 1024 -ny 2048 -nz 2048
2021-09-19 05:22:32.041087
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 1024 -ny 2048 -nz 2048
2021-09-19 05:23:33.378067
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 1024 -ny 2048 -nz 2048
2021-09-19 05:24:17.260843
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 1024 -ny 2048 -nz 2048
2021-09-19 05:25:16.745324
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 1024 -ny 2048 -nz 2048
2021-09-19 05:26:01.435334
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 1024 -ny 2048 -nz 2048
2021-09-19 05:27:14.441127
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 1024 -ny 2048 -nz 2048
2021-09-19 05:31:55.916839
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 1024 -ny 2048 -nz 2048
2021-09-19 05:32:59.224764
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 1024 -ny 2048 -nz 2048
2021-09-19 05:33:38.945042
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 1024 -ny 2048 -nz 2048
2021-09-19 05:34:59.065177
b''

Starting computation for size 2048
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048
2021-09-19 05:40:10.795125
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048
2021-09-19 05:42:05.745932
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048
2021-09-19 05:43:25.109099
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048
2021-09-19 05:45:18.012991
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048
2021-09-19 05:46:41.189581
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048
2021-09-19 05:49:01.789214
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048
2021-09-19 05:58:18.157035
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048
2021-09-19 05:58:34.321428
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048
2021-09-19 05:58:50.142539
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048
2021-09-19 05:59:06.285847
b''

Starting computation for size 128
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 05:59:22.187553
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 05:59:30.096403
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 05:59:38.363371
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 05:59:46.128774
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 05:59:54.295826
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 06:00:02.194362
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 06:00:10.525756
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 06:00:18.348725
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 06:00:26.495103
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 06:00:34.602958
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

Starting computation for size 256
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 06:00:43.107734
b'Result (avg): 5.01733e-09\nResult (max): 5.42241e-08\n'

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 06:00:51.048765
b'Result (avg): 5.01733e-09\nResult (max): 5.42241e-08\n'

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 06:00:59.316902
b'Result (avg): 5.01733e-09\nResult (max): 5.42241e-08\n'

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 06:01:07.234932
b'Result (avg): 5.01733e-09\nResult (max): 5.42241e-08\n'

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 06:01:15.542858
b'Result (avg): 5.01733e-09\nResult (max): 5.42241e-08\n'

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 06:01:23.496752
b'Result (avg): 5.01733e-09\nResult (max): 5.42241e-08\n'

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 06:01:32.351631
b'Result (avg): 5.01733e-09\nResult (max): 5.42241e-08\n'

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 06:01:40.245198
b'Result (avg): 5.01733e-09\nResult (max): 5.42241e-08\n'

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 06:01:48.564829
b'Result (avg): 5.01733e-09\nResult (max): 5.42241e-08\n'

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 06:01:56.549123
b'Result (avg): 5.01733e-09\nResult (max): 5.42241e-08\n'

Starting computation for size 512
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 06:02:05.529643
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 06:02:14.580754
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 06:02:24.001434
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 06:02:33.014343
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 06:02:42.274392
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 06:02:51.484640
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 06:03:03.917846
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 06:03:13.647114
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 06:03:23.410287
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 06:03:33.002064
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

Starting computation for size 1024
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 06:03:45.754644
b'Result (avg): 7.38396e-07\nResult (max): 8.69717e-06\n'

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 06:04:06.252539
b'Result (avg): 7.38396e-07\nResult (max): 8.69717e-06\n'

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 06:04:26.161472
b'Result (avg): 7.38396e-07\nResult (max): 8.69717e-06\n'

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 06:04:46.550448
b'Result (avg): 7.38396e-07\nResult (max): 8.69717e-06\n'

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 06:05:06.307908
b'Result (avg): 7.38396e-07\nResult (max): 8.69717e-06\n'

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 06:05:27.215236
b'Result (avg): 7.38396e-07\nResult (max): 8.69717e-06\n'

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 06:05:57.788156
b'Result (avg): 7.38396e-07\nResult (max): 8.69717e-06\n'

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 06:06:18.342461
b'Result (avg): 7.38396e-07\nResult (max): 8.69717e-06\n'

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 06:06:37.972839
b'Result (avg): 7.38396e-07\nResult (max): 8.69717e-06\n'

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 06:06:59.072426
b'Result (avg): 7.38396e-07\nResult (max): 8.69717e-06\n'

Starting computation for size 2048
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 06:07:30.450598
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 06:09:19.108295
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\n'

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 06:10:37.493898
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 06:12:39.724295
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\n'

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 06:13:57.592923
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 06:16:02.488775
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 06:18:27.670672
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 06:19:53.184344
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\n'

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 06:21:29.478742
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 06:23:08.184435
b''

Slab 1D->2D opt1
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1292607] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1292607] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1292894] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1292894] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1293162] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1293162] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1293678] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1293678] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1293945] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1293945] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1294222] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1294222] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1294489] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1294489] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1294757] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1294757] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1295024] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1295024] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1295305] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1295305] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1295572] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1295572] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1295837] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1295837] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1296116] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1296116] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1296628] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1296628] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1296897] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1296897] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1297164] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1297164] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1297439] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1297439] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1297704] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1297704] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1297972] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1297972] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1298238] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1298238] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1298517] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1298517] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1298787] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1298787] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1299054] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1299054] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1299581] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1299581] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1299847] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1299847] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1300113] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1300113] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1300381] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1300381] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1300662] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1300662] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1300927] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1300927] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1301192] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1301192] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1301467] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1301467] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1301739] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1301739] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1302008] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1302008] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1302513] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1302513] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1302792] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1302792] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1303059] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1303059] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1303322] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1303322] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1303601] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1303601] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1303866] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1303866] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1304132] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1304132] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1304397] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1304397] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1304682] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1304682] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1304949] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1304949] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1305464] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1305464] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1305741] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1305741] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1306010] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1306010] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1306277] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1306277] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1306544] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1306544] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1306823] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1306823] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1307088] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1307088] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1307354] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1307354] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1307635] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1307635] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1307902] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1307902] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1308418] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1308418] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1308683] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1308683] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1308966] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1308966] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1309234] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1309234] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1309500] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1309500] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1309777] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1309777] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1310043] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1310043] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1310312] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1310312] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1310589] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1310589] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1310864] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1310864] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1311379] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1311379] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1311646] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1311646] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1311925] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1311925] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1312192] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1312192] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1312457] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1312457] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1312741] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1312741] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1313010] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1313010] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1313277] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1313277] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1313558] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1313558] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1313825] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1313825] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1314342] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1314342] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1314621] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1314621] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1314891] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1314891] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1315170] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1315170] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1315435] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1315435] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1315704] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1315704] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1315987] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1315987] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1316258] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1316258] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1316525] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1316525] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1316803] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1316803] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1317320] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1317320] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1317597] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1317597] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1317865] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1317865] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1318150] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1318150] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1318420] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1318420] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1318704] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1318704] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1318975] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1318975] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1319254] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1319254] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1319525] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1319525] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1319814] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1319814] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1320346] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1320346] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1320619] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1320619] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1320907] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1320907] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1321189] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1321189] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1321459] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1321459] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1321738] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1321738] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1322023] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1322023] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1322294] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1322294] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1322589] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1322589] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1322875] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1322875] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1323410] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1323410] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1323700] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1323700] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1323990] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1323990] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1324274] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1324274] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1324565] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1324565] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1324850] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1324850] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1325156] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1325156] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1325430] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1325430] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1325751] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1325751] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1326047] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1326047] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1326607] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1326607] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1326902] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1326902] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1327234] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1327234] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1327529] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1327529] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1327845] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1327845] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1328153] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1328153] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1328466] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1328466] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1328763] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1328763] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1329125] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1329125] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1329460] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1329460] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1330108] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1330108] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1330430] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1330430] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1330823] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1330823] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1331134] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1331134] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n517:969064:0:969064] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x148c8c400000)
[uc2n517:969062:0:969062] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x153e06400000)
[uc2n517:969068:0:969068] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14c0de400000)
[uc2n517:969067:0:969067] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14cb94400000)
[uc2n517:969063:0:969063] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14a938400000)
[uc2n517:969061:0:969061] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x145c20400000)
[uc2n517:969065:0:969065] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x1479b6400000)
[uc2n517:969066:0:969066] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x151d58400000)
[uc2n515:1331148:0:1331148] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14f7e4400000)
[uc2n515:1331150:0:1331150] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14dccc400000)
[uc2n515:1331154:0:1331154] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x1502e6400000)
[uc2n515:1331153:0:1331153] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x147ed4400000)
[uc2n515:1331149:0:1331149] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14ec8c400000)
[uc2n515:1331147:0:1331147] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14ea1c400000)
[uc2n515:1331151:0:1331151] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x151e24400000)
[uc2n515:1331152:0:1331152] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x149a96400000)
==== backtrace (tid:1331150) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x0000000000038ae8 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::All2All_Sync()  ???:0
11 0x0000000000038312 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::execR2C()  ???:0
12 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
13 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
14 0x0000000000403591 main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1331147) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x0000000000038ae8 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::All2All_Sync()  ???:0
11 0x0000000000038312 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::execR2C()  ???:0
12 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
13 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
14 0x0000000000403591 main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
==== backtrace (tid:1331153) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x0000000000038ae8 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::All2All_Sync()  ???:0
11 0x0000000000038312 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::execR2C()  ???:0
12 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
13 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
14 0x0000000000403591 main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
=================================
==== backtrace (tid:1331148) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x0000000000038ae8 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::All2All_Sync()  ???:0
11 0x0000000000038312 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::execR2C()  ???:0
12 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
13 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
14 0x0000000000403591 main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1331149) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x0000000000038ae8 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::All2All_Sync()  ???:0
11 0x0000000000038312 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::execR2C()  ???:0
12 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
13 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
14 0x0000000000403591 main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1331154) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x0000000000038ae8 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::All2All_Sync()  ???:0
11 0x0000000000038312 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::execR2C()  ???:0
12 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
13 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
14 0x0000000000403591 main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1331152) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x0000000000038ae8 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::All2All_Sync()  ???:0
11 0x0000000000038312 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::execR2C()  ???:0
12 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
13 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
14 0x0000000000403591 main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1331151) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x0000000000038ae8 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::All2All_Sync()  ???:0
11 0x0000000000038312 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::execR2C()  ???:0
12 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
13 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
14 0x0000000000403591 main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 969066) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x0000000000038ae8 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::All2All_Sync()  ???:0
 9 0x0000000000038312 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::execR2C()  ???:0
10 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
11 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
12 0x0000000000403591 main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 969068) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x0000000000038ae8 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::All2All_Sync()  ???:0
 9 0x0000000000038312 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::execR2C()  ???:0
10 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
11 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
12 0x0000000000403591 main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 969067) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x0000000000038ae8 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::All2All_Sync()  ???:0
 9 0x0000000000038312 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::execR2C()  ???:0
10 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
11 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
12 0x0000000000403591 main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 969061) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x0000000000038ae8 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::All2All_Sync()  ???:0
 9 0x0000000000038312 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::execR2C()  ???:0
10 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
11 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
12 0x0000000000403591 main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 969063) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x0000000000038ae8 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::All2All_Sync()  ???:0
 9 0x0000000000038312 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::execR2C()  ???:0
10 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
11 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
12 0x0000000000403591 main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 969064) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x0000000000038ae8 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::All2All_Sync()  ???:0
 9 0x0000000000038312 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::execR2C()  ???:0
10 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
11 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
12 0x0000000000403591 main()  ???:0
==== backtrace (tid: 969062) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x0000000000038ae8 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::All2All_Sync()  ???:0
 9 0x0000000000038312 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::execR2C()  ???:0
10 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
11 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
12 0x0000000000403591 main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
=================================
==== backtrace (tid: 969065) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x0000000000038ae8 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::All2All_Sync()  ???:0
 9 0x0000000000038312 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::execR2C()  ???:0
10 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
11 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
12 0x0000000000403591 main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec noticed that process rank 14 with PID 969067 on node uc2n517 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1331490] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1331490] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
The call to cuMemcpyAsync failed. This is a unrecoverable error and will
cause the program to abort.
  cuMemcpyAsync(0x14bab4c00000, 0x14bcc0c00000, 131072) returned value 1
Check the cuda.h file for what the return value means.
--------------------------------------------------------------------------
[uc2n517.localdomain:969428] CUDA: Error in cuMemcpy: res=-1, dest=0x14bab4c00000, src=0x14bcc0c00000, size=131072
[uc2n517:969428] *** Process received signal ***
[uc2n517:969428] Signal: Aborted (6)
[uc2n517:969428] Signal code:  (-6)
[uc2n517:969428] [ 0] /usr/lib64/libpthread.so.0(+0x12dd0)[0x14bf32f2fdd0]
[uc2n517:969428] [ 1] /usr/lib64/libc.so.6(gsignal+0x10f)[0x14bf32b9270f]
[uc2n517:969428] [ 2] /usr/lib64/libc.so.6(abort+0x127)[0x14bf32b7cb25]
[uc2n517:969428] [ 3] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libopen-pal.so.40(+0x7c58f)[0x14bf31afc58f]
[uc2n517:969428] [ 4] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libopen-pal.so.40(+0x77119)[0x14bf31af7119]
[uc2n517:969428] [ 5] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libmpi.so.40(ompi_datatype_sndrcv+0x50a)[0x14bf3e935b2a]
[uc2n517:969428] [ 6] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libmpi.so.40(ompi_coll_base_alltoallv_intra_basic_linear+0x26e)[0x14bf3e98a80e]
[uc2n517:969428] [ 7] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libmpi.so.40(ompi_coll_tuned_alltoallv_intra_dec_fixed+0x42)[0x14bf3e996702]
[uc2n517:969428] [ 8] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libmpi.so.40(MPI_Alltoallv+0x1ab)[0x14bf3e93842b]
[uc2n517:969428] [ 9] /home/st/st_us-051200/st_st160727/DistributedFFT/build/libslab_decomp.so(_ZN28MPIcuFFT_Slab_Z_Then_YX_Opt1IdE12All2All_SyncEPvb+0xf8)[0x14bf4909fae8]
[uc2n517:969428] [10] /home/st/st_us-051200/st_st160727/DistributedFFT/build/libslab_decomp.so(_ZN28MPIcuFFT_Slab_Z_Then_YX_Opt1IdE7execR2CEPvPKv+0x122)[0x14bf4909f312]
[uc2n517:969428] [11] /home/st/st_us-051200/st_st160727/DistributedFFT/build/libslab_tests.so(_ZN27Tests_Slab_Random_Z_Then_YXIdE9testcase0Eii+0x49c)[0x14bf492e4076]
[uc2n517:969428] [12] /home/st/st_us-051200/st_st160727/DistributedFFT/build/libslab_tests.so(_ZN27Tests_Slab_Random_Z_Then_YXIdE3runEiii+0x2f)[0x14bf492e3b61]
[uc2n517:969428] [13] slab[0x403591]
[uc2n517:969428] [14] /usr/lib64/libc.so.6(__libc_start_main+0xf3)[0x14bf32b7e6a3]
[uc2n517:969428] [15] slab[0x4039ee]
[uc2n517:969428] *** End of error message ***
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec noticed that process rank 15 with PID 969428 on node uc2n517 exited on signal 6 (Aborted).
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1331758] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1331758] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n517:969715:0:969715] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14a45c400000)
[uc2n517:969714:0:969714] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14bc4e400000)
[uc2n517:969708:0:969708] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14d2c8400000)
[uc2n517:969709:0:969709] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x149b88400000)
[uc2n517:969710:0:969710] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14536c400000)
[uc2n517:969711:0:969711] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14841e400000)
[uc2n517:969712:0:969712] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14c384400000)
[uc2n517:969713:0:969713] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14f77c400000)
==== backtrace (tid: 969708) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x000000000008d803 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x00000000000392d3 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::All2All_MPIType()  ???:0
 8 0x0000000000038312 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::execR2C()  ???:0
 9 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
10 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 969709) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x000000000008d803 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x00000000000392d3 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::All2All_MPIType()  ???:0
 8 0x0000000000038312 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::execR2C()  ???:0
 9 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
10 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 969710) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x000000000008d803 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x00000000000392d3 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::All2All_MPIType()  ???:0
 8 0x0000000000038312 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::execR2C()  ???:0
 9 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
10 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 969714) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x000000000008d803 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x00000000000392d3 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::All2All_MPIType()  ???:0
 8 0x0000000000038312 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::execR2C()  ???:0
 9 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
10 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 969715) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x000000000008d803 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x00000000000392d3 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::All2All_MPIType()  ???:0
 8 0x0000000000038312 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::execR2C()  ???:0
 9 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
10 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 969711) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x000000000008d803 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x00000000000392d3 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::All2All_MPIType()  ???:0
 8 0x0000000000038312 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::execR2C()  ???:0
 9 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
10 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 969712) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x000000000008d803 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x00000000000392d3 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::All2All_MPIType()  ???:0
 8 0x0000000000038312 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::execR2C()  ???:0
 9 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
10 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 969713) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x000000000008d803 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x00000000000392d3 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::All2All_MPIType()  ???:0
 8 0x0000000000038312 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::execR2C()  ???:0
 9 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
10 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
[uc2n515:1331788:0:1331788] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14aea8400000)
[uc2n515:1331789:0:1331789] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x148000400000)
[uc2n515:1331784:0:1331784] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14ddd4400000)
[uc2n515:1331785:0:1331785] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14e1c8400000)
[uc2n515:1331782:0:1331782] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x150b72400000)
[uc2n515:1331783:0:1331783] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14719c400000)
[uc2n515:1331786:0:1331786] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x154a8c400000)
[uc2n515:1331787:0:1331787] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x149ba6400000)
==== backtrace (tid:1331788) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x0000000000121b20 mca_coll_basic_alltoallw_intra()  ???:0
 8 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 9 0x00000000000392d3 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::All2All_MPIType()  ???:0
10 0x0000000000038312 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::execR2C()  ???:0
11 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
12 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
13 0x0000000000403591 main()  ???:0
14 0x00000000000236a3 __libc_start_main()  ???:0
15 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1331789) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x0000000000121b20 mca_coll_basic_alltoallw_intra()  ???:0
 8 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 9 0x00000000000392d3 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::All2All_MPIType()  ???:0
10 0x0000000000038312 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::execR2C()  ???:0
11 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
12 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
13 0x0000000000403591 main()  ???:0
14 0x00000000000236a3 __libc_start_main()  ???:0
15 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1331782) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x0000000000121b20 mca_coll_basic_alltoallw_intra()  ???:0
 8 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 9 0x00000000000392d3 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::All2All_MPIType()  ???:0
10 0x0000000000038312 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::execR2C()  ???:0
11 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
12 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
13 0x0000000000403591 main()  ???:0
14 0x00000000000236a3 __libc_start_main()  ???:0
15 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1331784) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x0000000000121b20 mca_coll_basic_alltoallw_intra()  ???:0
 8 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 9 0x00000000000392d3 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::All2All_MPIType()  ???:0
10 0x0000000000038312 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::execR2C()  ???:0
11 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
12 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
13 0x0000000000403591 main()  ???:0
14 0x00000000000236a3 __libc_start_main()  ???:0
15 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1331785) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x0000000000121b20 mca_coll_basic_alltoallw_intra()  ???:0
 8 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 9 0x00000000000392d3 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::All2All_MPIType()  ???:0
10 0x0000000000038312 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::execR2C()  ???:0
11 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
12 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
13 0x0000000000403591 main()  ???:0
14 0x00000000000236a3 __libc_start_main()  ???:0
15 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1331783) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x0000000000121b20 mca_coll_basic_alltoallw_intra()  ???:0
 8 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 9 0x00000000000392d3 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::All2All_MPIType()  ???:0
10 0x0000000000038312 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::execR2C()  ???:0
11 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
12 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
13 0x0000000000403591 main()  ???:0
14 0x00000000000236a3 __libc_start_main()  ???:0
15 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1331786) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x0000000000121b20 mca_coll_basic_alltoallw_intra()  ???:0
 8 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 9 0x00000000000392d3 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::All2All_MPIType()  ???:0
10 0x0000000000038312 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::execR2C()  ???:0
11 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
12 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
13 0x0000000000403591 main()  ???:0
14 0x00000000000236a3 __libc_start_main()  ???:0
15 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1331787) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x0000000000121b20 mca_coll_basic_alltoallw_intra()  ???:0
 8 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 9 0x00000000000392d3 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::All2All_MPIType()  ???:0
10 0x0000000000038312 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::execR2C()  ???:0
11 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
12 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
13 0x0000000000403591 main()  ???:0
14 0x00000000000236a3 __libc_start_main()  ???:0
15 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec noticed that process rank 11 with PID 969711 on node uc2n517 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1332103] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1332103] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
The call to cuMemcpyAsync failed. This is a unrecoverable error and will
cause the program to abort.
  cuMemcpyAsync(0x8b2fe60, 0x147b8a800000, 65536) returned value 1
Check the cuda.h file for what the return value means.
--------------------------------------------------------------------------
[uc2n517.localdomain:970063] CUDA: Error in cuMemcpy: res=-1, dest=0x8b2fe60, src=0x147b8a800000, size=65536
[uc2n517:970063] *** Process received signal ***
[uc2n517:970063] Signal: Aborted (6)
[uc2n517:970063] Signal code:  (-6)
[uc2n517:970063] [ 0] /usr/lib64/libpthread.so.0(+0x12dd0)[0x147dfd631dd0]
[uc2n517:970063] [ 1] /usr/lib64/libc.so.6(gsignal+0x10f)[0x147dfd29470f]
[uc2n517:970063] [ 2] /usr/lib64/libc.so.6(abort+0x127)[0x147dfd27eb25]
[uc2n517:970063] [ 3] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libopen-pal.so.40(+0x7c375)[0x147dfc1fe375]
[uc2n517:970063] [ 4] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libopen-pal.so.40(opal_convertor_pack+0xd8)[0x147dfc1f51e8]
[uc2n517:970063] [ 5] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libmpi.so.40(ompi_datatype_sndrcv+0x1e3)[0x147e09037803]
[uc2n517:970063] [ 6] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libmpi.so.40(mca_coll_basic_alltoallw_intra+0xa4)[0x147e090cb984]
[uc2n517:970063] [ 7] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libmpi.so.40(MPI_Alltoallw+0x1eb)[0x147e0903accb]
[uc2n517:970063] [ 8] /home/st/st_us-051200/st_st160727/DistributedFFT/build/libslab_decomp.so(_ZN28MPIcuFFT_Slab_Z_Then_YX_Opt1IdE15All2All_MPITypeEPvb+0x153)[0x147e137a22d3]
[uc2n517:970063] [ 9] /home/st/st_us-051200/st_st160727/DistributedFFT/build/libslab_decomp.so(_ZN28MPIcuFFT_Slab_Z_Then_YX_Opt1IdE7execR2CEPvPKv+0x122)[0x147e137a1312]
[uc2n517:970063] [10] /home/st/st_us-051200/st_st160727/DistributedFFT/build/libslab_tests.so(_ZN27Tests_Slab_Random_Z_Then_YXIdE9testcase0Eii+0x49c)[0x147e139e6076]
[uc2n517:970063] [11] /home/st/st_us-051200/st_st160727/DistributedFFT/build/libslab_tests.so(_ZN27Tests_Slab_Random_Z_Then_YXIdE3runEiii+0x2f)[0x147e139e5b61]
[uc2n517:970063] [12] slab[0x403591]
[uc2n517:970063] [13] /usr/lib64/libc.so.6(__libc_start_main+0xf3)[0x147dfd2806a3]
[uc2n517:970063] [14] slab[0x4039ee]
[uc2n517:970063] *** End of error message ***
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec noticed that process rank 15 with PID 970063 on node uc2n517 exited on signal 6 (Aborted).
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1332383] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1332383] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1332648] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1332648] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1332918] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1332918] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1333210] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1333210] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1333512] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1333512] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1333782] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1333782] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1334049] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1334049] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1334330] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1334330] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1334593] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1334593] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1334860] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1334860] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1335127] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1335127] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1335407] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1335407] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1335675] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1335675] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1335966] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1335966] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1336266] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1336266] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1336534] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1336534] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1336801] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1336801] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1337078] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1337078] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1337345] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1337345] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1337611] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1337611] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1337878] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1337878] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1338159] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1338159] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1338428] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1338428] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1338719] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1338719] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1339018] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1339018] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1339286] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1339286] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1339552] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1339552] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1339831] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1339831] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1340096] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1340096] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1340366] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1340366] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1340634] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1340634] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1340919] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1340919] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1341207] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1341207] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1341506] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1341506] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1341803] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1341803] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1342087] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1342087] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1342360] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1342360] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1342648] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1342648] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1342932] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1342932] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1343204] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1343204] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1343485] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1343485] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1343845] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1343845] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[10953,1],14]
  Exit code:    1
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1344158] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1344158] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1344552] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1344552] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[12164,1],7]
  Exit code:    1
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1344880] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1344880] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1345251] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1345251] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1345593] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1345593] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515:1345613:0:1345613] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14a354400000)
==== backtrace (tid:1345613) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x0000000000038ae8 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::All2All_Sync()  ???:0
11 0x0000000000038312 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::execR2C()  ???:0
12 0x000000000002b234 Tests_Slab_Random_Z_Then_YX<double>::testcase4()  ???:0
13 0x0000000000028bd1 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
14 0x0000000000403591 main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
[uc2n517:983745:0:983745] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14ad24400000)
==== backtrace (tid: 983745) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x0000000000038ae8 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::All2All_Sync()  ???:0
 9 0x0000000000038312 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::execR2C()  ???:0
10 0x000000000002b234 Tests_Slab_Random_Z_Then_YX<double>::testcase4()  ???:0
11 0x0000000000028bd1 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
12 0x0000000000403591 main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
[uc2n517:983746:0:983746] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x148824400000)
==== backtrace (tid: 983746) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x0000000000038ae8 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::All2All_Sync()  ???:0
 9 0x0000000000038312 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::execR2C()  ???:0
10 0x000000000002b234 Tests_Slab_Random_Z_Then_YX<double>::testcase4()  ???:0
11 0x0000000000028bd1 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
12 0x0000000000403591 main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
[uc2n515:1345612:0:1345612] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x149750400000)
==== backtrace (tid:1345612) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x0000000000038ae8 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::All2All_Sync()  ???:0
11 0x0000000000038312 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::execR2C()  ???:0
12 0x000000000002b234 Tests_Slab_Random_Z_Then_YX<double>::testcase4()  ???:0
13 0x0000000000028bd1 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
14 0x0000000000403591 main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
[uc2n517:983740:0:983740] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x1531d8400000)
==== backtrace (tid: 983740) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x0000000000038ae8 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::All2All_Sync()  ???:0
 9 0x0000000000038312 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::execR2C()  ???:0
10 0x000000000002b234 Tests_Slab_Random_Z_Then_YX<double>::testcase4()  ???:0
11 0x0000000000028bd1 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
12 0x0000000000403591 main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
[uc2n517:983739:0:983739] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14f8d4400000)
==== backtrace (tid: 983739) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x0000000000038ae8 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::All2All_Sync()  ???:0
 9 0x0000000000038312 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::execR2C()  ???:0
10 0x000000000002b234 Tests_Slab_Random_Z_Then_YX<double>::testcase4()  ???:0
11 0x0000000000028bd1 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
12 0x0000000000403591 main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
[uc2n515:1345606:0:1345606] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x150b12400000)
==== backtrace (tid:1345606) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x0000000000038ae8 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::All2All_Sync()  ???:0
11 0x0000000000038312 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::execR2C()  ???:0
12 0x000000000002b234 Tests_Slab_Random_Z_Then_YX<double>::testcase4()  ???:0
13 0x0000000000028bd1 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
14 0x0000000000403591 main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
mpiexec noticed that process rank 7 with PID 0 on node uc2n515 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1345958] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1345958] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[8714,1],14]
  Exit code:    1
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1346296] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1346296] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515:1346343:0:1346343] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x145d6c400000)
==== backtrace (tid:1346343) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x0000000000121b20 mca_coll_basic_alltoallw_intra()  ???:0
 8 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 9 0x00000000000392d3 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::All2All_MPIType()  ???:0
10 0x0000000000038312 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::execR2C()  ???:0
11 0x000000000002b234 Tests_Slab_Random_Z_Then_YX<double>::testcase4()  ???:0
12 0x0000000000028bd1 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
13 0x0000000000403591 main()  ???:0
14 0x00000000000236a3 __libc_start_main()  ???:0
15 0x00000000004039ee _start()  ???:0
=================================
[uc2n517:984480:0:984480] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x153f78400000)
[uc2n517:984481:0:984481] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14ee8c400000)
==== backtrace (tid: 984480) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x000000000008d803 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x00000000000392d3 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::All2All_MPIType()  ???:0
 8 0x0000000000038312 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::execR2C()  ???:0
 9 0x000000000002b234 Tests_Slab_Random_Z_Then_YX<double>::testcase4()  ???:0
10 0x0000000000028bd1 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 984481) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x000000000008d803 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x00000000000392d3 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::All2All_MPIType()  ???:0
 8 0x0000000000038312 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::execR2C()  ???:0
 9 0x000000000002b234 Tests_Slab_Random_Z_Then_YX<double>::testcase4()  ???:0
10 0x0000000000028bd1 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
[uc2n515:1346342:0:1346342] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14b418400000)
==== backtrace (tid:1346342) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x0000000000121b20 mca_coll_basic_alltoallw_intra()  ???:0
 8 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 9 0x00000000000392d3 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::All2All_MPIType()  ???:0
10 0x0000000000038312 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::execR2C()  ???:0
11 0x000000000002b234 Tests_Slab_Random_Z_Then_YX<double>::testcase4()  ???:0
12 0x0000000000028bd1 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
13 0x0000000000403591 main()  ???:0
14 0x00000000000236a3 __libc_start_main()  ???:0
15 0x00000000004039ee _start()  ???:0
=================================
[uc2n517:984475:0:984475] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x144978400000)
[uc2n517:984474:0:984474] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x150c40400000)
==== backtrace (tid: 984475) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x000000000008d803 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x00000000000392d3 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::All2All_MPIType()  ???:0
 8 0x0000000000038312 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::execR2C()  ???:0
 9 0x000000000002b234 Tests_Slab_Random_Z_Then_YX<double>::testcase4()  ???:0
10 0x0000000000028bd1 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid: 984474) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x000000000008d803 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x00000000000392d3 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::All2All_MPIType()  ???:0
 8 0x0000000000038312 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::execR2C()  ???:0
 9 0x000000000002b234 Tests_Slab_Random_Z_Then_YX<double>::testcase4()  ???:0
10 0x0000000000028bd1 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
[uc2n515:1346336:0:1346336] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14b012400000)
==== backtrace (tid:1346336) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x0000000000121b20 mca_coll_basic_alltoallw_intra()  ???:0
 8 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 9 0x00000000000392d3 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::All2All_MPIType()  ???:0
10 0x0000000000038312 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::execR2C()  ???:0
11 0x000000000002b234 Tests_Slab_Random_Z_Then_YX<double>::testcase4()  ???:0
12 0x0000000000028bd1 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
13 0x0000000000403591 main()  ???:0
14 0x00000000000236a3 __libc_start_main()  ???:0
15 0x00000000004039ee _start()  ???:0
=================================
[uc2n515:1346337:0:1346337] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14deae400000)
==== backtrace (tid:1346337) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x0000000000121b20 mca_coll_basic_alltoallw_intra()  ???:0
 8 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 9 0x00000000000392d3 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::All2All_MPIType()  ???:0
10 0x0000000000038312 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::execR2C()  ???:0
11 0x000000000002b234 Tests_Slab_Random_Z_Then_YX<double>::testcase4()  ???:0
12 0x0000000000028bd1 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
13 0x0000000000403591 main()  ???:0
14 0x00000000000236a3 __libc_start_main()  ???:0
15 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
mpiexec noticed that process rank 7 with PID 0 on node uc2n515 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1346689] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1346689] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n517:984865:0:984865] Caught signal 11 (Segmentation fault: invalid permissions for mapped object at address 0x151bc4400000)
==== backtrace (tid: 984865) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x000000000008d803 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x00000000000392d3 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::All2All_MPIType()  ???:0
 8 0x0000000000038312 MPIcuFFT_Slab_Z_Then_YX_Opt1<double>::execR2C()  ???:0
 9 0x000000000002b234 Tests_Slab_Random_Z_Then_YX<double>::testcase4()  ???:0
10 0x0000000000028bd1 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec noticed that process rank 15 with PID 984865 on node uc2n517 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
Starting computation for size 128
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128
2021-09-19 06:24:43.954747
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128
2021-09-19 06:25:08.735909
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128
2021-09-19 06:25:17.277226
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128
2021-09-19 06:25:25.323544
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128
2021-09-19 06:25:33.766714
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128
2021-09-19 06:25:41.661402
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128
2021-09-19 06:25:50.273158
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128
2021-09-19 06:25:58.271318
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128
2021-09-19 06:26:06.896748
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128
2021-09-19 06:26:14.804475
b''

Starting computation for size [128, 128, 256]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256
2021-09-19 06:26:23.496322
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256
2021-09-19 06:26:31.348824
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256
2021-09-19 06:26:39.889686
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256
2021-09-19 06:26:47.922252
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256
2021-09-19 06:26:56.576544
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256
2021-09-19 06:27:04.476910
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256
2021-09-19 06:27:13.200348
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256
2021-09-19 06:27:21.087030
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256
2021-09-19 06:27:29.491349
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256
2021-09-19 06:27:37.501434
b''

Starting computation for size [128, 256, 256]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256
2021-09-19 06:27:46.150367
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256
2021-09-19 06:27:54.208911
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256
2021-09-19 06:28:02.867924
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256
2021-09-19 06:28:10.950126
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256
2021-09-19 06:28:19.487280
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256
2021-09-19 06:28:27.461848
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256
2021-09-19 06:28:36.152308
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256
2021-09-19 06:28:44.051981
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256
2021-09-19 06:28:52.575653
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256
2021-09-19 06:29:00.440987
b''

Starting computation for size 256
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256
2021-09-19 06:29:08.977598
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256
2021-09-19 06:29:17.091784
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256
2021-09-19 06:29:25.798518
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256
2021-09-19 06:29:34.639344
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256
2021-09-19 06:29:45.582803
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256
2021-09-19 06:29:53.607388
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256
2021-09-19 06:30:02.374196
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256
2021-09-19 06:30:10.403428
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256
2021-09-19 06:30:19.063254
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256
2021-09-19 06:30:27.241617
b''

Starting computation for size [256, 256, 512]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512
2021-09-19 06:30:36.063888
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512
2021-09-19 06:30:44.253971
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512
2021-09-19 06:30:53.181375
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512
2021-09-19 06:31:01.403294
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512
2021-09-19 06:31:10.215124
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512
2021-09-19 06:31:18.353043
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512
2021-09-19 06:31:27.128449
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512
2021-09-19 06:31:35.349917
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512
2021-09-19 06:31:44.074635
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512
2021-09-19 06:31:52.398771
b''

Starting computation for size [256, 512, 512]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512
2021-09-19 06:32:01.308155
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512
2021-09-19 06:32:09.879092
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512
2021-09-19 06:32:18.928484
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512
2021-09-19 06:32:27.582443
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512
2021-09-19 06:32:36.490561
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512
2021-09-19 06:32:45.443582
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512
2021-09-19 06:32:54.479743
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512
2021-09-19 06:33:03.142426
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512
2021-09-19 06:33:12.161909
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512
2021-09-19 06:33:20.976443
b''

Starting computation for size 512
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512
2021-09-19 06:33:29.952391
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512
2021-09-19 06:33:39.285236
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512
2021-09-19 06:33:48.733341
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512
2021-09-19 06:33:57.998152
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512
2021-09-19 06:34:07.345237
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512
2021-09-19 06:34:17.399916
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512
2021-09-19 06:34:26.901186
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512
2021-09-19 06:34:36.268573
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512
2021-09-19 06:34:45.533386
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512
2021-09-19 06:34:55.307216
b''

Starting computation for size [512, 512, 1024]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-19 06:35:04.829095
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-19 06:35:15.814170
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-19 06:35:26.182526
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-19 06:35:36.945150
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-19 06:35:47.202955
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-19 06:35:59.331375
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-19 06:36:09.654705
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-19 06:36:20.718821
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-19 06:36:31.126219
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-19 06:36:42.897292
b''

Starting computation for size [512, 1024, 1024]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-19 06:36:53.478425
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-19 06:37:07.933256
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-19 06:37:20.276252
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-19 06:37:34.235615
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-19 06:37:46.538664
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-19 06:38:03.077876
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-19 06:38:15.180517
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-19 06:38:29.740638
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-19 06:38:42.101657
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-19 06:38:57.727213
b''

Starting computation for size 1024
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-19 06:39:10.490128
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-19 06:39:32.049893
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-19 06:39:53.308870
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-19 06:40:13.674118
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-19 06:40:29.318877
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-19 06:40:53.947232
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-19 06:41:10.019401
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-19 06:41:31.079943
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-19 06:41:46.937652
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-19 06:42:10.290265
b''

Starting computation for size [1024, 1024, 2048]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-19 06:42:27.000963
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-19 06:43:03.943527
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-19 06:43:28.692718
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-19 06:44:01.937651
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-19 06:44:25.825157
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-19 06:45:07.605454
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-19 06:45:31.812668
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-19 06:46:07.209412
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-19 06:46:31.441115
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-19 06:47:10.432321
b''

Starting computation for size [1024, 2048, 2048]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-19 06:47:36.170168
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-19 06:48:40.344640
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-19 06:49:20.488115
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-19 06:50:21.021798
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-19 06:51:01.728095
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-19 06:52:18.020389
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-19 06:52:58.981286
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-19 06:54:02.771537
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-19 06:54:43.264420
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-19 06:55:53.774825
b''

Starting computation for size 2048
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-19 06:56:37.299448
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-19 06:58:38.180870
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-19 06:59:49.420186
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-19 07:01:42.404791
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-19 07:02:54.226835
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-19 07:05:26.507831
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-19 07:06:38.312624
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-19 07:06:54.500641
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-19 07:07:10.762776
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-19 07:07:27.511121
b''

Starting computation for size 128
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 07:07:43.837901
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 07:07:52.953414
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 07:08:01.543901
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 07:08:09.731304
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 07:08:18.344200
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 07:08:26.710327
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 07:08:35.340433
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 07:08:43.575334
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 07:08:52.144963
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 07:09:00.583624
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

Starting computation for size 256
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 07:09:09.329815
b'Result (avg): 5.19268e-09\nResult (max): 5.08699e-08\n'

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 07:09:17.810496
b'Result (avg): 5.19268e-09\nResult (max): 5.08699e-08\n'

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 07:09:26.642592
b'Result (avg): 5.19268e-09\nResult (max): 5.08699e-08\n'

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 07:09:34.948874
b'Result (avg): 5.19268e-09\nResult (max): 5.08699e-08\n'

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 07:09:46.723653
b'Result (avg): 5.19268e-09\nResult (max): 5.08699e-08\n'

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 07:09:55.191446
b'Result (avg): 5.19268e-09\nResult (max): 5.08699e-08\n'

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 07:10:03.962926
b'Result (avg): 5.19268e-09\nResult (max): 5.08699e-08\n'

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 07:10:12.277239
b'Result (avg): 5.19268e-09\nResult (max): 5.08699e-08\n'

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 07:10:20.872131
b'Result (avg): 5.19268e-09\nResult (max): 5.08699e-08\n'

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 07:10:29.630201
b'Result (avg): 5.19268e-09\nResult (max): 5.08699e-08\n'

Starting computation for size 512
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 07:10:38.525620
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 07:10:47.882880
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 07:10:57.230836
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 07:11:06.264326
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 07:11:15.701388
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 07:11:24.921987
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 07:11:34.321084
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 07:11:43.567248
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 07:11:52.897463
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 07:12:02.074333
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

Starting computation for size 1024
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 07:12:11.397955
b'Result (avg): 7.69765e-07\nResult (max): 9.22816e-06\n'

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 07:12:31.435983
b'Result (avg): 7.69765e-07\nResult (max): 9.22816e-06\n'

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 07:12:50.778057
b'Result (avg): 7.69765e-07\nResult (max): 9.22816e-06\n'

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 07:13:10.659617
b'Result (avg): 7.69765e-07\nResult (max): 9.22816e-06\n'

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 07:13:29.988853
b'Result (avg): 7.69765e-07\nResult (max): 9.22816e-06\n'

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 07:13:50.069797
b'Result (avg): 7.69765e-07\nResult (max): 9.22816e-06\n'

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 07:14:09.359115
b'Result (avg): 7.69765e-07\nResult (max): 9.22816e-06\n'

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 07:14:29.385153
b'Result (avg): 7.69765e-07\nResult (max): 9.22816e-06\n'

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 07:14:48.720498
b'Result (avg): 7.69765e-07\nResult (max): 9.22816e-06\n'

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 07:15:09.077155
b'Result (avg): 7.69765e-07\nResult (max): 9.22816e-06\n'

Starting computation for size 2048
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 07:15:28.481883
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 07:17:16.114269
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\n'

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 07:18:33.987963
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 07:20:35.555037
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\n'

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 07:21:54.295843
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 07:23:57.541324
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 07:25:37.709851
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 07:27:01.873595
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\n'

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 07:28:35.792019
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/forward -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 07:30:16.087761
b''

Slab 2D->1D default (inverse)
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1347029] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1347029] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1347311] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1347311] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1347575] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1347575] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1348090] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1348090] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1348357] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1348357] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1348637] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1348637] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1348904] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1348904] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1349171] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1349171] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1349438] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1349438] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1349717] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1349717] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1349982] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1349982] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1350248] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1350248] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1350515] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1350515] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1351042] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1351042] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1351309] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1351309] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1351574] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1351574] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1351852] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1351852] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1352118] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1352118] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1352386] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1352386] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1352651] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1352651] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1352930] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1352930] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1353192] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1353192] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1353460] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1353460] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1353975] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1353975] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1354257] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1354257] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1354521] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1354521] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1354785] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1354785] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1355052] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1355052] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1355329] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1355329] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1355597] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1355597] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1355862] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1355862] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1356129] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1356129] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1356406] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1356406] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1356923] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1356923] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1357188] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1357188] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1357470] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1357470] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1357733] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1357733] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1358005] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1358005] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1358270] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1358270] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1358549] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1358549] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1358813] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1358813] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1359080] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1359080] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1359347] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1359347] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1359874] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1359874] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1360140] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1360140] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1360407] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1360407] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1360674] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1360674] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1360952] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1360952] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1361222] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1361222] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1361487] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1361487] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1361767] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1361767] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1362032] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1362032] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1362300] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1362300] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1362815] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1362815] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1363094] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1363094] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1363361] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1363361] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1363629] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1363629] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1363910] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1363910] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1364177] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1364177] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1364444] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1364444] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1364710] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1364710] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1364990] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1364990] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1365257] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1365257] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1365772] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1365772] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1366053] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1366053] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1366320] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1366320] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1366589] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1366589] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1366873] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1366873] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1367139] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1367139] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1367409] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1367409] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1367675] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1367675] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1367953] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1367953] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1368221] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1368221] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1368733] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1368733] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1369012] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1369012] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1369280] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1369280] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1369549] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1369549] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1369832] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1369832] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1370098] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1370098] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1370379] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1370379] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1370647] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1370647] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1370917] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1370917] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1371193] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1371193] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1371713] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1371713] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1371978] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1371978] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1372261] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1372261] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1372530] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1372530] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1372810] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1372810] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1373081] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1373081] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1373362] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1373362] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1373629] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1373629] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1373915] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1373915] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1374187] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1374187] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1374717] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1374717] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1374989] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1374989] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1375272] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1375272] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1375560] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1375560] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1375832] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1375832] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1376109] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1376109] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1376379] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1376379] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1376663] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1376663] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1376960] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1376960] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1377243] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1377243] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1377778] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1377778] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1378054] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1378054] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1378355] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1378355] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1378629] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1378629] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1378932] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1378932] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1379205] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1379205] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1379493] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1379493] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1379784] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1379784] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1380099] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1380099] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1380386] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1380386] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1380946] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1380946] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1381239] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1381239] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1381557] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1381557] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1381861] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1381861] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1382174] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1382174] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1382467] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1382467] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1382780] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1382780] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1383117] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1383117] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1383478] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1383478] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1383789] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1383789] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1384379] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1384379] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1384686] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1384686] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1385054] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1385054] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1385367] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1385367] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n517:1024174:0:1024174] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x152de43c0000)
[uc2n517:1024171:0:1024171] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x15441c300000)
[uc2n517:1024170:0:1024170] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x154e282c0000)
[uc2n517:1024167:0:1024167] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x153970200000)
[uc2n517:1024169:0:1024169] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14aaac280000)
[uc2n517:1024173:0:1024173] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14e47c380000)
[uc2n517:1024172:0:1024172] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14b0ac340000)
[uc2n517:1024168:0:1024168] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14e88c240000)
==== backtrace (tid:1024174) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001088e MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
 9 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
10 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
11 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1024171) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001088e MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
 9 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
10 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
11 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1024170) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001088e MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
 9 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
10 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
11 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1024167) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001088e MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
 9 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
10 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
11 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1024169) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001088e MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
 9 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
10 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
11 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1024172) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001088e MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
 9 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
10 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
11 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1024173) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001088e MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
 9 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
10 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
11 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1024168) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001088e MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
 9 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
10 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
11 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec noticed that process rank 12 with PID 1024171 on node uc2n517 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1385648] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1385648] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515:1385672:0:1385672] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x1531b0200000)
[uc2n515:1385671:0:1385671] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14b752200000)
[uc2n515:1385670:0:1385670] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14fdb2200000)
[uc2n515:1385673:0:1385673] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14e72e200000)
[uc2n515:1385666:0:1385666] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14dade200000)
[uc2n515:1385668:0:1385668] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14ba1e200000)
[uc2n515:1385669:0:1385669] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14f6b6200000)
[uc2n515:1385667:0:1385667] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x148b8e200000)
[uc2n517:1024528:0:1024528] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x15140e200010)
[uc2n517:1024533:0:1024533] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14c52e340010)
[uc2n517:1024531:0:1024531] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x1538682c0010)
[uc2n517:1024534:0:1024534] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14c7e6380010)
[uc2n517:1024530:0:1024530] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x149e4a280010)
[uc2n517:1024535:0:1024535] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x146a323c0010)
[uc2n517:1024529:0:1024529] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x1478ae240010)
[uc2n517:1024532:0:1024532] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x152646300010)
==== backtrace (tid:1385672) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x000000000001088e MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
11 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
12 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
13 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
14 0x000000000040331f main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1385671) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x000000000001088e MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
11 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
12 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
13 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
14 0x000000000040331f main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1385673) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x000000000001088e MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
11 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
12 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
13 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
14 0x000000000040331f main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1385670) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x000000000001088e MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
11 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
12 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
13 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
14 0x000000000040331f main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1385666) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x000000000001088e MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
11 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
12 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
13 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
14 0x000000000040331f main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1385667) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x000000000001088e MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
11 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
12 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
13 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
14 0x000000000040331f main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1385668) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x000000000001088e MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
11 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
12 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
13 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
14 0x000000000040331f main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1385669) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x000000000001088e MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
11 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
12 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
13 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
14 0x000000000040331f main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1024534) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x00000000001d5e80 cuMemGetAttribute()  ???:0
 3 0x00000000002ffe53 cudbgGetAPI()  ???:0
 4 0x00000000004359a6 cudbgApiInit()  ???:0
 5 0x000000000018d730 ???()  /lib64/libcuda.so.1:0
 6 0x000000000018df04 ???()  /lib64/libcuda.so.1:0
 7 0x00000000001904d9 ???()  /lib64/libcuda.so.1:0
 8 0x00000000001fe77e cuGraphAddMemAllocNode()  ???:0
 9 0x00000000000a2944 mca_common_cuda_cu_memcpy()  common_cuda.c:0
10 0x000000000007c563 opal_cuda_memcpy_sync()  ???:0
11 0x0000000000077119 non_overlap_cuda_copy_content_same_ddt()  opal_datatype_copy.c:0
12 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
13 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
14 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
15 0x000000000009042b PMPI_Alltoallv()  ???:0
16 0x000000000001088e MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
17 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
18 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
19 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
20 0x000000000040331f main()  ???:0
21 0x00000000000236a3 __libc_start_main()  ???:0
22 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1024535) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x00000000001d5e80 cuMemGetAttribute()  ???:0
 3 0x00000000002ffe53 cudbgGetAPI()  ???:0
 4 0x00000000004359a6 cudbgApiInit()  ???:0
 5 0x000000000018d730 ???()  /lib64/libcuda.so.1:0
 6 0x000000000018df04 ???()  /lib64/libcuda.so.1:0
 7 0x00000000001904d9 ???()  /lib64/libcuda.so.1:0
 8 0x00000000001fe77e cuGraphAddMemAllocNode()  ???:0
 9 0x00000000000a2944 mca_common_cuda_cu_memcpy()  common_cuda.c:0
10 0x000000000007c563 opal_cuda_memcpy_sync()  ???:0
11 0x0000000000077119 non_overlap_cuda_copy_content_same_ddt()  opal_datatype_copy.c:0
12 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
13 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
14 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
15 0x000000000009042b PMPI_Alltoallv()  ???:0
16 0x000000000001088e MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
17 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
18 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
19 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
20 0x000000000040331f main()  ???:0
21 0x00000000000236a3 __libc_start_main()  ???:0
22 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1024533) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x00000000001d5e80 cuMemGetAttribute()  ???:0
 3 0x00000000002ffe53 cudbgGetAPI()  ???:0
 4 0x00000000004359a6 cudbgApiInit()  ???:0
 5 0x000000000018d730 ???()  /lib64/libcuda.so.1:0
 6 0x000000000018df04 ???()  /lib64/libcuda.so.1:0
 7 0x00000000001904d9 ???()  /lib64/libcuda.so.1:0
 8 0x00000000001fe77e cuGraphAddMemAllocNode()  ???:0
 9 0x00000000000a2944 mca_common_cuda_cu_memcpy()  common_cuda.c:0
10 0x000000000007c563 opal_cuda_memcpy_sync()  ???:0
11 0x0000000000077119 non_overlap_cuda_copy_content_same_ddt()  opal_datatype_copy.c:0
12 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
13 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
14 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
15 0x000000000009042b PMPI_Alltoallv()  ???:0
16 0x000000000001088e MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
17 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
18 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
19 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
20 0x000000000040331f main()  ???:0
21 0x00000000000236a3 __libc_start_main()  ???:0
22 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1024532) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x00000000001d5e80 cuMemGetAttribute()  ???:0
 3 0x00000000002ffe53 cudbgGetAPI()  ???:0
 4 0x00000000004359a6 cudbgApiInit()  ???:0
 5 0x000000000018d730 ???()  /lib64/libcuda.so.1:0
 6 0x000000000018df04 ???()  /lib64/libcuda.so.1:0
 7 0x00000000001904d9 ???()  /lib64/libcuda.so.1:0
 8 0x00000000001fe77e cuGraphAddMemAllocNode()  ???:0
 9 0x00000000000a2944 mca_common_cuda_cu_memcpy()  common_cuda.c:0
10 0x000000000007c563 opal_cuda_memcpy_sync()  ???:0
11 0x0000000000077119 non_overlap_cuda_copy_content_same_ddt()  opal_datatype_copy.c:0
12 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
13 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
14 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
15 0x000000000009042b PMPI_Alltoallv()  ???:0
16 0x000000000001088e MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
17 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
18 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
19 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
20 0x000000000040331f main()  ???:0
21 0x00000000000236a3 __libc_start_main()  ???:0
22 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1024530) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x00000000001d5e80 cuMemGetAttribute()  ???:0
 3 0x00000000002ffe53 cudbgGetAPI()  ???:0
 4 0x00000000004359a6 cudbgApiInit()  ???:0
 5 0x000000000018d730 ???()  /lib64/libcuda.so.1:0
 6 0x000000000018df04 ???()  /lib64/libcuda.so.1:0
 7 0x00000000001904d9 ???()  /lib64/libcuda.so.1:0
 8 0x00000000001fe77e cuGraphAddMemAllocNode()  ???:0
 9 0x00000000000a2944 mca_common_cuda_cu_memcpy()  common_cuda.c:0
10 0x000000000007c563 opal_cuda_memcpy_sync()  ???:0
11 0x0000000000077119 non_overlap_cuda_copy_content_same_ddt()  opal_datatype_copy.c:0
12 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
13 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
14 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
15 0x000000000009042b PMPI_Alltoallv()  ???:0
16 0x000000000001088e MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
17 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
18 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
19 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
20 0x000000000040331f main()  ???:0
==== backtrace (tid:1024529) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x00000000001d5e80 cuMemGetAttribute()  ???:0
 3 0x00000000002ffe53 cudbgGetAPI()  ???:0
 4 0x00000000004359a6 cudbgApiInit()  ???:0
 5 0x000000000018d730 ???()  /lib64/libcuda.so.1:0
 6 0x000000000018df04 ???()  /lib64/libcuda.so.1:0
 7 0x00000000001904d9 ???()  /lib64/libcuda.so.1:0
 8 0x00000000001fe77e cuGraphAddMemAllocNode()  ???:0
 9 0x00000000000a2944 mca_common_cuda_cu_memcpy()  common_cuda.c:0
10 0x000000000007c563 opal_cuda_memcpy_sync()  ???:0
11 0x0000000000077119 non_overlap_cuda_copy_content_same_ddt()  opal_datatype_copy.c:0
12 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
13 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
14 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
15 0x000000000009042b PMPI_Alltoallv()  ???:0
16 0x000000000001088e MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
17 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
18 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
19 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
20 0x000000000040331f main()  ???:0
21 0x00000000000236a3 __libc_start_main()  ???:0
22 0x00000000004039ee _start()  ???:0
=================================
21 0x00000000000236a3 __libc_start_main()  ???:0
22 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1024531) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x00000000001d5e80 cuMemGetAttribute()  ???:0
 3 0x00000000002ffe53 cudbgGetAPI()  ???:0
 4 0x00000000004359a6 cudbgApiInit()  ???:0
 5 0x000000000018d730 ???()  /lib64/libcuda.so.1:0
 6 0x000000000018df04 ???()  /lib64/libcuda.so.1:0
 7 0x00000000001904d9 ???()  /lib64/libcuda.so.1:0
 8 0x00000000001fe77e cuGraphAddMemAllocNode()  ???:0
 9 0x00000000000a2944 mca_common_cuda_cu_memcpy()  common_cuda.c:0
10 0x000000000007c563 opal_cuda_memcpy_sync()  ???:0
11 0x0000000000077119 non_overlap_cuda_copy_content_same_ddt()  opal_datatype_copy.c:0
12 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
13 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
14 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
15 0x000000000009042b PMPI_Alltoallv()  ???:0
16 0x000000000001088e MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
17 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
18 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
19 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
20 0x000000000040331f main()  ???:0
21 0x00000000000236a3 __libc_start_main()  ???:0
22 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1024528) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x00000000001d5e80 cuMemGetAttribute()  ???:0
 3 0x00000000002ffe53 cudbgGetAPI()  ???:0
 4 0x00000000004359a6 cudbgApiInit()  ???:0
 5 0x000000000018d730 ???()  /lib64/libcuda.so.1:0
 6 0x000000000018df04 ???()  /lib64/libcuda.so.1:0
 7 0x00000000001904d9 ???()  /lib64/libcuda.so.1:0
 8 0x00000000001fe77e cuGraphAddMemAllocNode()  ???:0
 9 0x00000000000a2944 mca_common_cuda_cu_memcpy()  common_cuda.c:0
10 0x000000000007c563 opal_cuda_memcpy_sync()  ???:0
11 0x0000000000077119 non_overlap_cuda_copy_content_same_ddt()  opal_datatype_copy.c:0
12 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
13 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
14 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
15 0x000000000009042b PMPI_Alltoallv()  ???:0
16 0x000000000001088e MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
17 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
18 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
19 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
20 0x000000000040331f main()  ???:0
21 0x00000000000236a3 __libc_start_main()  ???:0
22 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec noticed that process rank 6 with PID 0 on node uc2n515 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1385988] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1385988] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n517:1025234:0:1025234] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x151f22280000)
[uc2n517:1025233:0:1025233] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x150018240000)
[uc2n517:1025235:0:1025235] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x154fb82c0000)
[uc2n517:1025232:0:1025232] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14e1aa200000)
[uc2n517:1025238:0:1025238] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x145cba380000)
[uc2n517:1025239:0:1025239] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x149d263c0000)
[uc2n517:1025237:0:1025237] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14a80e340000)
[uc2n517:1025236:0:1025236] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x151b9e300000)
==== backtrace (tid:1025235) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x000000000008d803 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x0000000000010f81 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
 8 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
 9 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
10 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1025234) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x000000000008d803 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x0000000000010f81 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
 8 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
 9 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
10 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1025232) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x000000000008d803 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x0000000000010f81 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
 8 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
 9 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
10 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1025233) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x000000000008d803 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x0000000000010f81 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
 8 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
 9 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
10 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1025237) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x000000000008d803 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x0000000000010f81 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
 8 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
 9 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
10 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1025239) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x000000000008d803 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x0000000000010f81 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
 8 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
 9 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
10 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1025236) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x000000000008d803 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x0000000000010f81 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
 8 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
 9 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
10 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1025238) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x000000000008d803 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x0000000000010f81 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
 8 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
 9 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
10 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
[uc2n515:1386376:0:1386376] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14cbce200000)
[uc2n515:1386373:0:1386373] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x149540200000)
[uc2n515:1386374:0:1386374] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x152432200000)
[uc2n515:1386375:0:1386375] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x146fba200000)
[uc2n515:1386370:0:1386370] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14cd80200000)
[uc2n515:1386369:0:1386369] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x152ed6200000)
[uc2n515.localdomain:1386356] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1386356] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515:1386371:0:1386371] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x154a66200000)
[uc2n515:1386372:0:1386372] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x1545d6200000)
==== backtrace (tid:1386376) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x0000000000121b20 mca_coll_basic_alltoallw_intra()  ???:0
 8 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 9 0x0000000000010f81 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
10 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
11 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
12 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
13 0x000000000040331f main()  ???:0
14 0x00000000000236a3 __libc_start_main()  ???:0
15 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1386373) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x0000000000121b20 mca_coll_basic_alltoallw_intra()  ???:0
 8 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 9 0x0000000000010f81 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
10 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
11 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
12 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
13 0x000000000040331f main()  ???:0
==== backtrace (tid:1386374) ====
14 0x00000000000236a3 __libc_start_main()  ???:0
15 0x00000000004039ee _start()  ???:0
=================================
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x0000000000121b20 mca_coll_basic_alltoallw_intra()  ???:0
 8 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 9 0x0000000000010f81 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
10 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
11 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
12 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
13 0x000000000040331f main()  ???:0
14 0x00000000000236a3 __libc_start_main()  ???:0
15 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1386375) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x0000000000121b20 mca_coll_basic_alltoallw_intra()  ???:0
 8 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 9 0x0000000000010f81 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
10 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
11 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
12 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
13 0x000000000040331f main()  ???:0
14 0x00000000000236a3 __libc_start_main()  ???:0
==== backtrace (tid:1386369) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x0000000000121b20 mca_coll_basic_alltoallw_intra()  ???:0
 8 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 9 0x0000000000010f81 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
10 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
11 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
12 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
13 0x000000000040331f main()  ???:0
14 0x00000000000236a3 __libc_start_main()  ???:0
15 0x00000000004039ee _start()  ???:0
=================================
15 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1386370) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x0000000000121b20 mca_coll_basic_alltoallw_intra()  ???:0
 8 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 9 0x0000000000010f81 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
10 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
11 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
12 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
13 0x000000000040331f main()  ???:0
14 0x00000000000236a3 __libc_start_main()  ???:0
15 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1386371) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x0000000000121b20 mca_coll_basic_alltoallw_intra()  ???:0
 8 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 9 0x0000000000010f81 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
10 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
11 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
12 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
13 0x000000000040331f main()  ???:0
14 0x00000000000236a3 __libc_start_main()  ???:0
15 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1386372) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x0000000000121b20 mca_coll_basic_alltoallw_intra()  ???:0
 8 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 9 0x0000000000010f81 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
10 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
11 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
12 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
13 0x000000000040331f main()  ???:0
14 0x00000000000236a3 __libc_start_main()  ???:0
15 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec noticed that process rank 15 with PID 1025239 on node uc2n517 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
Starting computation for size 128
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 07:31:53.140577
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 07:32:15.559086
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 07:32:23.715985
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 07:32:31.231222
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 07:32:39.380348
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 07:32:46.837964
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 07:32:55.054287
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 07:33:02.526012
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 07:33:10.787925
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 07:33:18.272384
b''

Starting computation for size [128, 128, 256]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 07:33:26.430879
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 07:33:33.957373
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 07:33:42.098771
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 07:33:49.692568
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 07:33:58.030488
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 07:34:05.533499
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 07:34:13.697531
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 07:34:21.248067
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 07:34:29.455198
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 07:34:37.029093
b''

Starting computation for size [128, 256, 256]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 07:34:45.293979
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 07:34:52.842986
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 07:35:01.071409
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 07:35:08.615075
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 07:35:16.839497
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 07:35:24.482381
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 07:35:32.716449
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 07:35:40.271649
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 07:35:48.440978
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 07:35:56.074573
b''

Starting computation for size 256
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 07:36:04.256889
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 07:36:11.826726
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 07:36:20.034167
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 07:36:27.630141
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 07:36:35.824533
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 07:36:43.581907
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 07:36:51.766832
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 07:36:59.422767
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 07:37:07.696639
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 07:37:15.371766
b''

Starting computation for size [256, 256, 512]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 07:37:23.694244
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 07:37:31.442148
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 07:37:39.800453
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 07:37:47.642859
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 07:37:55.928546
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 07:38:03.775148
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 07:38:12.157359
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 07:38:19.998642
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 07:38:28.309507
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 07:38:36.249687
b''

Starting computation for size [256, 512, 512]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 07:38:44.611882
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 07:38:52.620987
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 07:39:01.221940
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 07:39:09.317375
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 07:39:17.890837
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 07:39:26.201469
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 07:39:34.890537
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 07:39:45.530976
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 07:39:54.029825
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 07:40:02.300132
b''

Starting computation for size 512
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 07:40:10.952762
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 07:40:19.939607
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 07:40:28.839141
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 07:40:37.529155
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 07:40:46.519034
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 07:40:55.773896
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 07:41:04.762169
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 07:41:13.691996
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 07:41:22.568749
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 07:41:31.857841
b''

Starting computation for size [512, 512, 1024]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 07:41:41.032273
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 07:41:51.544613
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 07:42:01.334905
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 07:42:11.600095
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 07:42:21.437358
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 07:42:32.450146
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 07:42:42.437681
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 07:42:52.823816
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 07:43:02.584961
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 07:43:13.705575
b''

Starting computation for size [512, 1024, 1024]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 07:43:23.734959
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 07:43:37.281458
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 07:43:48.465859
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 07:44:01.418534
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 07:44:12.604637
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 07:44:27.214849
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 07:44:38.613905
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 07:44:52.197814
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 07:45:03.517723
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 07:45:18.430879
b''

Starting computation for size 1024
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 07:45:30.207920
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 07:45:50.049857
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 07:46:04.501446
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 07:46:23.570575
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 07:46:37.974316
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 07:46:59.882841
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 07:47:14.997648
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 07:47:34.911721
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 07:47:49.391459
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 07:48:11.358426
b''

Starting computation for size [1024, 1024, 2048]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 07:48:27.071007
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 07:49:00.187846
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 07:49:21.625412
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 07:49:52.354464
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 07:50:13.623214
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 07:50:50.601418
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 07:51:11.948927
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 07:51:44.430200
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 07:52:05.902426
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 07:52:42.121407
b''

Starting computation for size [1024, 2048, 2048]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 07:53:05.200898
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 07:54:04.708120
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 07:54:38.734124
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 07:55:33.301976
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 07:56:07.420494
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 07:57:13.871213
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 07:57:48.648938
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 07:58:45.920294
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 07:59:20.977425
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 08:00:25.156241
b''

Starting computation for size 2048
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 08:01:02.708279
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 08:02:56.874206
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 08:03:55.973204
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 08:05:36.923090
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 08:06:35.888175
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 08:08:38.521946
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 08:09:38.012457
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 08:09:54.087804
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 08:10:06.271063
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 08:12:08.611641
b''

Slab 2D->1D opt1 (inverse)
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1386697] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1386697] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1386969] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1386969] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1387237] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1387237] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1387748] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1387748] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1388281] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1388281] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1388547] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1388547] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1388816] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1388816] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1389078] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1389078] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1389357] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1389357] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1389622] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1389622] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1389890] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1389890] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1390168] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1390168] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1390432] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1390432] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1390947] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1390947] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1391462] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1391462] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1391743] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1391743] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1392011] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1392011] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1392279] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1392279] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1392547] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1392547] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1392826] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1392826] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1393096] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1393096] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1393363] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1393363] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1393641] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1393641] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1394153] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1394153] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1394668] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1394668] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1394932] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1394932] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1395216] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1395216] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1395484] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1395484] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1395748] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1395748] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1396025] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1396025] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1396294] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1396294] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1396557] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1396557] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1396838] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1396838] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1397353] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1397353] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1397868] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1397868] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1398135] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1398135] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1398417] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1398417] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1398684] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1398684] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1398950] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1398950] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1399226] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1399226] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1399499] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1399499] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1399768] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1399768] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1400045] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1400045] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1400560] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1400560] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1401073] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1401073] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1401357] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1401357] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1401628] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1401628] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1401907] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1401907] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1402175] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1402175] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1402441] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1402441] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1402725] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1402725] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1402993] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1402993] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1403260] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1403260] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1403789] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1403789] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1404308] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1404308] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1404575] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1404575] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1404864] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1404864] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1405130] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1405130] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1405407] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1405407] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1405675] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1405675] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1405965] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1405965] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1406229] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1406229] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1406508] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1406508] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1407023] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1407023] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1407540] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1407540] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1407820] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1407820] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1408099] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1408099] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1408378] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1408378] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1408648] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1408648] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1408916] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1408916] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1409200] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1409200] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1409478] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1409478] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1409746] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1409746] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1410263] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1410263] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1410793] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1410793] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1411060] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1411060] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1411359] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1411359] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1411628] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1411628] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1411896] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1411896] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1412184] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1412184] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1412479] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1412479] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1412760] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1412760] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1413029] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1413029] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1413558] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1413558] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1414078] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1414078] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1414357] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1414357] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1414681] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1414681] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1414959] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1414959] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1415228] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1415228] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1415506] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1415506] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1415832] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1415832] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1416114] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1416114] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1416395] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1416395] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1416913] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1416913] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1417445] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1417445] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1417732] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1417732] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1418050] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1418050] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1418333] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1418333] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1418617] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1418617] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1418887] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1418887] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1419228] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1419228] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1419515] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1419515] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1419802] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1419802] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1420339] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1420339] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1420876] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1420876] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1421165] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1421165] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1421552] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1421552] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1421837] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1421837] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1422131] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1422131] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1422430] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1422430] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1422825] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1422825] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1423139] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1423139] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1423430] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1423430] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1423993] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1423993] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1424534] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1424534] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1424866] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1424866] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1425358] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1425358] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1425675] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1425675] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1425963] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1425963] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1426295] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1426295] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1426853] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1426853] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1427214] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1427214] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1427532] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1427532] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1428138] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1428138] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1428702] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1428702] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1429073] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1429073] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1429587] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1429587] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n517:1069125:0:1069125] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x151dec3c0000)
[uc2n517:1069120:0:1069120] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x1458a8280000)
[uc2n517:1069121:0:1069121] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14d6ec2c0000)
[uc2n517:1069118:0:1069118] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14e5e4200000)
[uc2n517:1069119:0:1069119] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x145b20240000)
[uc2n517:1069122:0:1069122] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14bb5e300000)
[uc2n517:1069123:0:1069123] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x154626340000)
[uc2n517:1069124:0:1069124] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14ee86380000)
==== backtrace (tid:1069125) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001ee17 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
 9 0x000000000001e765 MPIcuFFT_Slab_Opt1<double>::execC2R()  ???:0
10 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
11 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1069120) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001ee17 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
 9 0x000000000001e765 MPIcuFFT_Slab_Opt1<double>::execC2R()  ???:0
10 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
11 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1069118) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001ee17 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
 9 0x000000000001e765 MPIcuFFT_Slab_Opt1<double>::execC2R()  ???:0
10 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
11 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1069121) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001ee17 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
 9 0x000000000001e765 MPIcuFFT_Slab_Opt1<double>::execC2R()  ???:0
10 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
11 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1069119) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001ee17 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
 9 0x000000000001e765 MPIcuFFT_Slab_Opt1<double>::execC2R()  ???:0
10 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
11 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1069123) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001ee17 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
 9 0x000000000001e765 MPIcuFFT_Slab_Opt1<double>::execC2R()  ???:0
10 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
11 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1069124) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001ee17 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
 9 0x000000000001e765 MPIcuFFT_Slab_Opt1<double>::execC2R()  ???:0
10 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
11 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1069122) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001ee17 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
 9 0x000000000001e765 MPIcuFFT_Slab_Opt1<double>::execC2R()  ???:0
10 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
11 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
[uc2n515:1429607:0:1429607] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14af5c200000)
[uc2n515:1429604:0:1429604] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14a10c200000)
[uc2n515:1429601:0:1429601] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14d1cc200000)
[uc2n515:1429602:0:1429602] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x149566200000)
[uc2n515:1429603:0:1429603] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x145a40200000)
[uc2n515:1429605:0:1429605] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x146478200000)
[uc2n515:1429600:0:1429600] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14f95c200000)
[uc2n515:1429606:0:1429606] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x150e84200000)
==== backtrace (tid:1429607) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x000000000001ee17 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
11 0x000000000001e765 MPIcuFFT_Slab_Opt1<double>::execC2R()  ???:0
12 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
13 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
14 0x000000000040331f main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1429604) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x000000000001ee17 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
11 0x000000000001e765 MPIcuFFT_Slab_Opt1<double>::execC2R()  ???:0
12 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
13 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
14 0x000000000040331f main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1429606) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x000000000001ee17 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
11 0x000000000001e765 MPIcuFFT_Slab_Opt1<double>::execC2R()  ???:0
12 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
13 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
14 0x000000000040331f main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1429605) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x000000000001ee17 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
11 0x000000000001e765 MPIcuFFT_Slab_Opt1<double>::execC2R()  ???:0
12 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
13 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
14 0x000000000040331f main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1429600) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x000000000001ee17 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
11 0x000000000001e765 MPIcuFFT_Slab_Opt1<double>::execC2R()  ???:0
12 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
13 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
14 0x000000000040331f main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1429601) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x000000000001ee17 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
11 0x000000000001e765 MPIcuFFT_Slab_Opt1<double>::execC2R()  ???:0
12 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
13 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
14 0x000000000040331f main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1429602) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x000000000001ee17 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
11 0x000000000001e765 MPIcuFFT_Slab_Opt1<double>::execC2R()  ???:0
12 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
13 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
14 0x000000000040331f main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1429603) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x000000000001ee17 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
11 0x000000000001e765 MPIcuFFT_Slab_Opt1<double>::execC2R()  ???:0
12 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
13 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
14 0x000000000040331f main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec noticed that process rank 11 with PID 1069121 on node uc2n517 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1429938] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1429938] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
The call to cuMemcpyAsync failed. This is a unrecoverable error and will
cause the program to abort.
  cuMemcpyAsync(0x150d8ec00000, 0x150b87000000, 131072) returned value 1
Check the cuda.h file for what the return value means.
--------------------------------------------------------------------------
[uc2n517.localdomain:1069503] CUDA: Error in cuMemcpy: res=-1, dest=0x150d8ec00000, src=0x150b87000000, size=131072
[uc2n517:1069503] *** Process received signal ***
[uc2n517:1069503] Signal: Aborted (6)
[uc2n517:1069503] Signal code:  (-6)
[uc2n517:1069503] [ 0] /usr/lib64/libpthread.so.0(+0x12dd0)[0x150ffe92edd0]
[uc2n517:1069503] [ 1] /usr/lib64/libc.so.6(gsignal+0x10f)[0x150ffe59170f]
[uc2n517:1069503] [ 2] /usr/lib64/libc.so.6(abort+0x127)[0x150ffe57bb25]
[uc2n517:1069503] [ 3] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libopen-pal.so.40(+0x7c58f)[0x150ffd4fb58f]
[uc2n517:1069503] [ 4] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libopen-pal.so.40(+0x77119)[0x150ffd4f6119]
[uc2n517:1069503] [ 5] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libmpi.so.40(ompi_datatype_sndrcv+0x50a)[0x15100a334b2a]
[uc2n517:1069503] [ 6] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libmpi.so.40(ompi_coll_base_alltoallv_intra_basic_linear+0x26e)[0x15100a38980e]
[uc2n517:1069503] [ 7] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libmpi.so.40(ompi_coll_tuned_alltoallv_intra_dec_fixed+0x42)[0x15100a395702]
[uc2n517:1069503] [ 8] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libmpi.so.40(MPI_Alltoallv+0x1ab)[0x15100a33742b]
[uc2n517:1069503] [ 9] /home/st/st_us-051200/st_st160727/DistributedFFT/build/libslab_decomp.so(_ZN18MPIcuFFT_Slab_Opt1IdE12All2All_SyncEPvb+0x3e7)[0x151014a84e17]
[uc2n517:1069503] [10] /home/st/st_us-051200/st_st160727/DistributedFFT/build/libslab_decomp.so(_ZN18MPIcuFFT_Slab_Opt1IdE7execC2REPvPKv+0x115)[0x151014a84765]
[uc2n517:1069503] [11] /home/st/st_us-051200/st_st160727/DistributedFFT/build/libslab_tests.so(_ZN25Tests_Slab_Random_DefaultIdE9testcase2Eii+0x51f)[0x151014cd18d9]
[uc2n517:1069503] [12] /home/st/st_us-051200/st_st160727/DistributedFFT/build/libslab_tests.so(_ZN25Tests_Slab_Random_DefaultIdE3runEiii+0x67)[0x151014cd0c55]
[uc2n517:1069503] [13] slab[0x40331f]
[uc2n517:1069503] [14] /usr/lib64/libc.so.6(__libc_start_main+0xf3)[0x150ffe57d6a3]
[uc2n517:1069503] [15] slab[0x4039ee]
[uc2n517:1069503] *** End of error message ***
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec noticed that process rank 15 with PID 1069503 on node uc2n517 exited on signal 6 (Aborted).
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1430206] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1430206] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n517:1069785:0:1069785] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x150be0300000)
[uc2n517:1069783:0:1069783] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x1477a6280000)
[uc2n517:1069781:0:1069781] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x148c94200000)
[uc2n517:1069786:0:1069786] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x152884340000)
[uc2n517:1069782:0:1069782] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14db10240000)
[uc2n517:1069784:0:1069784] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14b6dc2c0000)
[uc2n517:1069788:0:1069788] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x147cb83c0000)
[uc2n517:1069787:0:1069787] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14f3ec380000)
==== backtrace (tid:1069785) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000001f4df MPIcuFFT_Slab_Opt1<double>::All2All_MPIType()  ???:0
 8 0x000000000001e765 MPIcuFFT_Slab_Opt1<double>::execC2R()  ???:0
 9 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
10 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1069783) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000001f4df MPIcuFFT_Slab_Opt1<double>::All2All_MPIType()  ???:0
 8 0x000000000001e765 MPIcuFFT_Slab_Opt1<double>::execC2R()  ???:0
 9 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
10 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1069782) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000001f4df MPIcuFFT_Slab_Opt1<double>::All2All_MPIType()  ???:0
 8 0x000000000001e765 MPIcuFFT_Slab_Opt1<double>::execC2R()  ???:0
 9 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
10 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1069781) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000001f4df MPIcuFFT_Slab_Opt1<double>::All2All_MPIType()  ???:0
 8 0x000000000001e765 MPIcuFFT_Slab_Opt1<double>::execC2R()  ???:0
 9 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
10 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1069784) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000001f4df MPIcuFFT_Slab_Opt1<double>::All2All_MPIType()  ???:0
 8 0x000000000001e765 MPIcuFFT_Slab_Opt1<double>::execC2R()  ???:0
 9 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
10 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1069786) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000001f4df MPIcuFFT_Slab_Opt1<double>::All2All_MPIType()  ???:0
 8 0x000000000001e765 MPIcuFFT_Slab_Opt1<double>::execC2R()  ???:0
 9 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
10 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1069787) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000001f4df MPIcuFFT_Slab_Opt1<double>::All2All_MPIType()  ???:0
 8 0x000000000001e765 MPIcuFFT_Slab_Opt1<double>::execC2R()  ???:0
 9 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
10 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1069788) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000001f4df MPIcuFFT_Slab_Opt1<double>::All2All_MPIType()  ???:0
 8 0x000000000001e765 MPIcuFFT_Slab_Opt1<double>::execC2R()  ???:0
 9 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
10 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec noticed that process rank 10 with PID 1069783 on node uc2n517 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1430488] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1430488] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
The call to cuMemcpyAsync failed. This is a unrecoverable error and will
cause the program to abort.
  cuMemcpyAsync(0x14bede800000, 0x944c980, 65536) returned value 1
Check the cuda.h file for what the return value means.
--------------------------------------------------------------------------
[uc2n517.localdomain:1070142] CUDA: Error in cuMemcpy: res=-1, dest=0x14bede800000, src=0x944c980, size=65536
[uc2n517:1070142] *** Process received signal ***
[uc2n517:1070142] Signal: Aborted (6)
[uc2n517:1070142] Signal code:  (-6)
[uc2n517:1070142] [ 0] /usr/lib64/libpthread.so.0(+0x12dd0)[0x14c1518c4dd0]
[uc2n517:1070142] [ 1] /usr/lib64/libc.so.6(gsignal+0x10f)[0x14c15152770f]
[uc2n517:1070142] [ 2] /usr/lib64/libc.so.6(abort+0x127)[0x14c151511b25]
[uc2n517:1070142] [ 3] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libopen-pal.so.40(+0x7c375)[0x14c150491375]
[uc2n517:1070142] [ 4] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libopen-pal.so.40(opal_convertor_unpack+0xb9)[0x14c150488399]
[uc2n517:1070142] [ 5] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libmpi.so.40(ompi_datatype_sndrcv+0x1f7)[0x14c15d2ca817]
[uc2n517:1070142] [ 6] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libmpi.so.40(mca_coll_basic_alltoallw_intra+0xa4)[0x14c15d35e984]
[uc2n517:1070142] [ 7] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libmpi.so.40(MPI_Alltoallw+0x1eb)[0x14c15d2cdccb]
[uc2n517:1070142] [ 8] /home/st/st_us-051200/st_st160727/DistributedFFT/build/libslab_decomp.so(_ZN18MPIcuFFT_Slab_Opt1IdE15All2All_MPITypeEPvb+0x35f)[0x14c167a1b4df]
[uc2n517:1070142] [ 9] /home/st/st_us-051200/st_st160727/DistributedFFT/build/libslab_decomp.so(_ZN18MPIcuFFT_Slab_Opt1IdE7execC2REPvPKv+0x115)[0x14c167a1a765]
[uc2n517:1070142] [10] /home/st/st_us-051200/st_st160727/DistributedFFT/build/libslab_tests.so(_ZN25Tests_Slab_Random_DefaultIdE9testcase2Eii+0x51f)[0x14c167c678d9]
[uc2n517:1070142] [11] /home/st/st_us-051200/st_st160727/DistributedFFT/build/libslab_tests.so(_ZN25Tests_Slab_Random_DefaultIdE3runEiii+0x67)[0x14c167c66c55]
[uc2n517:1070142] [12] slab[0x40331f]
[uc2n517:1070142] [13] /usr/lib64/libc.so.6(__libc_start_main+0xf3)[0x14c1515136a3]
[uc2n517:1070142] [14] slab[0x4039ee]
[uc2n517:1070142] *** End of error message ***
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec noticed that process rank 15 with PID 1070142 on node uc2n517 exited on signal 6 (Aborted).
--------------------------------------------------------------------------
Starting computation for size 128
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 08:12:15.938884
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 08:12:25.277288
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 08:12:33.454576
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 08:12:40.915885
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 08:12:48.948785
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 08:12:56.378517
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 08:13:05.584712
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 08:13:13.100561
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 08:13:21.244719
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 08:13:28.745281
b''

Starting computation for size [128, 128, 256]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 08:13:38.097907
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 08:13:45.634137
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 08:13:53.827885
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 08:14:01.360104
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 08:14:09.659909
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 08:14:17.203873
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 08:14:27.324177
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 08:14:35.152854
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 08:14:43.551618
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 08:14:51.384514
b''

Starting computation for size [128, 256, 256]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 08:15:02.136288
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 08:15:10.086923
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 08:15:18.780711
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 08:15:26.905639
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 08:15:35.614342
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 08:15:43.520906
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 08:15:55.956954
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 08:16:04.012515
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 08:16:12.493170
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 08:16:20.508214
b''

Starting computation for size 256
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 08:16:33.663332
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 08:16:41.787372
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 08:16:50.482158
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 08:16:58.572894
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 08:17:07.267639
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 08:17:15.308449
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 08:17:27.808238
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 08:17:35.886188
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 08:17:44.666646
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 08:17:52.644031
b''

Starting computation for size [256, 256, 512]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 08:18:05.667449
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 08:18:14.601514
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 08:18:24.020988
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 08:18:32.611333
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 08:18:41.960433
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 08:18:50.780300
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 08:19:07.695778
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 08:19:16.527833
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 08:19:25.840259
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 08:19:34.885875
b''

Starting computation for size [256, 512, 512]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 08:19:54.241933
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 08:20:03.364195
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 08:20:12.767132
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 08:20:21.799157
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 08:20:31.343708
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 08:20:40.439365
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 08:21:05.369278
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 08:21:14.550862
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 08:21:24.137908
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 08:21:33.505987
b''

Starting computation for size 512
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 08:22:00.083828
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 08:22:09.938042
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 08:22:19.754151
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 08:22:29.554932
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 08:22:39.450737
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 08:22:49.613927
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 08:23:14.984366
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 08:23:24.869982
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 08:23:34.701544
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 08:23:45.093916
b''

Starting computation for size [512, 512, 1024]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 08:24:12.462141
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 08:24:23.828123
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 08:24:34.430770
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 08:24:45.832400
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 08:24:56.692520
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 08:25:08.710570
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 08:25:49.777912
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 08:26:01.433248
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 08:26:12.309126
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 08:26:25.049903
b''

Starting computation for size [512, 1024, 1024]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 08:27:10.189491
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 08:27:25.113562
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 08:27:37.651762
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 08:27:51.928728
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 08:28:04.690349
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 08:28:20.711489
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 08:29:33.742378
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 08:29:52.860104
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 08:30:05.619541
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 08:30:22.660666
b''

Starting computation for size 1024
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 08:31:44.500753
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 08:32:06.085290
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 08:32:22.272232
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 08:32:42.981844
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 08:32:59.814353
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 08:33:23.621963
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 08:34:39.767249
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 08:35:01.372270
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 08:35:17.738897
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 08:35:43.289210
b''

Starting computation for size [1024, 1024, 2048]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 08:37:07.634257
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 08:37:41.225906
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 08:38:05.539501
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 08:38:38.737901
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 08:39:03.650182
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 08:39:42.816823
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 08:42:05.872763
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 08:42:40.935445
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 08:43:04.911094
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 08:43:47.861902
b''

Starting computation for size [1024, 2048, 2048]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 08:46:27.339926
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 08:47:26.929681
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 08:48:05.035728
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 08:49:03.218688
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 08:49:44.374247
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 08:50:55.119519
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 08:55:31.509618
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 08:56:33.675024
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 08:57:11.684152
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 08:58:29.615312
b''

Starting computation for size 2048
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 09:03:40.344431
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 09:05:31.296548
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 09:06:37.587380
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 09:08:25.105866
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 09:09:38.653192
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 09:11:48.212399
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 09:16:45.043418
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 09:17:01.621309
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 09:17:18.416061
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 09:17:36.527954
b''

Slab 1D->2D default (inverse)
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1430773] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1430773] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1431038] [[32339,0],0] ORTE_ERROR_LOG: Data unpack would read past end of buffer in file util/show_help.c at line 513
[uc2n515.localdomain:1431038] 14 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1431038] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1431305] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1431305] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1431830] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1431830] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1432094] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1432094] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1432361] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1432361] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1432638] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1432638] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1432912] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1432912] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1433179] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1433179] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1433447] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1433447] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1433724] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1433724] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1433991] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1433991] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1434256] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1434256] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1434784] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1434784] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1435053] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1435053] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1435316] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1435316] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1435599] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1435599] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1435864] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1435864] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1436129] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1436129] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1436408] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1436408] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1436679] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1436679] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1436941] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1436941] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1437220] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1437220] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1437733] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1437733] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1437999] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1437999] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1438264] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1438264] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1438549] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1438549] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1438817] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1438817] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1439094] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1439094] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1439361] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1439361] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1439632] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1439632] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1439913] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1439913] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1440182] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1440182] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1440695] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1440695] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1440978] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1440978] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1441246] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1441246] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1441527] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1441527] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1441792] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1441792] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1442059] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1442059] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1442323] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1442323] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1442606] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1442606] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1442892] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1442892] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1443158] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1443158] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1443675] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1443675] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1443943] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1443943] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1444220] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1444220] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1444492] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1444492] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1444770] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1444770] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1445037] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1445037] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1445304] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1445304] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1445592] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1445592] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1445857] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1445857] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1446136] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1446136] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1446651] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1446651] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1446920] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1446920] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1447200] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1447200] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1447491] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1447491] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1447762] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1447762] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1448039] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1448039] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1448304] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1448304] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1448597] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1448597] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1448864] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1448864] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1449145] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1449145] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1449660] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1449660] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1449929] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1449929] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1450206] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1450206] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1450521] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1450521] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1450788] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1450788] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1451055] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1451055] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1451335] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1451335] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1451647] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1451647] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1451914] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1451914] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1452193] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1452193] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1452709] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1452709] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1452979] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1452979] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1453258] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1453258] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1453571] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1453571] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1453838] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1453838] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1454103] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1454103] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1454383] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1454383] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1454698] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1454698] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1454968] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1454968] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1455243] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1455243] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1455760] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1455760] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1456043] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1456043] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1456314] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1456314] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1456673] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1456673] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1456954] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1456954] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1457221] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1457221] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1457511] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1457511] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1457871] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1457871] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1458142] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1458142] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1458420] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1458420] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1458943] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1458943] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1459232] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1459232] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1459513] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1459513] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1459954] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1459954] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1460240] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1460240] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1460509] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1460509] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1460794] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1460794] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1461253] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1461253] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1461546] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1461546] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1461833] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1461833] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1462370] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1462370] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1462655] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1462655] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1462945] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1462945] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1463403] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1463403] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1463695] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1463695] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1463967] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1463967] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1464312] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1464312] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1464820] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1464820] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1465134] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1465134] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1465427] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1465427] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1465983] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1465983] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1466290] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1466290] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1466614] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1466614] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1467245] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1467245] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1467561] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1467561] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1467854] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1467854] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1468188] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1468188] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1468831] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1468831] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1469205] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1469205] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1469521] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1469521] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1470125] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1470125] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1470447] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1470447] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1470837] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1470837] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1471832] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1471832] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n517:1112173:0:1112173] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x1532a0400000)
[uc2n517:1112171:0:1112171] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x152516400000)
[uc2n517:1112175:0:1112175] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14e9b8400000)
[uc2n517:1112170:0:1112170] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14c4d4400000)
[uc2n517:1112174:0:1112174] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14c31c400000)
[uc2n517:1112172:0:1112172] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14d9cc400000)
[uc2n517:1112176:0:1112176] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x15377c400000)
[uc2n517:1112177:0:1112177] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14d614400000)
==== backtrace (tid:1112173) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000002cf0e MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
 9 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
10 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
11 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
12 0x0000000000403591 main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1112172) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000002cf0e MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
 9 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
10 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
11 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
12 0x0000000000403591 main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1112171) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000002cf0e MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
==== backtrace (tid:1112170) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000002cf0e MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
 9 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
10 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
11 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
12 0x0000000000403591 main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
 9 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
10 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
11 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
12 0x0000000000403591 main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1112175) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
==== backtrace (tid:1112177) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
==== backtrace (tid:1112174) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000002cf0e MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
 9 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
10 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
11 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
12 0x0000000000403591 main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000002cf0e MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
 9 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
10 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
11 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
12 0x0000000000403591 main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
 8 0x000000000002cf0e MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
 9 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
10 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
11 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
12 0x0000000000403591 main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1112176) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000002cf0e MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
 9 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
10 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
11 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
12 0x0000000000403591 main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec noticed that process rank 8 with PID 1112170 on node uc2n517 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1472111] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1472111] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515:1472131:0:1472131] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14b296000000)
==== backtrace (tid:1472131) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x000000000002cf0e MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
11 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
12 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
13 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
14 0x0000000000403591 main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
[uc2n515:1472136:0:1472136] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x151232000000)
==== backtrace (tid:1472136) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x000000000002cf0e MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
11 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
12 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
13 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
14 0x0000000000403591 main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
[uc2n515:1472135:0:1472135] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14571e000000)
[uc2n515:1472137:0:1472137] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x1478ea000000)
[uc2n515:1472134:0:1472134] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14f0d6000000)
[uc2n515:1472133:0:1472133] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x154446000000)
[uc2n515:1472138:0:1472138] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14927a000000)
[uc2n515:1472132:0:1472132] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14afea000000)
==== backtrace (tid:1472135) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x000000000002cf0e MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
11 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
12 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
13 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
14 0x0000000000403591 main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1472137) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x000000000002cf0e MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
11 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
12 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
13 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
14 0x0000000000403591 main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1472138) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x000000000002cf0e MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
11 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
12 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
13 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
14 0x0000000000403591 main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1472134) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x000000000002cf0e MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
11 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
12 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
13 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
14 0x0000000000403591 main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1472133) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x000000000002cf0e MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
11 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
12 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
13 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
14 0x0000000000403591 main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1472132) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x000000000002cf0e MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
11 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
12 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
13 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
14 0x0000000000403591 main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
[uc2n517:1112532:0:1112532] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14d4de000010)
[uc2n517:1112530:0:1112530] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x154bb0000010)
[uc2n517:1112529:0:1112529] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x154526000010)
[uc2n517:1112531:0:1112531] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14f090000010)
[uc2n517:1112533:0:1112533] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x15463a000010)
[uc2n517:1112534:0:1112534] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x1532b0000010)
[uc2n517:1112535:0:1112535] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x147ef6000010)
[uc2n517:1112536:0:1112536] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14676e000010)
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
==== backtrace (tid:1112532) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x00000000001d5e80 cuMemGetAttribute()  ???:0
 3 0x00000000002ffe53 cudbgGetAPI()  ???:0
 4 0x00000000004359a6 cudbgApiInit()  ???:0
 5 0x000000000018d730 ???()  /lib64/libcuda.so.1:0
 6 0x000000000018df04 ???()  /lib64/libcuda.so.1:0
 7 0x00000000001904d9 ???()  /lib64/libcuda.so.1:0
 8 0x00000000001fe77e cuGraphAddMemAllocNode()  ???:0
 9 0x00000000000a2944 mca_common_cuda_cu_memcpy()  common_cuda.c:0
10 0x000000000007c563 opal_cuda_memcpy_sync()  ???:0
11 0x0000000000077119 non_overlap_cuda_copy_content_same_ddt()  opal_datatype_copy.c:0
12 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
13 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
14 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
15 0x000000000009042b PMPI_Alltoallv()  ???:0
16 0x000000000002cf0e MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
17 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
18 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
19 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
20 0x0000000000403591 main()  ???:0
21 0x00000000000236a3 __libc_start_main()  ???:0
22 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1112530) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x00000000001d5e80 cuMemGetAttribute()  ???:0
 3 0x00000000002ffe53 cudbgGetAPI()  ???:0
 4 0x00000000004359a6 cudbgApiInit()  ???:0
 5 0x000000000018d730 ???()  /lib64/libcuda.so.1:0
 6 0x000000000018df04 ???()  /lib64/libcuda.so.1:0
 7 0x00000000001904d9 ???()  /lib64/libcuda.so.1:0
 8 0x00000000001fe77e cuGraphAddMemAllocNode()  ???:0
 9 0x00000000000a2944 mca_common_cuda_cu_memcpy()  common_cuda.c:0
10 0x000000000007c563 opal_cuda_memcpy_sync()  ???:0
11 0x0000000000077119 non_overlap_cuda_copy_content_same_ddt()  opal_datatype_copy.c:0
12 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
13 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
14 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
15 0x000000000009042b PMPI_Alltoallv()  ???:0
16 0x000000000002cf0e MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
17 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
18 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
19 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
20 0x0000000000403591 main()  ???:0
21 0x00000000000236a3 __libc_start_main()  ???:0
22 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1112529) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x00000000001d5e80 cuMemGetAttribute()  ???:0
 3 0x00000000002ffe53 cudbgGetAPI()  ???:0
 4 0x00000000004359a6 cudbgApiInit()  ???:0
 5 0x000000000018d730 ???()  /lib64/libcuda.so.1:0
 6 0x000000000018df04 ???()  /lib64/libcuda.so.1:0
 7 0x00000000001904d9 ???()  /lib64/libcuda.so.1:0
 8 0x00000000001fe77e cuGraphAddMemAllocNode()  ???:0
 9 0x00000000000a2944 mca_common_cuda_cu_memcpy()  common_cuda.c:0
10 0x000000000007c563 opal_cuda_memcpy_sync()  ???:0
11 0x0000000000077119 non_overlap_cuda_copy_content_same_ddt()  opal_datatype_copy.c:0
12 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
13 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
14 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
15 0x000000000009042b PMPI_Alltoallv()  ???:0
16 0x000000000002cf0e MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
17 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
18 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
19 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
20 0x0000000000403591 main()  ???:0
21 0x00000000000236a3 __libc_start_main()  ???:0
22 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1112531) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x00000000001d5e80 cuMemGetAttribute()  ???:0
 3 0x00000000002ffe53 cudbgGetAPI()  ???:0
 4 0x00000000004359a6 cudbgApiInit()  ???:0
 5 0x000000000018d730 ???()  /lib64/libcuda.so.1:0
 6 0x000000000018df04 ???()  /lib64/libcuda.so.1:0
 7 0x00000000001904d9 ???()  /lib64/libcuda.so.1:0
 8 0x00000000001fe77e cuGraphAddMemAllocNode()  ???:0
 9 0x00000000000a2944 mca_common_cuda_cu_memcpy()  common_cuda.c:0
10 0x000000000007c563 opal_cuda_memcpy_sync()  ???:0
11 0x0000000000077119 non_overlap_cuda_copy_content_same_ddt()  opal_datatype_copy.c:0
12 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
13 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
14 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
15 0x000000000009042b PMPI_Alltoallv()  ???:0
16 0x000000000002cf0e MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
17 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
18 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
19 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
20 0x0000000000403591 main()  ???:0
21 0x00000000000236a3 __libc_start_main()  ???:0
22 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1112536) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x00000000001d5e80 cuMemGetAttribute()  ???:0
 3 0x00000000002ffe53 cudbgGetAPI()  ???:0
 4 0x00000000004359a6 cudbgApiInit()  ???:0
 5 0x000000000018d730 ???()  /lib64/libcuda.so.1:0
 6 0x000000000018df04 ???()  /lib64/libcuda.so.1:0
 7 0x00000000001904d9 ???()  /lib64/libcuda.so.1:0
 8 0x00000000001fe77e cuGraphAddMemAllocNode()  ???:0
 9 0x00000000000a2944 mca_common_cuda_cu_memcpy()  common_cuda.c:0
10 0x000000000007c563 opal_cuda_memcpy_sync()  ???:0
11 0x0000000000077119 non_overlap_cuda_copy_content_same_ddt()  opal_datatype_copy.c:0
12 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
13 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
14 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
15 0x000000000009042b PMPI_Alltoallv()  ???:0
16 0x000000000002cf0e MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
17 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
18 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
19 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
20 0x0000000000403591 main()  ???:0
21 0x00000000000236a3 __libc_start_main()  ???:0
22 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1112534) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x00000000001d5e80 cuMemGetAttribute()  ???:0
 3 0x00000000002ffe53 cudbgGetAPI()  ???:0
 4 0x00000000004359a6 cudbgApiInit()  ???:0
 5 0x000000000018d730 ???()  /lib64/libcuda.so.1:0
 6 0x000000000018df04 ???()  /lib64/libcuda.so.1:0
 7 0x00000000001904d9 ???()  /lib64/libcuda.so.1:0
 8 0x00000000001fe77e cuGraphAddMemAllocNode()  ???:0
 9 0x00000000000a2944 mca_common_cuda_cu_memcpy()  common_cuda.c:0
10 0x000000000007c563 opal_cuda_memcpy_sync()  ???:0
11 0x0000000000077119 non_overlap_cuda_copy_content_same_ddt()  opal_datatype_copy.c:0
12 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
13 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
14 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
15 0x000000000009042b PMPI_Alltoallv()  ???:0
16 0x000000000002cf0e MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
17 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
18 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
19 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
20 0x0000000000403591 main()  ???:0
21 0x00000000000236a3 __libc_start_main()  ???:0
22 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1112535) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x00000000001d5e80 cuMemGetAttribute()  ???:0
 3 0x00000000002ffe53 cudbgGetAPI()  ???:0
 4 0x00000000004359a6 cudbgApiInit()  ???:0
 5 0x000000000018d730 ???()  /lib64/libcuda.so.1:0
 6 0x000000000018df04 ???()  /lib64/libcuda.so.1:0
 7 0x00000000001904d9 ???()  /lib64/libcuda.so.1:0
 8 0x00000000001fe77e cuGraphAddMemAllocNode()  ???:0
 9 0x00000000000a2944 mca_common_cuda_cu_memcpy()  common_cuda.c:0
10 0x000000000007c563 opal_cuda_memcpy_sync()  ???:0
11 0x0000000000077119 non_overlap_cuda_copy_content_same_ddt()  opal_datatype_copy.c:0
12 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
13 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
14 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
15 0x000000000009042b PMPI_Alltoallv()  ???:0
16 0x000000000002cf0e MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
17 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
18 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
19 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
20 0x0000000000403591 main()  ???:0
21 0x00000000000236a3 __libc_start_main()  ???:0
22 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1112533) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x00000000001d5e80 cuMemGetAttribute()  ???:0
 3 0x00000000002ffe53 cudbgGetAPI()  ???:0
 4 0x00000000004359a6 cudbgApiInit()  ???:0
 5 0x000000000018d730 ???()  /lib64/libcuda.so.1:0
 6 0x000000000018df04 ???()  /lib64/libcuda.so.1:0
 7 0x00000000001904d9 ???()  /lib64/libcuda.so.1:0
 8 0x00000000001fe77e cuGraphAddMemAllocNode()  ???:0
 9 0x00000000000a2944 mca_common_cuda_cu_memcpy()  common_cuda.c:0
10 0x000000000007c563 opal_cuda_memcpy_sync()  ???:0
11 0x0000000000077119 non_overlap_cuda_copy_content_same_ddt()  opal_datatype_copy.c:0
12 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
13 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
14 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
15 0x000000000009042b PMPI_Alltoallv()  ???:0
16 0x000000000002cf0e MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
17 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
18 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
19 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
20 0x0000000000403591 main()  ???:0
21 0x00000000000236a3 __libc_start_main()  ???:0
22 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
mpiexec noticed that process rank 0 with PID 0 on node uc2n515 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1472450] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1472450] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1472840] 15 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1472840] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n517:1113257:0:1113257] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14b2e0000000)
[uc2n517:1113258:0:1113258] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x146128000000)
[uc2n517:1113260:0:1113260] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x148350000000)
[uc2n517:1113259:0:1113259] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14af16000000)
[uc2n517:1113254:0:1113254] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x147cb0000000)
[uc2n517:1113255:0:1113255] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x148b62000000)
[uc2n517:1113256:0:1113256] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x153430000000)
[uc2n517:1113253:0:1113253] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x1501d2000000)
==== backtrace (tid:1113257) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x000000000008d803 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000002d5ef MPIcuFFT_Slab_Z_Then_YX<double>::All2All_MPIType()  ???:0
 8 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
 9 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
10 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1113258) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x000000000008d803 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000002d5ef MPIcuFFT_Slab_Z_Then_YX<double>::All2All_MPIType()  ???:0
 8 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
 9 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
10 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1113259) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x000000000008d803 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000002d5ef MPIcuFFT_Slab_Z_Then_YX<double>::All2All_MPIType()  ???:0
 8 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
 9 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
10 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1113260) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x000000000008d803 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000002d5ef MPIcuFFT_Slab_Z_Then_YX<double>::All2All_MPIType()  ???:0
 8 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
 9 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
10 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1113254) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x000000000008d803 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000002d5ef MPIcuFFT_Slab_Z_Then_YX<double>::All2All_MPIType()  ???:0
 8 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
 9 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
10 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1113255) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x000000000008d803 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000002d5ef MPIcuFFT_Slab_Z_Then_YX<double>::All2All_MPIType()  ???:0
 8 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
 9 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
10 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1113256) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x000000000008d803 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000002d5ef MPIcuFFT_Slab_Z_Then_YX<double>::All2All_MPIType()  ???:0
 8 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
 9 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
10 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1113253) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x000000000008d803 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000002d5ef MPIcuFFT_Slab_Z_Then_YX<double>::All2All_MPIType()  ???:0
 8 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
 9 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
10 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
[uc2n515:1472853:0:1472853] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x153276000000)
==== backtrace (tid:1472853) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x0000000000121b20 mca_coll_basic_alltoallw_intra()  ???:0
 8 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 9 0x000000000002d5ef MPIcuFFT_Slab_Z_Then_YX<double>::All2All_MPIType()  ???:0
10 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
11 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
12 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
13 0x0000000000403591 main()  ???:0
14 0x00000000000236a3 __libc_start_main()  ???:0
15 0x00000000004039ee _start()  ???:0
=================================
[uc2n515:1472854:0:1472854] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14fc66000000)
==== backtrace (tid:1472854) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x0000000000121b20 mca_coll_basic_alltoallw_intra()  ???:0
 8 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 9 0x000000000002d5ef MPIcuFFT_Slab_Z_Then_YX<double>::All2All_MPIType()  ???:0
10 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
11 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
12 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
13 0x0000000000403591 main()  ???:0
14 0x00000000000236a3 __libc_start_main()  ???:0
15 0x00000000004039ee _start()  ???:0
=================================
[uc2n515:1472855:0:1472855] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x1481ee000000)
==== backtrace (tid:1472855) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x0000000000121b20 mca_coll_basic_alltoallw_intra()  ???:0
 8 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 9 0x000000000002d5ef MPIcuFFT_Slab_Z_Then_YX<double>::All2All_MPIType()  ???:0
10 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
11 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
12 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
13 0x0000000000403591 main()  ???:0
14 0x00000000000236a3 __libc_start_main()  ???:0
15 0x00000000004039ee _start()  ???:0
=================================
[uc2n515:1472856:0:1472856] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14ba5a000000)
==== backtrace (tid:1472856) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x0000000000121b20 mca_coll_basic_alltoallw_intra()  ???:0
 8 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 9 0x000000000002d5ef MPIcuFFT_Slab_Z_Then_YX<double>::All2All_MPIType()  ???:0
10 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
11 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
12 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
13 0x0000000000403591 main()  ???:0
14 0x00000000000236a3 __libc_start_main()  ???:0
15 0x00000000004039ee _start()  ???:0
=================================
[uc2n515:1472857:0:1472857] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14d048000000)
==== backtrace (tid:1472857) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x0000000000121b20 mca_coll_basic_alltoallw_intra()  ???:0
 8 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 9 0x000000000002d5ef MPIcuFFT_Slab_Z_Then_YX<double>::All2All_MPIType()  ???:0
10 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
11 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
12 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
13 0x0000000000403591 main()  ???:0
14 0x00000000000236a3 __libc_start_main()  ???:0
15 0x00000000004039ee _start()  ???:0
=================================
[uc2n515:1472858:0:1472858] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x1519de000000)
==== backtrace (tid:1472858) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x0000000000121b20 mca_coll_basic_alltoallw_intra()  ???:0
 8 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 9 0x000000000002d5ef MPIcuFFT_Slab_Z_Then_YX<double>::All2All_MPIType()  ???:0
10 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
11 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
12 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
13 0x0000000000403591 main()  ???:0
14 0x00000000000236a3 __libc_start_main()  ???:0
15 0x00000000004039ee _start()  ???:0
=================================
[uc2n515:1472859:0:1472859] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14bd46000000)
==== backtrace (tid:1472859) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x0000000000121b20 mca_coll_basic_alltoallw_intra()  ???:0
 8 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 9 0x000000000002d5ef MPIcuFFT_Slab_Z_Then_YX<double>::All2All_MPIType()  ???:0
10 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
11 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
12 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
13 0x0000000000403591 main()  ???:0
14 0x00000000000236a3 __libc_start_main()  ???:0
15 0x00000000004039ee _start()  ???:0
=================================
[uc2n515:1472860:0:1472860] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14b92a000000)
==== backtrace (tid:1472860) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000ce941 mca_btl_openib_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x0000000000121b20 mca_coll_basic_alltoallw_intra()  ???:0
 8 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 9 0x000000000002d5ef MPIcuFFT_Slab_Z_Then_YX<double>::All2All_MPIType()  ???:0
10 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
11 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
12 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
13 0x0000000000403591 main()  ???:0
14 0x00000000000236a3 __libc_start_main()  ???:0
15 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
mpiexec noticed that process rank 12 with PID 1113257 on node uc2n517 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
Starting computation for size 128
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 09:17:52.724522
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 09:18:02.116946
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 09:18:11.126049
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 09:18:19.567327
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 09:18:28.744417
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 09:18:37.055199
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 09:18:49.424828
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 09:18:57.877377
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 09:19:07.070538
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 09:19:15.557098
b''

Starting computation for size [128, 128, 256]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 09:19:28.237438
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 09:19:36.748481
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 09:19:46.313242
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 09:19:54.634106
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 09:20:03.872054
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 09:20:12.492551
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 09:20:24.997639
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 09:20:33.559096
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 09:20:42.681391
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 09:20:51.093404
b''

Starting computation for size [128, 256, 256]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 09:21:03.865663
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 09:21:12.594287
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 09:21:21.861585
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 09:21:30.391340
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 09:21:39.292793
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 09:21:47.804791
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 09:22:03.530138
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 09:22:11.999647
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 09:22:21.266147
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 09:22:29.803771
b''

Starting computation for size 256
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 09:22:46.240520
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 09:22:54.712007
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 09:23:03.946113
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 09:23:12.775355
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 09:23:21.847073
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 09:23:30.416228
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 09:23:52.320373
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 09:24:00.824227
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 09:24:09.992281
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 09:24:18.664952
b''

Starting computation for size [256, 256, 512]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 09:24:41.447127
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 09:24:50.185115
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 09:24:59.566039
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 09:25:08.412404
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 09:25:17.538352
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 09:25:26.620087
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 09:25:49.258762
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 09:25:57.932033
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 09:26:07.408270
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 09:26:16.322455
b''

Starting computation for size [256, 512, 512]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 09:26:39.235125
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 09:26:48.575125
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 09:26:58.194864
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 09:27:07.252836
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 09:27:16.570511
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 09:27:25.983701
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 09:28:01.134051
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 09:28:10.264173
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 09:28:19.793120
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 09:28:29.345096
b''

Starting computation for size 512
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 09:29:06.340407
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 09:29:16.024990
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 09:29:25.576324
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 09:29:34.734637
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 09:29:46.871819
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 09:29:57.094443
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 09:30:57.614649
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 09:31:07.296440
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 09:31:16.776136
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 09:31:26.749072
b''

Starting computation for size [512, 512, 1024]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 09:32:29.583829
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 09:32:40.828410
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 09:32:51.253003
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 09:33:02.100165
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 09:33:12.516334
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 09:33:24.973156
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 09:34:26.516980
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 09:34:37.647067
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 09:34:48.085647
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 09:35:00.397674
b''

Starting computation for size [512, 1024, 1024]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 09:36:05.101957
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 09:36:19.772928
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 09:36:31.849182
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 09:36:45.692797
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 09:36:57.298436
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 09:37:14.063037
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 09:39:06.825031
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 09:39:20.845391
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 09:39:32.536823
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 09:39:53.723554
b''

Starting computation for size 1024
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 09:41:54.204034
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 09:42:15.315419
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 09:42:30.485374
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 09:42:50.209676
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 09:43:05.455493
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 09:43:30.987352
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 09:47:07.247086
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 09:47:27.814609
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 09:47:43.223000
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 09:48:08.464441
b''

Starting computation for size [1024, 1024, 2048]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 09:51:58.128075
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 09:52:34.072977
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 09:52:57.513216
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 09:53:30.547783
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 09:53:54.083852
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 09:54:37.846239
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 09:58:21.687979
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 09:58:56.494718
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 09:59:20.508368
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 10:00:02.669290
b''

Starting computation for size [1024, 2048, 2048]
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 10:03:59.180753
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 10:05:03.939522
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 10:05:43.344548
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 10:06:43.065065
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 10:07:23.036839
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 10:08:43.977363
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 10:16:05.571978
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 10:17:08.689353
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 10:17:48.457470
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 10:19:06.562097
b''

Starting computation for size 2048
-> Executing test 0
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 10:26:51.606729
b''

-> Executing test 1
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 10:28:54.262023
b''

-> Executing test 2
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 10:30:05.056771
b''

-> Executing test 3
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 10:31:57.771568
b''

-> Executing test 4
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 10:33:08.840203
b''

-> Executing test 5
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 10:35:42.088647
b''

-> Executing test 6
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 10:50:22.395016
b''

-> Executing test 7
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 10:50:39.173323
b''

-> Executing test 8
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 10:50:51.474830
b''

-> Executing test 9
mpiexec -n 16 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 16 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 10:53:22.368989
b''

Slab 1D->2D opt1 (inverse)
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A rank is missing its location specification:

  Rank:        16
  Rank file:   ../mpi/rankfile_0

All processes must have their location specified in the rank file. Either
add an entry to the file, or provide a default slot_list to use for
any unspecified ranks.
--------------------------------------------------------------------------
Starting computation for size 128
-> Executing test 0
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 10:53:32.964896
b''

-> Executing test 1
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 10:53:33.286263
b''

-> Executing test 2
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 10:53:33.539144
b''

-> Executing test 3
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 10:53:33.788317
b''

-> Executing test 4
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 10:53:34.036169
b''

-> Executing test 5
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 10:53:34.541353
b''

-> Executing test 6
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 10:53:34.787979
b''

-> Executing test 7
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 10:53:35.037128
b''

-> Executing test 8
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 10:53:35.300678
b''

-> Executing test 9
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 10:53:35.548388
b''

Starting computation for size [128, 128, 256]
-> Executing test 0
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 10:53:35.796069
b''

-> Executing test 1
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 10:53:36.042028
b''

-> Executing test 2
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 10:53:36.312212
b''

-> Executing test 3
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 10:53:36.781402
b''

-> Executing test 4
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 10:53:37.026243
b''

-> Executing test 5
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 10:53:37.302657
b''

-> Executing test 6
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 10:53:37.548981
b''

-> Executing test 7
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 10:53:37.795291
b''

-> Executing test 8
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 10:53:38.040794
b''

-> Executing test 9
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 10:53:38.323754
b''

Starting computation for size [128, 256, 256]
-> Executing test 0
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 10:53:38.577453
b''

-> Executing test 1
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 10:53:39.073905
b''

-> Executing test 2
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 10:53:39.348925
b''

-> Executing test 3
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 10:53:39.598760
b''

-> Executing test 4
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 10:53:39.846409
b''

-> Executing test 5
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 10:53:40.092416
b''

-> Executing test 6
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 10:53:40.357794
b''

-> Executing test 7
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 10:53:40.607864
b''

-> Executing test 8
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 10:53:40.858014
b''

-> Executing test 9
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 10:53:41.369195
b''

Starting computation for size 256
-> Executing test 0
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 10:53:41.619503
b''

-> Executing test 1
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 10:53:41.868857
b''

-> Executing test 2
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 10:53:42.114744
b''

-> Executing test 3
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 10:53:42.379967
b''

-> Executing test 4
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 10:53:42.628485
b''

-> Executing test 5
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 10:53:42.874646
b''

-> Executing test 6
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 10:53:43.123848
b''

-> Executing test 7
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 10:53:43.640101
b''

-> Executing test 8
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 10:53:43.887198
b''

-> Executing test 9
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 10:53:44.151250
b''

Starting computation for size [256, 256, 512]
-> Executing test 0
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 10:53:44.397721
b''

-> Executing test 1
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 10:53:44.643826
b''

-> Executing test 2
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 10:53:44.892491
b''

-> Executing test 3
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 10:53:45.152762
b''

-> Executing test 4
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 10:53:45.401413
b''

-> Executing test 5
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 10:53:45.904068
b''

-> Executing test 6
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 10:53:46.171341
b''

-> Executing test 7
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 10:53:46.417608
b''

-> Executing test 8
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 10:53:46.665813
b''

-> Executing test 9
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 10:53:46.910231
b''

Starting computation for size [256, 512, 512]
-> Executing test 0
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 10:53:47.173697
b''

-> Executing test 1
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 10:53:47.419925
b''

-> Executing test 2
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 10:53:47.665999
b''

-> Executing test 3
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 10:53:48.198452
b''

-> Executing test 4
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 10:53:48.444513
b''

-> Executing test 5
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 10:53:48.696839
b''

-> Executing test 6
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 10:53:48.945959
b''

-> Executing test 7
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 10:53:49.212140
b''

-> Executing test 8
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 10:53:49.457831
b''

-> Executing test 9
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 10:53:49.705638
b''

Starting computation for size 512
-> Executing test 0
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 10:53:49.953331
b''

-> Executing test 1
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 10:53:50.494492
b''

-> Executing test 2
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 10:53:50.742341
b''

-> Executing test 3
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 10:53:50.990273
b''

-> Executing test 4
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 10:53:51.255389
b''

-> Executing test 5
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 10:53:51.499472
b''

-> Executing test 6
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 10:53:51.745556
b''

-> Executing test 7
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 10:53:51.991346
b''

-> Executing test 8
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 10:53:52.257690
b''

-> Executing test 9
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 10:53:52.768502
b''

Starting computation for size [512, 512, 1024]
-> Executing test 0
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 10:53:53.016245
b''

-> Executing test 1
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 10:53:53.280258
b''

-> Executing test 2
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 10:53:53.525875
b''

-> Executing test 3
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 10:53:53.771382
b''

-> Executing test 4
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 10:53:54.017940
b''

-> Executing test 5
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 10:53:54.297777
b''

-> Executing test 6
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 10:53:54.544272
b''

-> Executing test 7
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 10:53:55.071127
b''

-> Executing test 8
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 10:53:55.352922
b''

-> Executing test 9
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 10:53:55.604157
b''

Starting computation for size [512, 1024, 1024]
-> Executing test 0
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 10:53:55.858267
b''

-> Executing test 1
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 10:53:56.105417
b''

-> Executing test 2
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 10:53:56.387264
b''

-> Executing test 3
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 10:53:56.634036
b''

-> Executing test 4
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 10:53:56.882317
b''

-> Executing test 5
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 10:53:57.403225
b''

-> Executing test 6
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 10:53:57.652825
b''

-> Executing test 7
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 10:53:57.898076
b''

-> Executing test 8
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 10:53:58.161330
b''

-> Executing test 9
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 10:53:58.407490
b''

Starting computation for size 1024
-> Executing test 0
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 10:53:59.123618
b''

-> Executing test 1
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 10:53:59.693843
b''

-> Executing test 2
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 10:53:59.940264
b''

-> Executing test 3
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 10:54:00.204740
b''

-> Executing test 4
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 10:54:00.451901
b''

-> Executing test 5
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 10:54:00.701444
b''

-> Executing test 6
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 10:54:00.949459
b''

-> Executing test 7
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 10:54:01.212231
b''

-> Executing test 8
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 10:54:01.458821
b''

-> Executing test 9
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 10:54:02.001900
b''

Starting computation for size [1024, 1024, 2048]
-> Executing test 0
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 10:54:02.268386
b''

-> Executing test 1
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 10:54:02.516699
b''

-> Executing test 2
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 10:54:02.763946
b''

-> Executing test 3
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 10:54:03.009980
b''

-> Executing test 4
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 10:54:03.276013
b''

-> Executing test 5
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 10:54:03.522877
b''

-> Executing test 6
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 10:54:03.766957
b''

-> Executing test 7
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 10:54:04.356926
b''

-> Executing test 8
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 10:54:04.603851
b''

-> Executing test 9
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 10:54:04.853166
b''

Starting computation for size [1024, 2048, 2048]
-> Executing test 0
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 10:54:05.102384
b''

-> Executing test 1
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 10:54:05.363232
b''

-> Executing test 2
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 10:54:05.612670
b''

-> Executing test 3
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 10:54:05.859086
b''

-> Executing test 4
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 10:54:06.104283
b''

-> Executing test 5
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 10:54:06.666339
b''

-> Executing test 6
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 10:54:06.910173
b''

-> Executing test 7
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 10:54:07.171261
b''

-> Executing test 8
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 10:54:07.419086
b''

-> Executing test 9
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 10:54:07.665438
b''

Starting computation for size 2048
-> Executing test 0
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 10:54:07.914304
b''

-> Executing test 1
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 10:54:08.180354
b''

-> Executing test 2
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 10:54:08.425780
b''

-> Executing test 3
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 10:54:08.956696
b''

-> Executing test 4
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 10:54:09.219470
b''

-> Executing test 5
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 10:54:09.466188
b''

-> Executing test 6
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 10:54:09.712750
b''

-> Executing test 7
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 10:54:09.960064
b''

-> Executing test 8
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 10:54:10.220765
b''

-> Executing test 9
mpiexec -n 24 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 24 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 10:54:10.465629
b''

Starting on HOST8
-----------------------------------------------------------------------------
Slab 2D->1D default
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1474912] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1474912] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1474909] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1474909] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1475180] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1475180] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1475270] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1475270] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1475451] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1475451] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1475541] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1475541] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1475983] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1475983] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1476097] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1476097] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1476502] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1476502] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1476614] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1476614] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1476770] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1476770] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1476884] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1476884] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1477045] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1477045] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1477162] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1477162] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1477315] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1477315] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1477472] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1477472] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1477588] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1477588] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1477763] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1477763] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1477874] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1477874] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1478042] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1478042] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1478147] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1478147] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1478388] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1478388] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1478419] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1478419] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1478664] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1478664] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1478691] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1478691] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1478934] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1478934] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1479208] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1479208] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1479446] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1479446] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1479738] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1479738] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1479983] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1479983] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1480008] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1480008] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1480251] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1480251] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1480285] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1480285] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1480523] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1480523] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1480558] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1480558] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1480796] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1480796] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1480828] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1480828] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1481067] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1481067] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1481113] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1481113] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1481358] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1481358] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1481384] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1481384] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1481627] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1481627] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1481654] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1481654] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1481899] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1481899] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1481927] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1481927] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1482172] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1482172] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1482445] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1482445] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1482690] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1482690] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1482964] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1482964] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1483225] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1483225] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1483250] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1483250] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1483502] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1483502] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1483521] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1483521] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1483774] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1483774] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1483791] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1483791] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1484037] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1484037] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1484064] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1484064] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1484309] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1484309] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1484335] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1484335] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1484578] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1484578] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1484618] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1484618] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1484873] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1484873] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1484891] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1484891] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1485146] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1485146] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1485163] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1485163] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1485416] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1485416] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1485683] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1485683] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1485934] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1485934] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1486199] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1486199] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1486455] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1486455] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1486474] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1486474] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1486739] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1486739] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1486756] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1486756] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1487012] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1487012] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1487029] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1487029] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1487282] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1487282] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1487302] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1487302] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1487557] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1487557] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1487574] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1487574] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1487827] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1487827] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1487847] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1487847] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1488117] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1488117] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1488134] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1488134] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1488386] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1488386] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1488405] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1488405] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1488658] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1488658] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1488926] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1488926] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1489179] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1489179] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1489443] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1489443] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1489696] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1489696] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1489714] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1489714] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1489981] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1489981] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1489998] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1489998] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1490254] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1490254] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1490273] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1490273] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1490527] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1490527] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1490547] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1490547] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1490802] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1490802] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1490819] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1490819] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1491073] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1491073] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1491092] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1491092] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1491359] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1491359] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1491377] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1491377] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1491633] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1491633] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1491650] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1491650] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1491968] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1491968] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1492171] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1492171] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1492434] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1492434] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1492691] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1492691] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1492944] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1492944] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1492978] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1492978] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1493231] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1493231] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1493249] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1493249] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1493504] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1493504] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1493521] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1493521] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1493779] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1493779] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1493796] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1493796] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1494049] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1494049] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1494067] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1494067] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1494335] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1494335] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1494352] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1494352] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1494608] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1494608] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1494625] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1494625] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1494879] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1494879] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1494898] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1494898] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1495373] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1495373] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1495415] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1495415] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1495930] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1495930] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1495947] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1495947] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1496203] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1496203] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1496221] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1496221] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1496477] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1496477] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1496496] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1496496] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1496747] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1496747] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1496765] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1496765] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1497036] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1497036] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1497053] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1497053] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1497352] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1497352] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1497369] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1497369] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1497627] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1497627] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1497644] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1497644] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1497899] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1497899] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1497929] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1497929] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1498185] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1498185] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1498202] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1498202] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1498642] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1498642] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1498725] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1498725] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1499225] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1499225] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1499244] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1499244] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1499508] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1499508] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1499527] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1499527] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1499784] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1499784] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1499801] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1499801] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1500055] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1500055] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1500090] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1500090] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1500344] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1500344] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1500361] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1500361] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1500615] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1500615] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1500633] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1500633] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1500887] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1500887] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1500919] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1500919] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1501173] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1501173] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1501193] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1501193] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1501447] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1501447] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1501466] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1501466] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1501870] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1501870] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1501999] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1501999] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1502483] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1502483] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1502523] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1502523] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1502793] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1502793] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1502812] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1502812] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1503070] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1503070] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1503088] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1503088] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1503354] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1503354] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1503373] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1503373] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1503631] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1503631] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1503649] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1503649] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1503919] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1503919] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1503938] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1503938] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1504195] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1504195] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1504212] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1504212] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1504485] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1504485] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1504509] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1504509] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1504777] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1504777] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1504794] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1504794] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1505164] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1505164] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1505333] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1505333] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1505789] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1505789] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1505855] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1505855] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1506129] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1506129] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1506154] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1506154] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1506424] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1506424] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1506441] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1506441] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1506698] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1506698] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1506735] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1506735] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1506991] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1506991] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1507025] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1507025] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1507284] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1507284] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1507321] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1507321] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1507583] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1507583] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1507600] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1507600] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1507872] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1507872] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1507920] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1507920] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1508190] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1508190] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1508213] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1508213] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1508565] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1508565] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1508777] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1508777] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1509193] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1509193] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1509320] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1509320] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1509580] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1509580] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1509621] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1509621] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1509904] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1509904] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1509920] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1509920] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1510191] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1510191] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1510233] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1510233] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1510513] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1510513] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1510532] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1510532] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1510809] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1510809] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1510851] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1510851] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1511129] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1511129] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1511149] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1511149] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1511424] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1511424] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1511510] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1511510] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1511785] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1511785] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1511811] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1511811] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1512165] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1512165] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1512405] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1512405] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1512821] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1512821] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1512972] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1512972] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1513244] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1513244] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1513332] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1513332] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1513613] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1513613] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1513637] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1513637] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515:1513648:0:1513648] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x1466b8280000)
[uc2n515:1513650:0:1513650] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x1496e8380000)
[uc2n515:1513649:0:1513649] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x153148300000)
[uc2n515:1513647:0:1513647] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x148d24200000)
==== backtrace (tid:1513648) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001069f MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
 9 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
10 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
11 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1513647) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001069f MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
 9 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
10 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
11 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1513650) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001069f MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
 9 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
10 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
11 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1513649) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001069f MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
 9 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
10 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
11 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec noticed that process rank 6 with PID 1513649 on node uc2n515 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515:1513954:0:1513954] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x1517de380010)
[uc2n515:1513951:0:1513951] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14b9ae200010)
[uc2n515:1513953:0:1513953] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14cc52300010)
[uc2n515:1513952:0:1513952] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14c2c6280010)
==== backtrace (tid:1513951) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x00000000001d5e98 cuMemGetAttribute()  ???:0
 3 0x00000000002ffe53 cudbgGetAPI()  ???:0
 4 0x00000000003009e8 cudbgGetAPI()  ???:0
 5 0x0000000000435d90 cudbgApiInit()  ???:0
 6 0x000000000018d730 ???()  /lib64/libcuda.so.1:0
 7 0x000000000018df04 ???()  /lib64/libcuda.so.1:0
 8 0x00000000001904d9 ???()  /lib64/libcuda.so.1:0
 9 0x00000000001fe77e cuGraphAddMemAllocNode()  ???:0
10 0x00000000000a2944 mca_common_cuda_cu_memcpy()  common_cuda.c:0
11 0x000000000007c563 opal_cuda_memcpy_sync()  ???:0
12 0x0000000000077119 non_overlap_cuda_copy_content_same_ddt()  opal_datatype_copy.c:0
13 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
14 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
15 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
16 0x000000000009042b PMPI_Alltoallv()  ???:0
17 0x000000000001069f MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
18 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
19 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
20 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
21 0x000000000040331f main()  ???:0
22 0x00000000000236a3 __libc_start_main()  ???:0
23 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1513952) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x00000000001d5e98 cuMemGetAttribute()  ???:0
 3 0x00000000002ffe53 cudbgGetAPI()  ???:0
 4 0x00000000003009e8 cudbgGetAPI()  ???:0
 5 0x0000000000435d90 cudbgApiInit()  ???:0
 6 0x000000000018d730 ???()  /lib64/libcuda.so.1:0
 7 0x000000000018df04 ???()  /lib64/libcuda.so.1:0
 8 0x00000000001904d9 ???()  /lib64/libcuda.so.1:0
 9 0x00000000001fe77e cuGraphAddMemAllocNode()  ???:0
10 0x00000000000a2944 mca_common_cuda_cu_memcpy()  common_cuda.c:0
11 0x000000000007c563 opal_cuda_memcpy_sync()  ???:0
12 0x0000000000077119 non_overlap_cuda_copy_content_same_ddt()  opal_datatype_copy.c:0
13 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
14 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
15 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
16 0x000000000009042b PMPI_Alltoallv()  ???:0
17 0x000000000001069f MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
18 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
19 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
20 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
21 0x000000000040331f main()  ???:0
22 0x00000000000236a3 __libc_start_main()  ???:0
23 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1513954) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x00000000001d5e98 cuMemGetAttribute()  ???:0
 3 0x00000000002ffe53 cudbgGetAPI()  ???:0
 4 0x00000000003009e8 cudbgGetAPI()  ???:0
 5 0x0000000000435d90 cudbgApiInit()  ???:0
 6 0x000000000018d730 ???()  /lib64/libcuda.so.1:0
 7 0x000000000018df04 ???()  /lib64/libcuda.so.1:0
 8 0x00000000001904d9 ???()  /lib64/libcuda.so.1:0
 9 0x00000000001fe77e cuGraphAddMemAllocNode()  ???:0
10 0x00000000000a2944 mca_common_cuda_cu_memcpy()  common_cuda.c:0
11 0x000000000007c563 opal_cuda_memcpy_sync()  ???:0
12 0x0000000000077119 non_overlap_cuda_copy_content_same_ddt()  opal_datatype_copy.c:0
13 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
14 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
15 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
16 0x000000000009042b PMPI_Alltoallv()  ???:0
17 0x000000000001069f MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
18 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
19 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
20 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
21 0x000000000040331f main()  ???:0
22 0x00000000000236a3 __libc_start_main()  ???:0
23 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1513953) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x00000000001d5e98 cuMemGetAttribute()  ???:0
 3 0x00000000002ffe53 cudbgGetAPI()  ???:0
 4 0x00000000003009e8 cudbgGetAPI()  ???:0
 5 0x0000000000435d90 cudbgApiInit()  ???:0
 6 0x000000000018d730 ???()  /lib64/libcuda.so.1:0
 7 0x000000000018df04 ???()  /lib64/libcuda.so.1:0
 8 0x00000000001904d9 ???()  /lib64/libcuda.so.1:0
 9 0x00000000001fe77e cuGraphAddMemAllocNode()  ???:0
10 0x00000000000a2944 mca_common_cuda_cu_memcpy()  common_cuda.c:0
11 0x000000000007c563 opal_cuda_memcpy_sync()  ???:0
12 0x0000000000077119 non_overlap_cuda_copy_content_same_ddt()  opal_datatype_copy.c:0
13 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
14 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
15 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
16 0x000000000009042b PMPI_Alltoallv()  ???:0
17 0x000000000001069f MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
18 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
19 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
20 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
21 0x000000000040331f main()  ???:0
22 0x00000000000236a3 __libc_start_main()  ???:0
23 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec noticed that process rank 4 with PID 1513951 on node uc2n515 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
[uc2n515.localdomain:1513941] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1513941] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1514236] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1514236] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515:1514250:0:1514250] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14ee4c300000)
[uc2n515:1514251:0:1514251] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x152bb8380000)
[uc2n515:1514248:0:1514248] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14a854200000)
[uc2n515:1514249:0:1514249] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x152c64280000)
==== backtrace (tid:1514248) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x0000000000010d69 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
 8 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
 9 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
10 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1514250) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x0000000000010d69 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
 8 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
 9 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
10 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1514249) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x0000000000010d69 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
 8 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
 9 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
10 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1514251) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x0000000000010d69 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
 8 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
 9 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
10 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec noticed that process rank 7 with PID 1514251 on node uc2n515 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515:1514522:0:1514522] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14d086280000)
[uc2n515:1514521:0:1514521] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x145fe6200000)
[uc2n515:1514523:0:1514523] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14f38a300000)
[uc2n515:1514524:0:1514524] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x1506ce380000)
==== backtrace (tid:1514523) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x0000000000010d69 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
 8 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
 9 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
10 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1514522) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x0000000000010d69 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
 8 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
 9 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
10 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1514524) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x0000000000010d69 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
 8 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
 9 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
10 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1514521) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x0000000000010d69 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
 8 0x0000000000017c2b MPIcuFFT_Slab<double>::execR2C()  ???:0
 9 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
10 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec noticed that process rank 7 with PID 1514524 on node uc2n515 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
[uc2n515.localdomain:1514511] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1514511] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1514664] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1514664] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[46825,1],7]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1514822] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1514822] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n517:1151759:0:1151759] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14d4be200000)
[uc2n517:1151761:0:1151761] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14fef6300000)
[uc2n517:1151760:0:1151760] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x1508ec280000)
[uc2n517:1151762:0:1151762] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x154c46380000)
==== backtrace (tid:1151762) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001088e MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
 9 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
10 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
11 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1151761) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001088e MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
 9 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
10 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
11 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1151760) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001088e MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
 9 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
10 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
11 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1151759) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001088e MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
 9 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
10 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
11 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[46483,1],6]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1515068] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1515068] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
mpiexec noticed that process rank 7 with PID 1151762 on node uc2n517 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[46225,1],7]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1515326] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1515326] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[35815,1],2]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1515592] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1515592] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n517:1152085:0:1152085] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14e160200010)
[uc2n517:1152088:0:1152088] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14b8ae380010)
[uc2n517:1152086:0:1152086] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14f78e280010)
[uc2n517:1152087:0:1152087] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x154ebe300010)
[uc2n517:1152083:0:1152083] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14ebf4200000)
[uc2n517:1152082:0:1152082] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14b18e200000)
[uc2n517:1152084:0:1152084] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14e18a200000)
[uc2n517:1152081:0:1152081] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14f488200000)
==== backtrace (tid:1152086) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x00000000001d5e80 cuMemGetAttribute()  ???:0
 3 0x00000000002ffe53 cudbgGetAPI()  ???:0
 4 0x00000000004359a6 cudbgApiInit()  ???:0
 5 0x000000000018d730 ???()  /lib64/libcuda.so.1:0
 6 0x000000000018df04 ???()  /lib64/libcuda.so.1:0
 7 0x00000000001904d9 ???()  /lib64/libcuda.so.1:0
 8 0x00000000001fe77e cuGraphAddMemAllocNode()  ???:0
 9 0x00000000000a2944 mca_common_cuda_cu_memcpy()  common_cuda.c:0
10 0x000000000007c563 opal_cuda_memcpy_sync()  ???:0
11 0x0000000000077119 non_overlap_cuda_copy_content_same_ddt()  opal_datatype_copy.c:0
12 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
13 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
14 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
15 0x000000000009042b PMPI_Alltoallv()  ???:0
16 0x000000000001088e MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
17 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
18 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
19 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
20 0x000000000040331f main()  ???:0
==== backtrace (tid:1152087) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x00000000001d5e80 cuMemGetAttribute()  ???:0
 3 0x00000000002ffe53 cudbgGetAPI()  ???:0
 4 0x00000000004359a6 cudbgApiInit()  ???:0
 5 0x000000000018d730 ???()  /lib64/libcuda.so.1:0
 6 0x000000000018df04 ???()  /lib64/libcuda.so.1:0
 7 0x00000000001904d9 ???()  /lib64/libcuda.so.1:0
 8 0x00000000001fe77e cuGraphAddMemAllocNode()  ???:0
 9 0x00000000000a2944 mca_common_cuda_cu_memcpy()  common_cuda.c:0
10 0x000000000007c563 opal_cuda_memcpy_sync()  ???:0
11 0x0000000000077119 non_overlap_cuda_copy_content_same_ddt()  opal_datatype_copy.c:0
12 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
13 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
14 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
15 0x000000000009042b PMPI_Alltoallv()  ???:0
16 0x000000000001088e MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
17 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
18 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
19 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
20 0x000000000040331f main()  ???:0
21 0x00000000000236a3 __libc_start_main()  ???:0
22 0x00000000004039ee _start()  ???:0
=================================
21 0x00000000000236a3 __libc_start_main()  ???:0
22 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1152088) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x00000000001d5e80 cuMemGetAttribute()  ???:0
 3 0x00000000002ffe53 cudbgGetAPI()  ???:0
 4 0x00000000004359a6 cudbgApiInit()  ???:0
 5 0x000000000018d730 ???()  /lib64/libcuda.so.1:0
 6 0x000000000018df04 ???()  /lib64/libcuda.so.1:0
 7 0x00000000001904d9 ???()  /lib64/libcuda.so.1:0
 8 0x00000000001fe77e cuGraphAddMemAllocNode()  ???:0
 9 0x00000000000a2944 mca_common_cuda_cu_memcpy()  common_cuda.c:0
10 0x000000000007c563 opal_cuda_memcpy_sync()  ???:0
11 0x0000000000077119 non_overlap_cuda_copy_content_same_ddt()  opal_datatype_copy.c:0
12 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
13 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
14 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
15 0x000000000009042b PMPI_Alltoallv()  ???:0
16 0x000000000001088e MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
17 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
18 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
19 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
20 0x000000000040331f main()  ???:0
21 0x00000000000236a3 __libc_start_main()  ???:0
22 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1152085) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x00000000001d5e80 cuMemGetAttribute()  ???:0
 3 0x00000000002ffe53 cudbgGetAPI()  ???:0
 4 0x00000000004359a6 cudbgApiInit()  ???:0
 5 0x000000000018d730 ???()  /lib64/libcuda.so.1:0
 6 0x000000000018df04 ???()  /lib64/libcuda.so.1:0
 7 0x00000000001904d9 ???()  /lib64/libcuda.so.1:0
 8 0x00000000001fe77e cuGraphAddMemAllocNode()  ???:0
 9 0x00000000000a2944 mca_common_cuda_cu_memcpy()  common_cuda.c:0
10 0x000000000007c563 opal_cuda_memcpy_sync()  ???:0
11 0x0000000000077119 non_overlap_cuda_copy_content_same_ddt()  opal_datatype_copy.c:0
12 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
13 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
14 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
15 0x000000000009042b PMPI_Alltoallv()  ???:0
16 0x000000000001088e MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
17 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
18 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
19 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
20 0x000000000040331f main()  ???:0
21 0x00000000000236a3 __libc_start_main()  ???:0
22 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1152083) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015ddb5 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000cb0a6 mca_btl_smcuda_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x000000000001088e MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
11 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
12 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
13 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
14 0x000000000040331f main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1152081) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015ddb5 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000cb0a6 mca_btl_smcuda_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x000000000001088e MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
11 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
12 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
13 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
14 0x000000000040331f main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1152084) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015ddb5 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000cb0a6 mca_btl_smcuda_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x000000000001088e MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
11 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
12 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
13 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
14 0x000000000040331f main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1152082) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015ddb5 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000cb0a6 mca_btl_smcuda_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x000000000001088e MPIcuFFT_Slab<double>::All2All_Sync()  ???:0
11 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
12 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
13 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
14 0x000000000040331f main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec noticed that process rank 5 with PID 1152086 on node uc2n517 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
[uc2n515.localdomain:1515478] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1515478] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[35552,1],7]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1515855] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1515855] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[35315,1],5]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1516124] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1516124] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[35020,1],7]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1516387] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1516387] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1516117] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1516117] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[36810,1],4]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1516645] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1516645] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[36551,1],5]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1516904] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1516904] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[36308,1],7]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1517179] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1517179] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1517437] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1517437] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1517700] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1517700] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1517960] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1517960] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1518239] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1518239] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1518513] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1518513] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1518780] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1518780] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1519037] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1519037] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1519293] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1519293] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1519552] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1519552] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1519810] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1519810] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1520079] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1520079] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1520337] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1520337] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1520593] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1520593] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1520877] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1520877] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1521149] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1521149] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1521422] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1521422] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1521680] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1521680] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n517:1152794:0:1152794] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x151c48280000)
[uc2n517:1152796:0:1152796] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x1468c2380000)
[uc2n517:1152793:0:1152793] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14c6ee200000)
[uc2n517:1152795:0:1152795] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x147986300000)
==== backtrace (tid:1152793) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x000000000008d803 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x0000000000010f81 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
 8 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
 9 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
10 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1152795) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x000000000008d803 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x0000000000010f81 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
 8 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
 9 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
10 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1152796) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x000000000008d803 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x0000000000010f81 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
 8 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
 9 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
10 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1152794) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x000000000008d803 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x0000000000010f81 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
 8 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
 9 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
10 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
[uc2n517:1152790:0:1152790] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x146858200000)
[uc2n517:1152789:0:1152789] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14bbe6200000)
[uc2n517:1152791:0:1152791] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x1477de200000)
[uc2n517:1152792:0:1152792] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x149d56200000)
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
==== backtrace (tid:1152789) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015ddb5 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000cb0a6 mca_btl_smcuda_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x0000000000121b20 mca_coll_basic_alltoallw_intra()  ???:0
 8 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 9 0x0000000000010f81 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
10 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
11 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
12 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
13 0x000000000040331f main()  ???:0
14 0x00000000000236a3 __libc_start_main()  ???:0
15 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1152792) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015ddb5 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000cb0a6 mca_btl_smcuda_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x0000000000121b20 mca_coll_basic_alltoallw_intra()  ???:0
 8 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 9 0x0000000000010f81 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
10 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
11 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
12 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
13 0x000000000040331f main()  ???:0
14 0x00000000000236a3 __libc_start_main()  ???:0
15 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1152790) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015ddb5 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000cb0a6 mca_btl_smcuda_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x0000000000121b20 mca_coll_basic_alltoallw_intra()  ???:0
 8 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 9 0x0000000000010f81 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
10 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
11 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
12 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
13 0x000000000040331f main()  ???:0
14 0x00000000000236a3 __libc_start_main()  ???:0
15 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1152791) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015ddb5 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000cb0a6 mca_btl_smcuda_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x0000000000121b20 mca_coll_basic_alltoallw_intra()  ???:0
 8 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 9 0x0000000000010f81 MPIcuFFT_Slab<double>::All2All_MPIType()  ???:0
10 0x0000000000010205 MPIcuFFT_Slab<double>::execC2R()  ???:0
11 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
12 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
13 0x000000000040331f main()  ???:0
14 0x00000000000236a3 __libc_start_main()  ???:0
15 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
mpiexec noticed that process rank 7 with PID 1152796 on node uc2n517 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
[uc2n515.localdomain:1521932] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1521932] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1521948] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1521948] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[37297,1],3]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1522206] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1522206] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1522223] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1522223] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[36997,1],1]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1522474] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1522474] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1522492] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1522492] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[38814,1],4]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1522737] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1522737] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[38758,1],6]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1522889] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1522889] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1522776] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1522776] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[38604,1],4]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1523043] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1523043] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[38384,1],4]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1523295] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1523295] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1523060] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1523060] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[38191,1],7]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1523328] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1523328] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1523347] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1523347] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[37921,1],4]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1523598] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1523598] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[37916,1],6]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1523635] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1523635] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1523647] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1523647] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[60259,1],4]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1523916] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1523916] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Starting computation for size 128
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 10:54:10.777965
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 10:54:16.208719
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 10:54:22.145826
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 10:54:27.557599
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 10:54:33.575800
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 10:54:38.607560
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 10:54:44.664137
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 10:54:50.178048
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 10:54:56.147759
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 10:55:01.330051
b''

Starting computation for size [128, 128, 256]
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 10:55:07.358835
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 10:55:12.618968
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 10:55:18.508586
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 10:55:23.687046
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 10:55:29.576677
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 10:55:34.704188
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 10:55:40.607313
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 10:55:45.827151
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 10:55:51.678567
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 10:55:56.962021
b''

Starting computation for size [128, 256, 256]
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 10:56:03.034514
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 10:56:08.315520
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 10:56:14.197389
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 10:56:19.436555
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 10:56:25.393306
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 10:56:30.737653
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 10:56:36.595517
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 10:56:41.828290
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 10:56:47.625889
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 10:56:52.982446
b''

Starting computation for size 256
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 10:56:58.878261
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 10:57:04.365160
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 10:57:10.431393
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 10:57:15.939585
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 10:57:22.015969
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 10:57:27.592949
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 10:57:33.679538
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 10:57:39.198500
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 10:57:45.234288
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 10:57:50.752784
b''

Starting computation for size [256, 256, 512]
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 10:57:56.815591
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 10:58:02.634833
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 10:58:08.862828
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 10:58:14.537866
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 10:58:20.634588
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 10:58:26.496243
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 10:58:32.671645
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 10:58:38.630043
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 10:58:44.719952
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 10:58:50.596280
b''

Starting computation for size [256, 512, 512]
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 10:58:56.927050
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 10:59:03.614034
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 10:59:10.128480
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 10:59:16.571623
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 10:59:22.957612
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 10:59:29.616419
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 10:59:36.003915
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 10:59:43.535819
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 10:59:50.063757
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 10:59:56.912731
b''

Starting computation for size 512
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 11:00:04.278588
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 11:00:12.677338
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 11:00:19.864108
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 11:00:27.486148
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 11:00:34.522546
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 11:00:43.100027
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 11:00:50.450297
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 11:00:58.903295
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 11:01:06.154118
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 11:01:14.918588
b''

Starting computation for size [512, 512, 1024]
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 11:01:22.620757
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 11:01:34.073699
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 11:01:42.885030
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 11:01:53.132590
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 11:02:01.741900
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 11:02:13.655902
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 11:02:22.464324
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 11:02:34.405327
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 11:02:43.116198
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 11:02:55.090049
b''

Starting computation for size [512, 1024, 1024]
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 11:03:04.841331
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 11:03:22.318864
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 11:03:33.828202
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 11:03:49.443068
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 11:04:00.884492
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 11:04:19.608292
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 11:04:31.330406
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 11:04:49.949013
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 11:05:01.413986
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 11:05:20.400756
b''

Starting computation for size 1024
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 11:05:33.637310
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 11:06:04.049756
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 11:06:20.911102
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 11:06:47.230734
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 11:07:04.071211
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 11:07:35.393817
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 11:07:52.507724
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 11:08:24.565020
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 11:08:41.813779
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 11:09:14.874776
b''

Starting computation for size [1024, 1024, 2048]
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 11:09:35.420799
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 11:10:32.507419
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 11:11:00.156124
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 11:11:46.640137
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 11:12:15.060809
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 11:13:15.619672
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 11:13:43.337407
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 11:14:42.999855
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 11:15:11.347661
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 11:16:11.872734
b''

Starting computation for size [1024, 2048, 2048]
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 11:16:46.472906
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 11:18:32.637137
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 11:19:22.068285
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 11:20:49.170605
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 11:21:39.826178
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 11:23:31.560083
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 11:24:21.085531
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 11:24:33.718130
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 11:24:43.541091
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 11:26:38.695757
b''

Starting computation for size 2048
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 11:26:43.479642
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 11:26:49.123190
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 11:26:54.199462
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 11:26:59.251123
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 11:27:03.980614
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 11:27:08.497390
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 11:27:13.219304
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 11:27:17.826563
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 11:27:22.339311
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 11:27:27.014566
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1523951] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1523951] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1524212] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1524212] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1524473] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1524473] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1524733] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1524733] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1525006] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1525006] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1525266] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1525266] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1525523] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1525523] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1525801] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1525801] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1526082] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1526082] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1526389] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1526389] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1526685] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1526685] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1526963] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1526963] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1527242] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1527242] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1527521] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1527521] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1527800] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1527800] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1528082] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1528082] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1528363] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1528363] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
mpiexec has exited due to process rank 4 with PID 1528373 on
node uc2n515 exiting improperly. There are three reasons this could occur:

1. this process did not call "init" before exiting, but others in
the job did. This can cause a job to hang indefinitely while it waits
for all processes to call "init". By rule, if one process calls "init",
then ALL processes must call "init" prior to termination.

2. this process called "init", but exited without calling "finalize".
By rule, all processes that call "init" MUST call "finalize" prior to
exiting or it will be considered an "abnormal termination"

3. this process called "MPI_Abort" or "orte_abort" and the mca parameter
orte_create_session_dirs is set to false. In this case, the run-time cannot
detect that the abort call was an abnormal termination. Hence, the only
error message you will receive is this one.

This may have caused other processes in the application to be
terminated by signals sent by mpiexec (as reported here).

You can avoid this message by specifying -quiet on the mpiexec command line.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1528623] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1528623] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
mpiexec has exited due to process rank 5 with PID 1528634 on
node uc2n515 exiting improperly. There are three reasons this could occur:

1. this process did not call "init" before exiting, but others in
the job did. This can cause a job to hang indefinitely while it waits
for all processes to call "init". By rule, if one process calls "init",
then ALL processes must call "init" prior to termination.

2. this process called "init", but exited without calling "finalize".
By rule, all processes that call "init" MUST call "finalize" prior to
exiting or it will be considered an "abnormal termination"

3. this process called "MPI_Abort" or "orte_abort" and the mca parameter
orte_create_session_dirs is set to false. In this case, the run-time cannot
detect that the abort call was an abnormal termination. Hence, the only
error message you will receive is this one.

This may have caused other processes in the application to be
terminated by signals sent by mpiexec (as reported here).

You can avoid this message by specifying -quiet on the mpiexec command line.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1528896] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1528896] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
mpiexec has exited due to process rank 5 with PID 1528907 on
node uc2n515 exiting improperly. There are three reasons this could occur:

1. this process did not call "init" before exiting, but others in
the job did. This can cause a job to hang indefinitely while it waits
for all processes to call "init". By rule, if one process calls "init",
then ALL processes must call "init" prior to termination.

2. this process called "init", but exited without calling "finalize".
By rule, all processes that call "init" MUST call "finalize" prior to
exiting or it will be considered an "abnormal termination"

3. this process called "MPI_Abort" or "orte_abort" and the mca parameter
orte_create_session_dirs is set to false. In this case, the run-time cannot
detect that the abort call was an abnormal termination. Hence, the only
error message you will receive is this one.

This may have caused other processes in the application to be
terminated by signals sent by mpiexec (as reported here).

You can avoid this message by specifying -quiet on the mpiexec command line.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1529156] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1529156] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
mpiexec has exited due to process rank 4 with PID 1529166 on
node uc2n515 exiting improperly. There are three reasons this could occur:

1. this process did not call "init" before exiting, but others in
the job did. This can cause a job to hang indefinitely while it waits
for all processes to call "init". By rule, if one process calls "init",
then ALL processes must call "init" prior to termination.

2. this process called "init", but exited without calling "finalize".
By rule, all processes that call "init" MUST call "finalize" prior to
exiting or it will be considered an "abnormal termination"

3. this process called "MPI_Abort" or "orte_abort" and the mca parameter
orte_create_session_dirs is set to false. In this case, the run-time cannot
detect that the abort call was an abnormal termination. Hence, the only
error message you will receive is this one.

This may have caused other processes in the application to be
terminated by signals sent by mpiexec (as reported here).

You can avoid this message by specifying -quiet on the mpiexec command line.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1529420] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1529420] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
mpiexec has exited due to process rank 7 with PID 1529433 on
node uc2n515 exiting improperly. There are three reasons this could occur:

1. this process did not call "init" before exiting, but others in
the job did. This can cause a job to hang indefinitely while it waits
for all processes to call "init". By rule, if one process calls "init",
then ALL processes must call "init" prior to termination.

2. this process called "init", but exited without calling "finalize".
By rule, all processes that call "init" MUST call "finalize" prior to
exiting or it will be considered an "abnormal termination"

3. this process called "MPI_Abort" or "orte_abort" and the mca parameter
orte_create_session_dirs is set to false. In this case, the run-time cannot
detect that the abort call was an abnormal termination. Hence, the only
error message you will receive is this one.

This may have caused other processes in the application to be
terminated by signals sent by mpiexec (as reported here).

You can avoid this message by specifying -quiet on the mpiexec command line.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1529696] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1529696] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
mpiexec has exited due to process rank 4 with PID 1529706 on
node uc2n515 exiting improperly. There are three reasons this could occur:

1. this process did not call "init" before exiting, but others in
the job did. This can cause a job to hang indefinitely while it waits
for all processes to call "init". By rule, if one process calls "init",
then ALL processes must call "init" prior to termination.

2. this process called "init", but exited without calling "finalize".
By rule, all processes that call "init" MUST call "finalize" prior to
exiting or it will be considered an "abnormal termination"

3. this process called "MPI_Abort" or "orte_abort" and the mca parameter
orte_create_session_dirs is set to false. In this case, the run-time cannot
detect that the abort call was an abnormal termination. Hence, the only
error message you will receive is this one.

This may have caused other processes in the application to be
terminated by signals sent by mpiexec (as reported here).

You can avoid this message by specifying -quiet on the mpiexec command line.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1529961] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1529961] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
mpiexec has exited due to process rank 4 with PID 1529971 on
node uc2n515 exiting improperly. There are three reasons this could occur:

1. this process did not call "init" before exiting, but others in
the job did. This can cause a job to hang indefinitely while it waits
for all processes to call "init". By rule, if one process calls "init",
then ALL processes must call "init" prior to termination.

2. this process called "init", but exited without calling "finalize".
By rule, all processes that call "init" MUST call "finalize" prior to
exiting or it will be considered an "abnormal termination"

3. this process called "MPI_Abort" or "orte_abort" and the mca parameter
orte_create_session_dirs is set to false. In this case, the run-time cannot
detect that the abort call was an abnormal termination. Hence, the only
error message you will receive is this one.

This may have caused other processes in the application to be
terminated by signals sent by mpiexec (as reported here).

You can avoid this message by specifying -quiet on the mpiexec command line.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1530233] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1530233] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
mpiexec has exited due to process rank 5 with PID 1530244 on
node uc2n515 exiting improperly. There are three reasons this could occur:

1. this process did not call "init" before exiting, but others in
the job did. This can cause a job to hang indefinitely while it waits
for all processes to call "init". By rule, if one process calls "init",
then ALL processes must call "init" prior to termination.

2. this process called "init", but exited without calling "finalize".
By rule, all processes that call "init" MUST call "finalize" prior to
exiting or it will be considered an "abnormal termination"

3. this process called "MPI_Abort" or "orte_abort" and the mca parameter
orte_create_session_dirs is set to false. In this case, the run-time cannot
detect that the abort call was an abnormal termination. Hence, the only
error message you will receive is this one.

This may have caused other processes in the application to be
terminated by signals sent by mpiexec (as reported here).

You can avoid this message by specifying -quiet on the mpiexec command line.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1530496] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1530496] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
mpiexec has exited due to process rank 4 with PID 1530506 on
node uc2n515 exiting improperly. There are three reasons this could occur:

1. this process did not call "init" before exiting, but others in
the job did. This can cause a job to hang indefinitely while it waits
for all processes to call "init". By rule, if one process calls "init",
then ALL processes must call "init" prior to termination.

2. this process called "init", but exited without calling "finalize".
By rule, all processes that call "init" MUST call "finalize" prior to
exiting or it will be considered an "abnormal termination"

3. this process called "MPI_Abort" or "orte_abort" and the mca parameter
orte_create_session_dirs is set to false. In this case, the run-time cannot
detect that the abort call was an abnormal termination. Hence, the only
error message you will receive is this one.

This may have caused other processes in the application to be
terminated by signals sent by mpiexec (as reported here).

You can avoid this message by specifying -quiet on the mpiexec command line.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1530776] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1530776] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
mpiexec has exited due to process rank 4 with PID 1530786 on
node uc2n515 exiting improperly. There are three reasons this could occur:

1. this process did not call "init" before exiting, but others in
the job did. This can cause a job to hang indefinitely while it waits
for all processes to call "init". By rule, if one process calls "init",
then ALL processes must call "init" prior to termination.

2. this process called "init", but exited without calling "finalize".
By rule, all processes that call "init" MUST call "finalize" prior to
exiting or it will be considered an "abnormal termination"

3. this process called "MPI_Abort" or "orte_abort" and the mca parameter
orte_create_session_dirs is set to false. In this case, the run-time cannot
detect that the abort call was an abnormal termination. Hence, the only
error message you will receive is this one.

This may have caused other processes in the application to be
terminated by signals sent by mpiexec (as reported here).

You can avoid this message by specifying -quiet on the mpiexec command line.
--------------------------------------------------------------------------
Starting computation for size 128
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 128
2021-09-19 10:54:10.789340
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 128
2021-09-19 10:54:15.964329
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 128
2021-09-19 10:54:21.830018
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 128
2021-09-19 10:54:26.975719
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 128
2021-09-19 10:54:32.792452
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 128
2021-09-19 10:54:38.020264
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 128
2021-09-19 10:54:43.978564
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 128
2021-09-19 10:54:48.982418
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 128
2021-09-19 10:54:54.794884
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 128
2021-09-19 10:54:59.964907
b''

Starting computation for size [128, 128, 256]
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 256
2021-09-19 10:55:05.748745
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 256
2021-09-19 10:55:10.761932
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 256
2021-09-19 10:55:16.720123
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 256
2021-09-19 10:55:21.897786
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 256
2021-09-19 10:55:27.832501
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 256
2021-09-19 10:55:32.949548
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 256
2021-09-19 10:55:38.931953
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 256
2021-09-19 10:55:44.118704
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 256
2021-09-19 10:55:49.991906
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 256
2021-09-19 10:55:55.174906
b''

Starting computation for size [128, 256, 256]
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 256 -nz 256
2021-09-19 10:56:01.061101
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 256 -nz 256
2021-09-19 10:56:06.381307
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 256 -nz 256
2021-09-19 10:56:12.247896
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 256 -nz 256
2021-09-19 10:56:17.524936
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 256 -nz 256
2021-09-19 10:56:23.339133
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 256 -nz 256
2021-09-19 10:56:28.634780
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 256 -nz 256
2021-09-19 10:56:34.498225
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 256 -nz 256
2021-09-19 10:56:39.821904
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 256 -nz 256
2021-09-19 10:56:45.811055
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 256 -nz 256
2021-09-19 10:56:51.055339
b''

Starting computation for size 256
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 256
2021-09-19 10:56:56.877040
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 256
2021-09-19 10:57:02.265902
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 256
2021-09-19 10:57:08.204315
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 256
2021-09-19 10:57:13.532079
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 256
2021-09-19 10:57:19.419338
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 256
2021-09-19 10:57:24.690732
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 256
2021-09-19 10:57:30.589234
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 256
2021-09-19 10:57:36.050045
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 256
2021-09-19 10:57:42.057912
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 256
2021-09-19 10:57:47.582843
b''

Starting computation for size [256, 256, 512]
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 512
2021-09-19 10:57:53.558495
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 512
2021-09-19 10:57:59.326928
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 512
2021-09-19 10:58:05.482782
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 512
2021-09-19 10:58:11.203702
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 512
2021-09-19 10:58:17.299258
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 512
2021-09-19 10:58:23.071747
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 512
2021-09-19 10:58:29.158911
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 512
2021-09-19 10:58:35.010667
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 512
2021-09-19 10:58:41.175683
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 512
2021-09-19 10:58:47.174182
b''

Starting computation for size [256, 512, 512]
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 512 -nz 512
2021-09-19 10:58:53.452316
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 512 -nz 512
2021-09-19 10:59:00.071935
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 512 -nz 512
2021-09-19 10:59:06.542067
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 512 -nz 512
2021-09-19 10:59:12.716053
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 512 -nz 512
2021-09-19 10:59:19.211851
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 512 -nz 512
2021-09-19 10:59:25.730017
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 512 -nz 512
2021-09-19 10:59:32.217835
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 512 -nz 512
2021-09-19 10:59:38.995697
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 512 -nz 512
2021-09-19 10:59:45.422560
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 512 -nz 512
2021-09-19 10:59:52.239538
b''

Starting computation for size 512
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 512
2021-09-19 10:59:58.952065
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 512
2021-09-19 11:00:07.030960
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 512
2021-09-19 11:00:14.229388
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 512
2021-09-19 11:00:21.778709
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 512
2021-09-19 11:00:28.859683
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 512
2021-09-19 11:00:37.147175
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 512
2021-09-19 11:00:44.195869
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 512
2021-09-19 11:00:52.630546
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 512
2021-09-19 11:00:59.817191
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 512
2021-09-19 11:01:08.268542
b''

Starting computation for size [512, 512, 1024]
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 1024
2021-09-19 11:01:15.850249
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 1024
2021-09-19 11:01:27.011184
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 1024
2021-09-19 11:01:35.620475
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 1024
2021-09-19 11:01:45.872493
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 1024
2021-09-19 11:01:54.466870
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 1024
2021-09-19 11:02:05.917951
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 1024
2021-09-19 11:02:14.485933
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 1024
2021-09-19 11:02:26.239684
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 1024
2021-09-19 11:02:34.706423
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 1024
2021-09-19 11:02:46.666642
b''

Starting computation for size [512, 1024, 1024]
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 1024 -nz 1024
2021-09-19 11:02:56.028763
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 1024 -nz 1024
2021-09-19 11:03:13.254580
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 1024 -nz 1024
2021-09-19 11:03:24.790347
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 1024 -nz 1024
2021-09-19 11:03:40.410026
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 1024 -nz 1024
2021-09-19 11:03:51.712706
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 1024 -nz 1024
2021-09-19 11:04:09.256166
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 1024 -nz 1024
2021-09-19 11:04:20.379565
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 1024 -nz 1024
2021-09-19 11:04:38.997999
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 1024 -nz 1024
2021-09-19 11:04:50.472349
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 1024 -nz 1024
2021-09-19 11:05:09.297327
b''

Starting computation for size 1024
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 1024
2021-09-19 11:05:22.317878
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 1024
2021-09-19 11:05:51.801103
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 1024
2021-09-19 11:06:08.404706
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 1024
2021-09-19 11:06:34.655411
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 1024
2021-09-19 11:06:51.356023
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 1024
2021-09-19 11:07:21.657596
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 1024
2021-09-19 11:07:38.229592
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 1024
2021-09-19 11:08:10.907054
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 1024
2021-09-19 11:08:28.350586
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 1024
2021-09-19 11:09:00.774339
b''

Starting computation for size [1024, 1024, 2048]
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 2048
2021-09-19 11:09:20.482852
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 2048
2021-09-19 11:10:14.786655
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 2048
2021-09-19 11:10:43.261591
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 2048
2021-09-19 11:11:29.579331
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 2048
2021-09-19 11:11:57.468166
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 2048
2021-09-19 11:12:53.321920
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 2048
2021-09-19 11:13:20.748743
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 2048
2021-09-19 11:14:19.893108
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 2048
2021-09-19 11:14:48.089299
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 2048
2021-09-19 11:15:47.652950
b''

Starting computation for size [1024, 2048, 2048]
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 2048 -nz 2048
2021-09-19 11:16:21.508767
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 2048 -nz 2048
2021-09-19 11:18:04.546687
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 2048 -nz 2048
2021-09-19 11:18:54.883998
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 2048 -nz 2048
2021-09-19 11:20:22.472893
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 2048 -nz 2048
2021-09-19 11:21:12.145831
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 2048 -nz 2048
2021-09-19 11:22:58.043838
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 2048 -nz 2048
2021-09-19 11:23:46.201602
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 2048 -nz 2048
2021-09-19 11:23:57.092284
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 2048 -nz 2048
2021-09-19 11:24:06.324055
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 2048 -nz 2048
2021-09-19 11:24:16.905073
b''

Starting computation for size 2048
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 2048 -ny 2048 -nz 2048
2021-09-19 11:24:26.283285
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 2048 -ny 2048 -nz 2048
2021-09-19 11:24:29.381552
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 2048 -ny 2048 -nz 2048
2021-09-19 11:24:33.008589
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 2048 -ny 2048 -nz 2048
2021-09-19 11:24:36.512081
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 2048 -ny 2048 -nz 2048
2021-09-19 11:24:40.208687
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 2048 -ny 2048 -nz 2048
2021-09-19 11:24:43.818875
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 2048 -ny 2048 -nz 2048
2021-09-19 11:24:47.401339
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 2048 -ny 2048 -nz 2048
2021-09-19 11:24:50.970768
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 2048 -ny 2048 -nz 2048
2021-09-19 11:24:54.486488
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 2048 -ny 2048 -nz 2048
2021-09-19 11:24:58.091613
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

Starting computation for size 128
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 11:25:01.627893
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 11:25:07.758914
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 11:25:13.539287
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 11:25:19.117662
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 11:25:24.901819
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 11:25:30.336129
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 11:25:36.092656
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 11:25:41.752723
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 11:25:47.560571
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 11:25:53.081934
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

Starting computation for size 256
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 11:25:59.035690
b'Result (avg): 5.01735e-09\nResult (max): 5.42241e-08\n'

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 11:26:04.862037
b'Result (avg): 5.01735e-09\nResult (max): 5.42241e-08\n'

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 11:26:10.936566
b'Result (avg): 5.01735e-09\nResult (max): 5.42241e-08\n'

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 11:26:16.785002
b'Result (avg): 5.01735e-09\nResult (max): 5.42241e-08\n'

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 11:26:22.855666
b'Result (avg): 5.01735e-09\nResult (max): 5.42241e-08\n'

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 11:26:28.542911
b'Result (avg): 5.01735e-09\nResult (max): 5.42241e-08\n'

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 11:26:34.746522
b'Result (avg): 5.01735e-09\nResult (max): 5.42241e-08\n'

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 11:26:40.534033
b'Result (avg): 5.01735e-09\nResult (max): 5.42241e-08\n'

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 11:26:46.651236
b'Result (avg): 5.01735e-09\nResult (max): 5.42241e-08\n'

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 11:26:52.354343
b'Result (avg): 5.01735e-09\nResult (max): 5.42241e-08\n'

Starting computation for size 512
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 11:26:58.471523
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 11:27:06.812109
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 11:27:14.999137
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 11:27:23.249391
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 11:27:31.445137
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 11:27:39.724778
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 11:27:47.896384
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 11:27:56.296853
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 11:28:04.871515
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 11:28:13.587116
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

Starting computation for size 1024
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 11:28:22.384701
b'Result (avg): 7.38396e-07\nResult (max): 9.23959e-06\n'

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 11:28:51.322601
b'Result (avg): 7.38396e-07\nResult (max): 9.23959e-06\n'

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 11:29:18.599884
b'Result (avg): 7.38396e-07\nResult (max): 9.23959e-06\n'

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 11:29:47.249574
b'Result (avg): 7.38396e-07\nResult (max): 9.23959e-06\n'

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 11:30:14.727226
b'Result (avg): 7.38396e-07\nResult (max): 9.23959e-06\n'

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 11:30:43.890163
b'Result (avg): 7.38396e-07\nResult (max): 9.23959e-06\n'

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 11:31:11.141378
b'Result (avg): 7.38396e-07\nResult (max): 9.23959e-06\n'

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 11:31:40.222634
b'Result (avg): 7.38396e-07\nResult (max): 9.23959e-06\n'

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 11:32:07.633116
b'Result (avg): 7.38396e-07\nResult (max): 9.23959e-06\n'

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 11:32:36.703684
b'Result (avg): 7.38396e-07\nResult (max): 9.23959e-06\n'

Starting computation for size 2048
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 11:33:04.281037
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\n'

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 11:33:14.161987
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\n'

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 11:33:28.097695
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\n'

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 11:33:42.036816
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\n'

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 11:33:56.066389
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\n'

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 11:34:10.112844
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\n'

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 11:34:24.195658
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\n'

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 11:34:38.094503
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\n'

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 11:34:52.069768
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\n'

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 11:35:06.082772
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\n'

Slab 2D->1D opt1
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1531041] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1531041] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1531040] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1531040] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1531306] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1531306] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1531339] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1531339] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1531591] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1531591] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1531610] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1531610] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1531857] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1531857] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1532130] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1532130] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1532388] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1532388] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1532405] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1532405] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1532556] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1532556] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1532678] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1532678] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1532929] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1532929] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1532958] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1532958] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1533064] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1533064] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1533229] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1533229] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1533375] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1533375] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1533500] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1533500] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1533593] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1533593] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1533773] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1533773] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1534025] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1534025] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1534041] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1534041] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1534058] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1534058] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1534323] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1534323] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1534331] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1534331] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1534594] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1534594] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1534610] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1534610] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1535116] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1535116] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1535133] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1535133] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1535386] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1535386] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1535403] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1535403] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1535769] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1535769] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1535673] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1535673] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1535947] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1535947] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1535965] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1535965] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1536111] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1536111] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1536239] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1536239] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1536329] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1536329] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1536509] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1536509] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1536765] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1536765] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1536782] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1536782] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1537048] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1537048] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1537064] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1537064] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1537081] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1537081] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1537334] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1537334] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1537353] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1537353] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1537609] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1537609] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1537626] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1537626] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1537871] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1537871] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1538147] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1538147] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1538414] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1538414] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1538563] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1538563] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1538775] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1538775] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1538687] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1538687] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1538957] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1538957] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1538970] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1538970] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1538987] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1538987] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1539261] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1539261] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1539517] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1539517] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1539534] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1539534] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1539787] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1539787] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1539804] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1539804] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1540065] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1540065] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1540077] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1540077] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1540097] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1540097] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1540364] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1540364] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1540383] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1540383] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1540628] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1540628] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1540656] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1540656] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1541163] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1541163] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1541312] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1541312] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1541436] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1541436] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1541526] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1541526] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1541798] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1541798] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1541708] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1541708] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1541986] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1541986] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1542006] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1542006] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1542267] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1542267] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1542519] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1542519] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1542537] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1542537] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1542791] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1542791] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1542825] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1542825] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1543078] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1543078] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1543094] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1543094] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1543115] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1543115] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1543261] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1543261] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1543385] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1543385] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1543475] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1543475] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1543664] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1543664] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1544182] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1544182] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1544435] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1544435] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1544452] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1544452] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1544710] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1544710] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1544727] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1544727] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1544982] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1544982] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1545013] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1545013] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1545033] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1545033] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1545294] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1545294] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1545548] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1545548] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1545565] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1545565] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1545820] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1545820] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1545853] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1545853] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1546106] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1546106] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1546122] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1546122] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1546140] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1546140] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1546157] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1546157] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1546410] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1546410] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1546430] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1546430] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1546703] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1546703] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1547208] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1547208] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1547469] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1547469] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1547724] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1547724] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1547742] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1547742] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1548007] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1548007] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1548023] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1548023] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1548042] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1548042] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1548079] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1548079] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1548337] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1548337] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1548599] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1548599] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1548852] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1548852] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1548869] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1548869] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1549126] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1549126] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1549153] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1549153] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1549166] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1549166] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1549184] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1549184] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1549203] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1549203] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1549469] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1549469] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1549489] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1549489] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1549751] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1549751] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1550257] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1550257] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1550623] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1550623] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1550535] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1550535] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1550802] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1550802] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1550819] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1550819] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1551075] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1551075] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1551091] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1551091] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1551130] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1551130] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1551390] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1551390] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1551657] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1551657] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1551747] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1551747] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1551929] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1551929] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1552129] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1552129] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1552201] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1552201] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1552233] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1552233] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1552251] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1552251] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1552269] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1552269] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1552525] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1552525] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1552554] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1552554] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1552817] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1552817] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1553330] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1553330] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1553604] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1553604] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1553866] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1553866] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1554123] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1554123] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1554155] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1554155] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1554173] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1554173] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1554201] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1554201] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1554226] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1554226] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1554498] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1554498] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1554758] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1554758] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1555020] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1555020] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1555298] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1555298] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1555319] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1555319] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1555347] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1555347] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1555367] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1555367] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1555397] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1555397] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1555651] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1555651] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1555672] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1555672] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1555945] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1555945] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1555962] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1555962] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1556474] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1556474] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1556748] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1556748] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1557015] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1557015] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1557318] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1557318] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1557349] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1557349] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1557367] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1557367] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1557404] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1557404] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1557561] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1557561] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1557699] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1557699] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1557963] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1557963] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1558239] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1558239] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1558549] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1558549] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1558587] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1558587] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1558619] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1558619] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1558870] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1558870] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1558914] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1558914] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1559061] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1559061] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1559207] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1559207] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1559360] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1559360] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1559749] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1559749] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1560005] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1560005] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1560027] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1560027] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1560313] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1560313] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1560630] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1560630] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1560668] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1560668] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1560693] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1560693] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1560852] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1560852] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1560988] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1560988] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1561250] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1561250] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1561327] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1561327] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1561605] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1561605] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1561928] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1561928] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1561991] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1561991] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1562246] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1562246] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1562283] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1562283] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1562322] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1562322] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1562600] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1562600] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1562619] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1562619] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1563048] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1563048] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1563168] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1563168] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1563468] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1563468] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1563722] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1563722] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1563786] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1563786] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1564171] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1564171] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1564240] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1564240] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1564496] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1564496] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1564533] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1564533] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1564573] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1564573] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1564861] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1564861] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1565129] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1565129] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1565198] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1565198] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1565602] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1565602] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1565667] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1565667] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1565968] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1565968] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1566033] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1566033] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1566055] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1566055] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1566361] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1566361] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1566713] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1566713] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1566950] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1566950] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1566967] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1566967] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1567277] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1567277] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1567581] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1567581] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1567644] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1567644] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1568220] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1568220] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n517:1196177:0:1196177] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x147476200000)
[uc2n517:1196178:0:1196178] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14c0c8280000)
[uc2n517:1196179:0:1196179] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x15116c300000)
[uc2n517:1196180:0:1196180] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14a45c380000)
[uc2n517:1196176:0:1196176] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x146518200000)
[uc2n517:1196174:0:1196174] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14bce4200000)
[uc2n517:1196173:0:1196173] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14fc70200000)
[uc2n517:1196175:0:1196175] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14812c200000)
==== backtrace (tid:1196178) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001ee17 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
 9 0x000000000001e765 MPIcuFFT_Slab_Opt1<double>::execC2R()  ???:0
10 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
11 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1196180) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001ee17 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
 9 0x000000000001e765 MPIcuFFT_Slab_Opt1<double>::execC2R()  ???:0
10 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
11 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1196175) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015ddb5 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000cb0a6 mca_btl_smcuda_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x000000000001ee17 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
11 0x000000000001e765 MPIcuFFT_Slab_Opt1<double>::execC2R()  ???:0
12 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
13 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
14 0x000000000040331f main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1196177) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001ee17 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
 9 0x000000000001e765 MPIcuFFT_Slab_Opt1<double>::execC2R()  ???:0
10 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
11 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1196174) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015ddb5 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000cb0a6 mca_btl_smcuda_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x000000000001ee17 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
11 0x000000000001e765 MPIcuFFT_Slab_Opt1<double>::execC2R()  ???:0
12 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
13 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
14 0x000000000040331f main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1196176) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015ddb5 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000cb0a6 mca_btl_smcuda_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x000000000001ee17 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
11 0x000000000001e765 MPIcuFFT_Slab_Opt1<double>::execC2R()  ???:0
12 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
13 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
14 0x000000000040331f main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1196173) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015ddb5 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000cb0a6 mca_btl_smcuda_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x000000000001ee17 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
11 0x000000000001e765 MPIcuFFT_Slab_Opt1<double>::execC2R()  ???:0
12 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
13 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
14 0x000000000040331f main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1196179) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001ee17 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
 9 0x000000000001e765 MPIcuFFT_Slab_Opt1<double>::execC2R()  ???:0
10 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
11 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec noticed that process rank 2 with PID 1196175 on node uc2n517 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
The call to cuMemcpyAsync failed. This is a unrecoverable error and will
cause the program to abort.
  cuMemcpyAsync(0x14c506c00000, 0x14c2fb000000, 131072) returned value 1
Check the cuda.h file for what the return value means.
--------------------------------------------------------------------------
[uc2n517.localdomain:1196529] CUDA: Error in cuMemcpy: res=-1, dest=0x14c506c00000, src=0x14c2fb000000, size=131072
[uc2n517:1196529] *** Process received signal ***
[uc2n517:1196529] Signal: Aborted (6)
[uc2n517:1196529] Signal code:  (-6)
[uc2n517:1196529] [ 0] /usr/lib64/libpthread.so.0(+0x12dd0)[0x14c7781f8dd0]
[uc2n517:1196529] [ 1] /usr/lib64/libc.so.6(gsignal+0x10f)[0x14c777e5b70f]
[uc2n517:1196529] [ 2] /usr/lib64/libc.so.6(abort+0x127)[0x14c777e45b25]
[uc2n517:1196529] [ 3] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libopen-pal.so.40(+0x7c58f)[0x14c776dc558f]
[uc2n517:1196529] [ 4] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libopen-pal.so.40(+0x77119)[0x14c776dc0119]
[uc2n517:1196529] [ 5] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libmpi.so.40(ompi_datatype_sndrcv+0x50a)[0x14c783bfeb2a]
[uc2n517:1196529] [ 6] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libmpi.so.40(ompi_coll_base_alltoallv_intra_basic_linear+0x26e)[0x14c783c5380e]
[uc2n517:1196529] [ 7] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libmpi.so.40(ompi_coll_tuned_alltoallv_intra_dec_fixed+0x42)[0x14c783c5f702]
[uc2n517:1196529] [ 8] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libmpi.so.40(MPI_Alltoallv+0x1ab)[0x14c783c0142b]
[uc2n517:1196529] [ 9] /home/st/st_us-051200/st_st160727/DistributedFFT/build/libslab_decomp.so(_ZN18MPIcuFFT_Slab_Opt1IdE12All2All_SyncEPvb+0x3e7)[0x14c78e34ee17]
[uc2n517:1196529] [10] /home/st/st_us-051200/st_st160727/DistributedFFT/build/libslab_decomp.so(_ZN18MPIcuFFT_Slab_Opt1IdE7execC2REPvPKv+0x115)[0x14c78e34e765]
[uc2n517:1196529] [11] /home/st/st_us-051200/st_st160727/DistributedFFT/build/libslab_tests.so(_ZN25Tests_Slab_Random_DefaultIdE9testcase2Eii+0x51f)[0x14c78e59b8d9]
[uc2n517:1196529] [12] /home/st/st_us-051200/st_st160727/DistributedFFT/build/libslab_tests.so(_ZN25Tests_Slab_Random_DefaultIdE3runEiii+0x67)[0x14c78e59ac55]
[uc2n517:1196529] [13] slab[0x40331f]
[uc2n517:1196529] [14] /usr/lib64/libc.so.6(__libc_start_main+0xf3)[0x14c777e476a3]
[uc2n517:1196529] [15] slab[0x4039ee]
[uc2n517:1196529] *** End of error message ***
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
[uc2n515.localdomain:1568240] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1568240] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
mpiexec noticed that process rank 7 with PID 1196529 on node uc2n517 exited on signal 6 (Aborted).
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1568271] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1568271] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n517:1196820:0:1196820] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14aad6280000)
[uc2n517:1196819:0:1196819] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14b5ec200000)
[uc2n517:1196822:0:1196822] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14c7fe380000)
[uc2n517:1196821:0:1196821] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x153058300000)
==== backtrace (tid:1196822) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000001f4df MPIcuFFT_Slab_Opt1<double>::All2All_MPIType()  ???:0
 8 0x000000000001e765 MPIcuFFT_Slab_Opt1<double>::execC2R()  ???:0
 9 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
10 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1196821) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000001f4df MPIcuFFT_Slab_Opt1<double>::All2All_MPIType()  ???:0
 8 0x000000000001e765 MPIcuFFT_Slab_Opt1<double>::execC2R()  ???:0
 9 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
10 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1196820) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000001f4df MPIcuFFT_Slab_Opt1<double>::All2All_MPIType()  ???:0
 8 0x000000000001e765 MPIcuFFT_Slab_Opt1<double>::execC2R()  ???:0
 9 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
10 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1196819) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000001f4df MPIcuFFT_Slab_Opt1<double>::All2All_MPIType()  ???:0
 8 0x000000000001e765 MPIcuFFT_Slab_Opt1<double>::execC2R()  ???:0
 9 0x00000000000178d9 Tests_Slab_Random_Default<double>::testcase2()  ???:0
10 0x0000000000016c55 Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec noticed that process rank 5 with PID 1196820 on node uc2n517 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
The call to cuMemcpyAsync failed. This is a unrecoverable error and will
cause the program to abort.
  cuMemcpyAsync(0x152026800000, 0x9414d80, 65536) returned value 1
Check the cuda.h file for what the return value means.
--------------------------------------------------------------------------
[uc2n517.localdomain:1197132] CUDA: Error in cuMemcpy: res=-1, dest=0x152026800000, src=0x9414d80, size=65536
[uc2n517:1197132] *** Process received signal ***
[uc2n517:1197132] Signal: Aborted (6)
[uc2n517:1197132] Signal code:  (-6)
[uc2n517:1197132] [ 0] /usr/lib64/libpthread.so.0(+0x12dd0)[0x15229667bdd0]
[uc2n517:1197132] [ 1] /usr/lib64/libc.so.6(gsignal+0x10f)[0x1522962de70f]
[uc2n517:1197132] [ 2] /usr/lib64/libc.so.6(abort+0x127)[0x1522962c8b25]
[uc2n517:1197132] [ 3] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libopen-pal.so.40(+0x7c375)[0x152295248375]
[uc2n517:1197132] [ 4] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libopen-pal.so.40(opal_convertor_unpack+0xb9)[0x15229523f399]
[uc2n517:1197132] [ 5] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libmpi.so.40(ompi_datatype_sndrcv+0x1f7)[0x1522a2081817]
[uc2n517:1197132] [ 6] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libmpi.so.40(mca_coll_basic_alltoallw_intra+0xa4)[0x1522a2115984]
[uc2n517:1197132] [ 7] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libmpi.so.40(MPI_Alltoallw+0x1eb)[0x1522a2084ccb]
[uc2n517:1197132] [ 8] /home/st/st_us-051200/st_st160727/DistributedFFT/build/libslab_decomp.so(_ZN18MPIcuFFT_Slab_Opt1IdE15All2All_MPITypeEPvb+0x35f)[0x1522ac7d24df]
[uc2n517:1197132] [ 9] /home/st/st_us-051200/st_st160727/DistributedFFT/build/libslab_decomp.so(_ZN18MPIcuFFT_Slab_Opt1IdE7execC2REPvPKv+0x115)[0x1522ac7d1765]
[uc2n517:1197132] [10] /home/st/st_us-051200/st_st160727/DistributedFFT/build/libslab_tests.so(_ZN25Tests_Slab_Random_DefaultIdE9testcase2Eii+0x51f)[0x1522aca1e8d9]
[uc2n517:1197132] [11] /home/st/st_us-051200/st_st160727/DistributedFFT/build/libslab_tests.so(_ZN25Tests_Slab_Random_DefaultIdE3runEiii+0x67)[0x1522aca1dc55]
[uc2n517:1197132] [12] slab[0x40331f]
[uc2n517:1197132] [13] /usr/lib64/libc.so.6(__libc_start_main+0xf3)[0x1522962ca6a3]
[uc2n517:1197132] [14] slab[0x4039ee]
[uc2n517:1197132] *** End of error message ***
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
[uc2n515.localdomain:1568290] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1568290] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
mpiexec noticed that process rank 7 with PID 1197132 on node uc2n517 exited on signal 6 (Aborted).
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[17809,1],5]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1568318] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1568318] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[17889,1],5]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1568334] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1568334] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[17870,1],4]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1568353] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1568353] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1568350] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1568350] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[17601,1],4]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1568622] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1568622] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515:1568366:0:1568366] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x1498ec380000)
[uc2n515:1568365:0:1568365] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14c58a300000)
[uc2n515:1568364:0:1568364] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x145d86280000)
[uc2n515:1568363:0:1568363] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14bc04200000)
==== backtrace (tid:1568366) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001eb28 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
 9 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
==== backtrace (tid:1568363) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001eb28 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
 9 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
10 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
11 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
10 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
11 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1568365) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001eb28 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
 9 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
10 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
11 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1568364) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000001eb28 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
 9 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
10 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
11 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
12 0x000000000040331f main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
[uc2n515:1568361:0:1568361] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x154bc8200000)
[uc2n515:1568360:0:1568360] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x153fc8200000)
[uc2n515:1568362:0:1568362] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x1481f4200000)
[uc2n515:1568359:0:1568359] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x149f26200000)
==== backtrace (tid:1568361) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015ddb5 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000cb0a6 mca_btl_smcuda_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x000000000001eb28 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
11 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
12 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
13 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
14 0x000000000040331f main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1568360) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015ddb5 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000cb0a6 mca_btl_smcuda_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x000000000001eb28 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
11 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
12 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
13 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
14 0x000000000040331f main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1568362) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015ddb5 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000cb0a6 mca_btl_smcuda_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x000000000001eb28 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
11 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
12 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
13 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
14 0x000000000040331f main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1568359) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015ddb5 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000cb0a6 mca_btl_smcuda_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x000000000001eb28 MPIcuFFT_Slab_Opt1<double>::All2All_Sync()  ???:0
11 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
12 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
13 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
14 0x000000000040331f main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec noticed that process rank 5 with PID 1568364 on node uc2n515 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[17620,1],4]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1568635] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1568635] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[23499,1],2]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1568868] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1568868] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
The call to cuMemcpyAsync failed. This is a unrecoverable error and will
cause the program to abort.
  cuMemcpyAsync(0x146477000000, 0x146682c00000, 131072) returned value 1
Check the cuda.h file for what the return value means.
--------------------------------------------------------------------------
[uc2n515.localdomain:1568735] CUDA: Error in cuMemcpy: res=-1, dest=0x146477000000, src=0x146682c00000, size=131072
[uc2n515:1568735] *** Process received signal ***
[uc2n515:1568735] Signal: Aborted (6)
[uc2n515:1568735] Signal code:  (-6)
[uc2n515:1568735] [ 0] /usr/lib64/libpthread.so.0(+0x12dd0)[0x1468f4a53dd0]
[uc2n515:1568735] [ 1] /usr/lib64/libc.so.6(gsignal+0x10f)[0x1468f46b670f]
[uc2n515:1568735] [ 2] /usr/lib64/libc.so.6(abort+0x127)[0x1468f46a0b25]
[uc2n515:1568735] [ 3] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libopen-pal.so.40(+0x7c58f)[0x1468f362058f]
[uc2n515:1568735] [ 4] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libopen-pal.so.40(+0x77119)[0x1468f361b119]
[uc2n515:1568735] [ 5] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libmpi.so.40(ompi_datatype_sndrcv+0x50a)[0x146900459b2a]
[uc2n515:1568735] [ 6] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libmpi.so.40(ompi_coll_base_alltoallv_intra_basic_linear+0x26e)[0x1469004ae80e]
[uc2n515:1568735] [ 7] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libmpi.so.40(ompi_coll_tuned_alltoallv_intra_dec_fixed+0x42)[0x1469004ba702]
[uc2n515:1568735] [ 8] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libmpi.so.40(MPI_Alltoallv+0x1ab)[0x14690045c42b]
[uc2n515:1568735] [ 9] /home/st/st_us-051200/st_st160727/DistributedFFT/build/libslab_decomp.so(_ZN18MPIcuFFT_Slab_Opt1IdE12All2All_SyncEPvb+0xf8)[0x14690aba9b28]
[uc2n515:1568735] [10] /home/st/st_us-051200/st_st160727/DistributedFFT/build/libslab_decomp.so(_ZN18MPIcuFFT_Slab_Opt1IdE7execR2CEPvPKv+0x122)[0x14690aba9352]
[uc2n515:1568735] [11] /home/st/st_us-051200/st_st160727/DistributedFFT/build/libslab_tests.so(_ZN25Tests_Slab_Random_DefaultIdE9testcase0Eii+0x518)[0x14690adf61ae]
[uc2n515:1568735] [12] /home/st/st_us-051200/st_st160727/DistributedFFT/build/libslab_tests.so(_ZN25Tests_Slab_Random_DefaultIdE3runEiii+0x2f)[0x14690adf5c1d]
[uc2n515:1568735] [13] slab[0x40331f]
[uc2n515:1568735] [14] /usr/lib64/libc.so.6(__libc_start_main+0xf3)[0x1468f46a26a3]
[uc2n515:1568735] [15] slab[0x4039ee]
[uc2n515:1568735] *** End of error message ***
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec noticed that process rank 7 with PID 1568735 on node uc2n515 exited on signal 6 (Aborted).
--------------------------------------------------------------------------
[uc2n515.localdomain:1568720] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1568720] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[23365,1],7]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1569002] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1569002] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1569029] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1569029] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[23132,1],3]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1569267] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1569267] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515:1569041:0:1569041] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14ee1c300000)
[uc2n515:1569040:0:1569040] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x146a80280000)
[uc2n515:1569039:0:1569039] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x15062a200000)
[uc2n515:1569042:0:1569042] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14c134380000)
==== backtrace (tid:1569040) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x000000000008d803 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000001f2c9 MPIcuFFT_Slab_Opt1<double>::All2All_MPIType()  ???:0
 8 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
 9 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
10 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1569041) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x000000000008d803 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000001f2c9 MPIcuFFT_Slab_Opt1<double>::All2All_MPIType()  ???:0
 8 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
 9 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
10 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1569042) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x000000000008d803 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000001f2c9 MPIcuFFT_Slab_Opt1<double>::All2All_MPIType()  ???:0
 8 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
 9 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
10 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1569039) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x000000000008d803 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000001f2c9 MPIcuFFT_Slab_Opt1<double>::All2All_MPIType()  ???:0
 8 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
 9 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
10 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
11 0x000000000040331f main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515:1569036:0:1569036] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14e34c200000)
[uc2n515:1569038:0:1569038] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x1494c0200000)
[uc2n515:1569037:0:1569037] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x149ee4200000)
[uc2n515:1569035:0:1569035] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14686c200000)
==== backtrace (tid:1569036) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015ddb5 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000cb0a6 mca_btl_smcuda_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x0000000000121b20 mca_coll_basic_alltoallw_intra()  ???:0
 8 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 9 0x000000000001f2c9 MPIcuFFT_Slab_Opt1<double>::All2All_MPIType()  ???:0
10 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
11 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
12 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
13 0x000000000040331f main()  ???:0
14 0x00000000000236a3 __libc_start_main()  ???:0
15 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1569037) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015ddb5 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000cb0a6 mca_btl_smcuda_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x0000000000121b20 mca_coll_basic_alltoallw_intra()  ???:0
 8 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 9 0x000000000001f2c9 MPIcuFFT_Slab_Opt1<double>::All2All_MPIType()  ???:0
10 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
11 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
12 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
13 0x000000000040331f main()  ???:0
14 0x00000000000236a3 __libc_start_main()  ???:0
15 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1569038) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015ddb5 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000cb0a6 mca_btl_smcuda_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x0000000000121b20 mca_coll_basic_alltoallw_intra()  ???:0
 8 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 9 0x000000000001f2c9 MPIcuFFT_Slab_Opt1<double>::All2All_MPIType()  ???:0
10 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
11 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
12 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
13 0x000000000040331f main()  ???:0
14 0x00000000000236a3 __libc_start_main()  ???:0
15 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1569035) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015ddb5 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000cb0a6 mca_btl_smcuda_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x0000000000121b20 mca_coll_basic_alltoallw_intra()  ???:0
 8 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 9 0x000000000001f2c9 MPIcuFFT_Slab_Opt1<double>::All2All_MPIType()  ???:0
10 0x000000000001e352 MPIcuFFT_Slab_Opt1<double>::execR2C()  ???:0
11 0x00000000000171ae Tests_Slab_Random_Default<double>::testcase0()  ???:0
12 0x0000000000016c1d Tests_Slab_Random_Default<double>::run()  ???:0
13 0x000000000040331f main()  ???:0
14 0x00000000000236a3 __libc_start_main()  ???:0
15 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec noticed that process rank 6 with PID 1569041 on node uc2n515 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[22958,1],7]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1569281] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1569281] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[23036,1],4]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1569363] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1569363] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Starting computation for size 128
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 11:35:20.096911
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 11:35:27.134982
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 11:35:33.019535
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 11:35:38.212763
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 11:35:44.065186
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 11:35:49.336095
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 11:35:56.966942
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 11:36:02.183713
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 11:36:08.326158
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 11:36:13.601217
b''

Starting computation for size [128, 128, 256]
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 11:36:21.342088
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 11:36:26.566944
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 11:36:32.469076
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 11:36:37.824792
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 11:36:43.843919
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 11:36:49.189993
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 11:36:58.225103
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 11:37:03.462188
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 11:37:09.408028
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 11:37:14.674686
b''

Starting computation for size [128, 256, 256]
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 11:37:24.498033
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 11:37:29.821057
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 11:37:35.843407
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 11:37:41.197438
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 11:37:47.170309
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 11:37:52.434810
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 11:38:04.679362
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 11:38:10.109310
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 11:38:16.109430
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 11:38:21.419361
b''

Starting computation for size 256
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 11:38:34.444478
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 11:38:40.021305
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 11:38:46.062019
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 11:38:51.538945
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 11:38:57.476853
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 11:39:02.981116
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 11:39:15.396096
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 11:39:21.033837
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 11:39:27.148217
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 11:39:32.734724
b''

Starting computation for size [256, 256, 512]
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 11:39:51.186080
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 11:39:57.046786
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 11:40:03.238175
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 11:40:09.020778
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 11:40:15.357163
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 11:40:21.317495
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 11:40:39.552548
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 11:40:45.569748
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 11:40:51.848238
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 11:40:57.980873
b''

Starting computation for size [256, 512, 512]
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 11:41:18.870839
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 11:41:25.628064
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 11:41:32.290068
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 11:41:38.897414
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 11:41:45.489195
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 11:41:52.500490
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 11:42:23.507442
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 11:42:30.443797
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 11:42:37.131002
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 11:42:44.304267
b''

Starting computation for size 512
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 11:43:19.180504
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 11:43:27.524656
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 11:43:34.948053
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 11:43:42.984196
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 11:43:50.650282
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 11:43:59.539910
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 11:44:31.732571
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 11:44:40.343227
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 11:44:47.851941
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 11:44:57.018385
b''

Starting computation for size [512, 512, 1024]
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 11:45:33.080280
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 11:45:44.824842
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 11:45:53.847462
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 11:46:05.041465
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 11:46:14.385941
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 11:46:26.936174
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 11:47:25.291940
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 11:47:37.630714
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 11:47:46.855724
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 11:48:00.326703
b''

Starting computation for size [512, 1024, 1024]
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 11:49:08.135024
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 11:49:26.309651
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 11:49:38.549698
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 11:49:55.448815
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 11:50:08.768129
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 11:50:29.501909
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 11:52:19.360250
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 11:52:39.023643
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 11:52:51.391133
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 11:53:13.351765
b''

Starting computation for size 1024
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 11:55:22.970911
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 11:55:53.996680
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 11:56:12.697910
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 11:56:41.607923
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 11:57:01.732480
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 11:57:36.950213
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 11:59:28.884684
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 12:00:02.353286
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 12:00:20.927690
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 12:00:58.633531
b''

Starting computation for size [1024, 1024, 2048]
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 12:03:09.886072
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 12:04:07.931090
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 12:04:39.684171
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 12:05:32.821946
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 12:06:07.784966
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 12:07:12.553691
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 12:10:50.705773
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 12:11:53.910301
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 12:12:25.606110
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 12:13:35.751717
b''

Starting computation for size [1024, 2048, 2048]
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 12:17:52.709887
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 12:19:42.375549
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 12:20:39.022605
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 12:22:17.562835
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 12:23:21.241589
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 12:25:26.999378
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 12:32:41.310537
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 12:32:54.139314
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 12:33:05.739644
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 12:33:19.040200
b''

Starting computation for size 2048
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 12:33:32.477361
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 12:33:37.179233
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 12:33:41.902332
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 12:33:46.662416
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 12:33:51.477251
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 12:33:56.307202
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 12:34:00.915734
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 12:34:05.491220
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 12:34:10.196956
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 12:34:14.832621
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
The call to cuMemcpyAsync failed. This is a unrecoverable error and will
cause the program to abort.
  cuMemcpyAsync(0x8f5a820, 0x14f142800000, 65536) returned value 1
Check the cuda.h file for what the return value means.
--------------------------------------------------------------------------
[uc2n515.localdomain:1569376] CUDA: Error in cuMemcpy: res=-1, dest=0x8f5a820, src=0x14f142800000, size=65536
[uc2n515:1569376] *** Process received signal ***
[uc2n515:1569376] Signal: Aborted (6)
[uc2n515:1569376] Signal code:  (-6)
[uc2n515:1569376] [ 0] /usr/lib64/libpthread.so.0(+0x12dd0)[0x14f3b4880dd0]
[uc2n515:1569376] [ 1] /usr/lib64/libc.so.6(gsignal+0x10f)[0x14f3b44e370f]
[uc2n515:1569376] [ 2] /usr/lib64/libc.so.6(abort+0x127)[0x14f3b44cdb25]
[uc2n515:1569376] [ 3] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libopen-pal.so.40(+0x7c375)[0x14f3b344d375]
[uc2n515:1569376] [ 4] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libopen-pal.so.40(opal_convertor_pack+0xd8)[0x14f3b34441e8]
[uc2n515:1569376] [ 5] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libmpi.so.40(ompi_datatype_sndrcv+0x1e3)[0x14f3c0286803]
[uc2n515:1569376] [ 6] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libmpi.so.40(mca_coll_basic_alltoallw_intra+0xa4)[0x14f3c031a984]
[uc2n515:1569376] [ 7] /opt/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libmpi.so.40(MPI_Alltoallw+0x1eb)[0x14f3c0289ccb]
[uc2n515:1569376] [ 8] /home/st/st_us-051200/st_st160727/DistributedFFT/build/libslab_decomp.so(_ZN18MPIcuFFT_Slab_Opt1IdE15All2All_MPITypeEPvb+0x149)[0x14f3ca9d72c9]
[uc2n515:1569376] [ 9] /home/st/st_us-051200/st_st160727/DistributedFFT/build/libslab_decomp.so(_ZN18MPIcuFFT_Slab_Opt1IdE7execR2CEPvPKv+0x122)[0x14f3ca9d6352]
[uc2n515:1569376] [10] /home/st/st_us-051200/st_st160727/DistributedFFT/build/libslab_tests.so(_ZN25Tests_Slab_Random_DefaultIdE9testcase0Eii+0x518)[0x14f3cac231ae]
[uc2n515:1569376] [11] /home/st/st_us-051200/st_st160727/DistributedFFT/build/libslab_tests.so(_ZN25Tests_Slab_Random_DefaultIdE3runEiii+0x2f)[0x14f3cac22c1d]
[uc2n515:1569376] [12] slab[0x40331f]
[uc2n515:1569376] [13] /usr/lib64/libc.so.6(__libc_start_main+0xf3)[0x14f3b44cf6a3]
[uc2n515:1569376] [14] slab[0x4039ee]
[uc2n515:1569376] *** End of error message ***
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec noticed that process rank 7 with PID 1569376 on node uc2n515 exited on signal 6 (Aborted).
--------------------------------------------------------------------------
[uc2n515.localdomain:1569360] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1569360] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[22721,1],5]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1569646] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1569646] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[24566,1],4]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1569881] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1569881] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[24265,1],7]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1570150] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1570150] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[24005,1],3]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1570410] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1570410] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[23748,1],6]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1570667] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1570667] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[21443,1],5]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1570924] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1570924] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[21215,1],3]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1571184] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1571184] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[20956,1],6]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1571443] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1571443] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[20699,1],6]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1571700] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1571700] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[22487,1],4]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1571960] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1571960] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1572217] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1572217] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1572495] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1572495] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1572750] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1572750] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1573034] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1573034] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1573307] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1573307] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1573581] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1573581] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1573840] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1573840] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1574098] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1574098] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1574355] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1574355] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1574615] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1574615] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1574884] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1574884] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1575140] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1575140] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1575401] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1575401] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1575682] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1575682] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1575966] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1575966] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1576224] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1576224] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1576483] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1576483] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1576740] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1576740] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1576999] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1576999] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1577274] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1577274] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1577536] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1577536] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1577793] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1577793] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1578065] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1578065] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1578347] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1578347] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1578623] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1578623] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1578884] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1578884] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1579154] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1579154] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1579414] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1579414] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1579672] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1579672] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1579946] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1579946] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1580206] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1580206] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1580488] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1580488] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1580766] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1580766] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1581073] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1581073] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1581367] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1581367] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1581647] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1581647] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1581932] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1581932] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1582209] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1582209] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1582484] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1582484] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1582768] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1582768] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1583051] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1583051] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
mpiexec has exited due to process rank 6 with PID 1583063 on
node uc2n515 exiting improperly. There are three reasons this could occur:

1. this process did not call "init" before exiting, but others in
the job did. This can cause a job to hang indefinitely while it waits
for all processes to call "init". By rule, if one process calls "init",
then ALL processes must call "init" prior to termination.

2. this process called "init", but exited without calling "finalize".
By rule, all processes that call "init" MUST call "finalize" prior to
exiting or it will be considered an "abnormal termination"

3. this process called "MPI_Abort" or "orte_abort" and the mca parameter
orte_create_session_dirs is set to false. In this case, the run-time cannot
detect that the abort call was an abnormal termination. Hence, the only
error message you will receive is this one.

This may have caused other processes in the application to be
terminated by signals sent by mpiexec (as reported here).

You can avoid this message by specifying -quiet on the mpiexec command line.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1583308] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1583308] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
mpiexec has exited due to process rank 5 with PID 1583319 on
node uc2n515 exiting improperly. There are three reasons this could occur:

1. this process did not call "init" before exiting, but others in
the job did. This can cause a job to hang indefinitely while it waits
for all processes to call "init". By rule, if one process calls "init",
then ALL processes must call "init" prior to termination.

2. this process called "init", but exited without calling "finalize".
By rule, all processes that call "init" MUST call "finalize" prior to
exiting or it will be considered an "abnormal termination"

3. this process called "MPI_Abort" or "orte_abort" and the mca parameter
orte_create_session_dirs is set to false. In this case, the run-time cannot
detect that the abort call was an abnormal termination. Hence, the only
error message you will receive is this one.

This may have caused other processes in the application to be
terminated by signals sent by mpiexec (as reported here).

You can avoid this message by specifying -quiet on the mpiexec command line.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1583580] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1583580] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
mpiexec has exited due to process rank 7 with PID 1583593 on
node uc2n515 exiting improperly. There are three reasons this could occur:

1. this process did not call "init" before exiting, but others in
the job did. This can cause a job to hang indefinitely while it waits
for all processes to call "init". By rule, if one process calls "init",
then ALL processes must call "init" prior to termination.

2. this process called "init", but exited without calling "finalize".
By rule, all processes that call "init" MUST call "finalize" prior to
exiting or it will be considered an "abnormal termination"

3. this process called "MPI_Abort" or "orte_abort" and the mca parameter
orte_create_session_dirs is set to false. In this case, the run-time cannot
detect that the abort call was an abnormal termination. Hence, the only
error message you will receive is this one.

This may have caused other processes in the application to be
terminated by signals sent by mpiexec (as reported here).

You can avoid this message by specifying -quiet on the mpiexec command line.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1583845] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1583845] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
mpiexec has exited due to process rank 7 with PID 1583859 on
node uc2n515 exiting improperly. There are three reasons this could occur:

1. this process did not call "init" before exiting, but others in
the job did. This can cause a job to hang indefinitely while it waits
for all processes to call "init". By rule, if one process calls "init",
then ALL processes must call "init" prior to termination.

2. this process called "init", but exited without calling "finalize".
By rule, all processes that call "init" MUST call "finalize" prior to
exiting or it will be considered an "abnormal termination"

3. this process called "MPI_Abort" or "orte_abort" and the mca parameter
orte_create_session_dirs is set to false. In this case, the run-time cannot
detect that the abort call was an abnormal termination. Hence, the only
error message you will receive is this one.

This may have caused other processes in the application to be
terminated by signals sent by mpiexec (as reported here).

You can avoid this message by specifying -quiet on the mpiexec command line.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1584123] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1584123] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
mpiexec has exited due to process rank 5 with PID 1584134 on
node uc2n515 exiting improperly. There are three reasons this could occur:

1. this process did not call "init" before exiting, but others in
the job did. This can cause a job to hang indefinitely while it waits
for all processes to call "init". By rule, if one process calls "init",
then ALL processes must call "init" prior to termination.

2. this process called "init", but exited without calling "finalize".
By rule, all processes that call "init" MUST call "finalize" prior to
exiting or it will be considered an "abnormal termination"

3. this process called "MPI_Abort" or "orte_abort" and the mca parameter
orte_create_session_dirs is set to false. In this case, the run-time cannot
detect that the abort call was an abnormal termination. Hence, the only
error message you will receive is this one.

This may have caused other processes in the application to be
terminated by signals sent by mpiexec (as reported here).

You can avoid this message by specifying -quiet on the mpiexec command line.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1584390] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1584390] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
mpiexec has exited due to process rank 5 with PID 1584401 on
node uc2n515 exiting improperly. There are three reasons this could occur:

1. this process did not call "init" before exiting, but others in
the job did. This can cause a job to hang indefinitely while it waits
for all processes to call "init". By rule, if one process calls "init",
then ALL processes must call "init" prior to termination.

2. this process called "init", but exited without calling "finalize".
By rule, all processes that call "init" MUST call "finalize" prior to
exiting or it will be considered an "abnormal termination"

3. this process called "MPI_Abort" or "orte_abort" and the mca parameter
orte_create_session_dirs is set to false. In this case, the run-time cannot
detect that the abort call was an abnormal termination. Hence, the only
error message you will receive is this one.

This may have caused other processes in the application to be
terminated by signals sent by mpiexec (as reported here).

You can avoid this message by specifying -quiet on the mpiexec command line.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1584665] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1584665] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
mpiexec has exited due to process rank 4 with PID 1584675 on
node uc2n515 exiting improperly. There are three reasons this could occur:

1. this process did not call "init" before exiting, but others in
the job did. This can cause a job to hang indefinitely while it waits
for all processes to call "init". By rule, if one process calls "init",
then ALL processes must call "init" prior to termination.

2. this process called "init", but exited without calling "finalize".
By rule, all processes that call "init" MUST call "finalize" prior to
exiting or it will be considered an "abnormal termination"

3. this process called "MPI_Abort" or "orte_abort" and the mca parameter
orte_create_session_dirs is set to false. In this case, the run-time cannot
detect that the abort call was an abnormal termination. Hence, the only
error message you will receive is this one.

This may have caused other processes in the application to be
terminated by signals sent by mpiexec (as reported here).

You can avoid this message by specifying -quiet on the mpiexec command line.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1584927] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1584927] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
mpiexec has exited due to process rank 5 with PID 1584938 on
node uc2n515 exiting improperly. There are three reasons this could occur:

1. this process did not call "init" before exiting, but others in
the job did. This can cause a job to hang indefinitely while it waits
for all processes to call "init". By rule, if one process calls "init",
then ALL processes must call "init" prior to termination.

2. this process called "init", but exited without calling "finalize".
By rule, all processes that call "init" MUST call "finalize" prior to
exiting or it will be considered an "abnormal termination"

3. this process called "MPI_Abort" or "orte_abort" and the mca parameter
orte_create_session_dirs is set to false. In this case, the run-time cannot
detect that the abort call was an abnormal termination. Hence, the only
error message you will receive is this one.

This may have caused other processes in the application to be
terminated by signals sent by mpiexec (as reported here).

You can avoid this message by specifying -quiet on the mpiexec command line.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1585202] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1585202] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
mpiexec has exited due to process rank 5 with PID 1585213 on
node uc2n515 exiting improperly. There are three reasons this could occur:

1. this process did not call "init" before exiting, but others in
the job did. This can cause a job to hang indefinitely while it waits
for all processes to call "init". By rule, if one process calls "init",
then ALL processes must call "init" prior to termination.

2. this process called "init", but exited without calling "finalize".
By rule, all processes that call "init" MUST call "finalize" prior to
exiting or it will be considered an "abnormal termination"

3. this process called "MPI_Abort" or "orte_abort" and the mca parameter
orte_create_session_dirs is set to false. In this case, the run-time cannot
detect that the abort call was an abnormal termination. Hence, the only
error message you will receive is this one.

This may have caused other processes in the application to be
terminated by signals sent by mpiexec (as reported here).

You can avoid this message by specifying -quiet on the mpiexec command line.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1585464] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1585464] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
mpiexec has exited due to process rank 6 with PID 1585476 on
node uc2n515 exiting improperly. There are three reasons this could occur:

1. this process did not call "init" before exiting, but others in
the job did. This can cause a job to hang indefinitely while it waits
for all processes to call "init". By rule, if one process calls "init",
then ALL processes must call "init" prior to termination.

2. this process called "init", but exited without calling "finalize".
By rule, all processes that call "init" MUST call "finalize" prior to
exiting or it will be considered an "abnormal termination"

3. this process called "MPI_Abort" or "orte_abort" and the mca parameter
orte_create_session_dirs is set to false. In this case, the run-time cannot
detect that the abort call was an abnormal termination. Hence, the only
error message you will receive is this one.

This may have caused other processes in the application to be
terminated by signals sent by mpiexec (as reported here).

You can avoid this message by specifying -quiet on the mpiexec command line.
--------------------------------------------------------------------------
Starting computation for size 128
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 128
2021-09-19 11:35:20.096801
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 128
2021-09-19 11:35:29.786160
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 128
2021-09-19 11:35:35.983638
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 128
2021-09-19 11:35:41.578699
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 128
2021-09-19 11:35:47.810718
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 128
2021-09-19 11:35:53.445716
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 128
2021-09-19 11:36:01.260048
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 128
2021-09-19 11:36:06.919600
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 128
2021-09-19 11:36:13.166911
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 128
2021-09-19 11:36:18.836779
b''

Starting computation for size [128, 128, 256]
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 256
2021-09-19 11:36:26.989334
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 256
2021-09-19 11:36:32.574893
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 256
2021-09-19 11:36:38.802770
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 256
2021-09-19 11:36:44.797262
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 256
2021-09-19 11:36:51.513222
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 256
2021-09-19 11:36:57.574791
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 256
2021-09-19 11:37:07.673579
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 256
2021-09-19 11:37:13.664486
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 256
2021-09-19 11:37:20.485164
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 256
2021-09-19 11:37:26.474797
b''

Starting computation for size [128, 256, 256]
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 256 -nz 256
2021-09-19 11:37:36.727341
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 256 -nz 256
2021-09-19 11:37:42.984738
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 256 -nz 256
2021-09-19 11:37:49.790128
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 256 -nz 256
2021-09-19 11:37:55.854699
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 256 -nz 256
2021-09-19 11:38:02.715639
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 256 -nz 256
2021-09-19 11:38:09.010910
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 256 -nz 256
2021-09-19 11:38:21.797167
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 256 -nz 256
2021-09-19 11:38:27.970138
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 256 -nz 256
2021-09-19 11:38:34.713539
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 256 -nz 256
2021-09-19 11:38:40.844595
b''

Starting computation for size 256
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 256
2021-09-19 11:38:54.013215
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 256
2021-09-19 11:39:00.243236
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 256
2021-09-19 11:39:07.070985
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 256
2021-09-19 11:39:13.353025
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 256
2021-09-19 11:39:20.366759
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 256
2021-09-19 11:39:26.758138
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 256
2021-09-19 11:39:40.191025
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 256
2021-09-19 11:39:46.460258
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 256
2021-09-19 11:39:53.406938
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 256
2021-09-19 11:39:59.814433
b''

Starting computation for size [256, 256, 512]
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 512
2021-09-19 11:40:13.598689
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 512
2021-09-19 11:40:20.354477
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 512
2021-09-19 11:40:27.409571
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 512
2021-09-19 11:40:33.950543
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 512
2021-09-19 11:40:41.133261
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 512
2021-09-19 11:40:48.029597
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 512
2021-09-19 11:41:06.899544
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 512
2021-09-19 11:41:13.924359
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 512
2021-09-19 11:41:21.039108
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 512
2021-09-19 11:41:28.032837
b''

Starting computation for size [256, 512, 512]
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 512 -nz 512
2021-09-19 11:41:49.328924
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 512 -nz 512
2021-09-19 11:41:56.995350
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 512 -nz 512
2021-09-19 11:42:04.527304
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 512 -nz 512
2021-09-19 11:42:11.880854
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 512 -nz 512
2021-09-19 11:42:19.280921
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 512 -nz 512
2021-09-19 11:42:27.117738
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 512 -nz 512
2021-09-19 11:42:58.931650
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 512 -nz 512
2021-09-19 11:43:06.714740
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 512 -nz 512
2021-09-19 11:43:14.109187
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 512 -nz 512
2021-09-19 11:43:22.019768
b''

Starting computation for size 512
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 512
2021-09-19 11:43:56.520201
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 512
2021-09-19 11:44:05.693634
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 512
2021-09-19 11:44:13.955761
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 512
2021-09-19 11:44:22.662545
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 512
2021-09-19 11:44:31.026182
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 512
2021-09-19 11:44:40.519356
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 512
2021-09-19 11:45:14.256622
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 512
2021-09-19 11:45:23.834004
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 512
2021-09-19 11:45:32.335755
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 512
2021-09-19 11:45:42.281823
b''

Starting computation for size [512, 512, 1024]
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-19 11:46:19.632252
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-19 11:46:32.476717
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-19 11:46:42.434521
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-19 11:46:54.044195
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-19 11:47:04.111782
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-19 11:47:17.429444
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-19 11:48:15.497489
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-19 11:48:28.896388
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-19 11:48:38.960997
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-19 11:48:52.898883
b''

Starting computation for size [512, 1024, 1024]
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-19 11:49:58.979604
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-19 11:50:18.272509
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-19 11:50:31.967083
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-19 11:50:49.207583
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-19 11:51:02.797583
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-19 11:51:23.803930
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-19 11:53:11.292066
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-19 11:53:31.937329
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-19 11:53:45.227210
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-19 11:54:07.368142
b''

Starting computation for size 1024
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-19 11:56:06.236791
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-19 11:56:39.696916
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-19 11:56:59.791982
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-19 11:57:29.039196
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-19 11:57:49.183472
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-19 11:58:23.855524
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-19 12:00:18.823090
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-19 12:00:53.966084
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-19 12:01:14.460862
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-19 12:01:51.793871
b''

Starting computation for size [1024, 1024, 2048]
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-19 12:04:01.791293
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-19 12:05:03.406865
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-19 12:05:36.506362
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-19 12:06:28.625440
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-19 12:07:02.941681
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-19 12:08:08.309713
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-19 12:11:50.849249
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-19 12:12:55.050857
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-19 12:13:28.944519
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-19 12:14:39.101224
b''

Starting computation for size [1024, 2048, 2048]
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-19 12:18:51.986027
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-19 12:20:47.514406
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-19 12:21:46.461897
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-19 12:23:22.824766
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-19 12:24:24.096880
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-19 12:26:27.201073
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-19 12:33:41.838717
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-19 12:33:54.067671
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-19 12:34:03.506062
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-19 12:34:14.730520
b''

Starting computation for size 2048
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-19 12:34:25.416925
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-19 12:34:28.732946
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-19 12:34:32.352710
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-19 12:34:35.784944
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-19 12:34:39.182929
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-19 12:34:42.671784
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-19 12:34:46.110337
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-19 12:34:49.685093
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-19 12:34:53.267788
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-19 12:34:56.776990
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

Starting computation for size 128
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 12:35:00.310743
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 12:35:05.845432
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 12:35:12.415575
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 12:35:18.704894
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 12:35:25.420763
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 12:35:31.864292
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 12:35:38.710508
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 12:35:45.290945
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 12:35:52.072875
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 12:35:58.523329
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

Starting computation for size 256
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 12:36:05.472234
b'Result (avg): 5.19278e-09\nResult (max): 5.37366e-08\n'

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 12:36:12.202101
b'Result (avg): 5.19278e-09\nResult (max): 5.37366e-08\n'

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 12:36:19.268656
b'Result (avg): 5.19278e-09\nResult (max): 5.37366e-08\n'

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 12:36:25.884636
b'Result (avg): 5.19278e-09\nResult (max): 5.37366e-08\n'

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 12:36:32.902962
b'Result (avg): 5.19278e-09\nResult (max): 5.37366e-08\n'

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 12:36:39.340184
b'Result (avg): 5.19278e-09\nResult (max): 5.37366e-08\n'

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 12:36:46.729269
b'Result (avg): 5.19278e-09\nResult (max): 5.37366e-08\n'

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 12:36:53.394903
b'Result (avg): 5.19278e-09\nResult (max): 5.37366e-08\n'

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 12:37:00.121104
b'Result (avg): 5.19278e-09\nResult (max): 5.37366e-08\n'

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 12:37:06.732847
b'Result (avg): 5.19278e-09\nResult (max): 5.37366e-08\n'

Starting computation for size 512
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 12:37:14.213379
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 12:37:23.337442
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 12:37:32.330015
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 12:37:41.426203
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 12:37:50.644301
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 12:37:59.730154
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 12:38:10.158220
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 12:38:19.120319
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 12:38:28.061946
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 12:38:37.235486
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

Starting computation for size 1024
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 12:38:48.292246
b'Result (avg): 7.69766e-07\nResult (max): 9.22813e-06\n'

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 12:39:17.731062
b'Result (avg): 7.69766e-07\nResult (max): 9.22813e-06\n'

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 12:39:45.332918
b'Result (avg): 7.69766e-07\nResult (max): 9.22813e-06\n'

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 12:40:14.773762
b'Result (avg): 7.69766e-07\nResult (max): 9.22813e-06\n'

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 12:40:42.509668
b'Result (avg): 7.69766e-07\nResult (max): 9.22813e-06\n'

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 12:41:12.289421
b'Result (avg): 7.69766e-07\nResult (max): 9.22813e-06\n'

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 12:41:46.074673
b'Result (avg): 7.69766e-07\nResult (max): 9.22813e-06\n'

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 12:42:15.496032
b'Result (avg): 7.69766e-07\nResult (max): 9.22813e-06\n'

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 12:42:43.098985
b'Result (avg): 7.69766e-07\nResult (max): 9.22813e-06\n'

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 12:43:13.046593
b'Result (avg): 7.69766e-07\nResult (max): 9.22813e-06\n'

Starting computation for size 2048
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 12:43:48.406431
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\n'

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 12:43:58.699771
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\n'

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 12:44:12.756772
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\n'

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 12:44:26.901342
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\n'

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 12:44:40.892965
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\n'

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 12:44:54.943780
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\n'

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 12:45:09.084818
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\n'

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 12:45:23.134368
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\n'

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 12:45:37.077876
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\n'

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_1 --rankfile ../mpi/rankfile_1 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/forward --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 12:45:51.096912
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/tests/src/slab/random_dist_default.cu:666\n'

Slab 1D->2D default
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1585745] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1585745] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1585744] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1585744] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1585906] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1585906] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1586027] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1586027] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1586174] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1586174] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1586300] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1586300] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1586390] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1586390] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1586911] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1586911] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1586821] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1586821] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1587347] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1587347] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1587366] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1587366] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1587783] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1587783] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1587624] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1587624] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1587893] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1587893] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1587912] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1587912] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1588167] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1588167] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1588186] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1588186] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1588358] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1588358] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1588469] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1588469] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1588727] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1588727] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1588744] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1588744] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1588997] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1588997] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1589017] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1589017] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1589034] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1589034] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1589290] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1589290] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1589307] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1589307] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1589559] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1589559] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1589590] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1589590] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1589847] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1589847] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1590113] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1590113] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1590614] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1590614] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1590631] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1590631] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1590885] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1590885] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1590902] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1590902] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1591158] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1591158] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1591188] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1591188] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1591171] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1591171] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1591458] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1591458] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1591709] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1591709] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1591726] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1591726] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1591882] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1591882] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1592154] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1592154] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1592001] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1592001] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1592270] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1592270] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1592287] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1592287] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1592552] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1592552] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1592572] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1592572] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1592734] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1592734] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1592842] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1592842] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1593440] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1593440] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1593350] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1593350] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1593866] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1593866] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1593883] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1593883] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1594136] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1594136] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1594163] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1594163] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1594429] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1594429] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1594448] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1594448] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1594703] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1594703] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1594719] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1594719] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1594974] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1594974] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1594992] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1594992] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1595245] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1595245] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1595276] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1595276] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1595513] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1595513] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1595545] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1595545] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1595565] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1595565] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1595655] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1595655] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1595839] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1595839] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1596095] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1596095] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1596610] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1596610] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1596627] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1596627] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1596889] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1596889] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1597148] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1597148] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1597404] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1597404] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1597421] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1597421] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1597677] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1597677] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1597713] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1597713] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1597967] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1597967] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1597986] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1597986] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1598241] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1598241] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1598258] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1598258] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1598506] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1598506] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1598531] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1598531] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1598688] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1598688] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1598814] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1598814] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1598827] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1598827] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1598849] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1598849] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1599109] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1599109] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1599362] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1599362] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1599393] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1599393] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1599645] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1599645] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1599911] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1599911] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1600156] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1600156] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1600432] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1600432] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1600550] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1600550] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1600706] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1600706] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1600979] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1600979] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1600998] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1600998] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1601145] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1601145] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1601271] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1601271] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1601389] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1601389] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1601544] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1601544] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1601634] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1601634] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1601828] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1601828] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1601943] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1601943] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1602095] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1602095] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1602119] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1602119] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1602391] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1602391] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1602652] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1602652] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1602898] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1602898] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1603176] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1603176] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1603404] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1603404] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1603694] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1603694] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1603809] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1603809] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1603981] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1603981] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1604071] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1604071] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1604275] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1604275] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1604392] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1604392] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1604544] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1604544] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1604711] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1604711] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1604819] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1604819] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1604982] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1604982] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1605089] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1605089] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1605245] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1605245] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1605371] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1605371] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1605386] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1605386] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1605429] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1605429] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1605688] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1605688] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1605948] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1605948] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1606456] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1606456] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1606974] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1606974] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1607228] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1607228] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1607247] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1607247] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1607503] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1607503] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1607518] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1607518] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1607546] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1607546] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1607593] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1607593] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1607851] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1607851] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1608109] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1608109] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1608376] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1608376] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1608394] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1608394] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1608651] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1608651] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1608667] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1608667] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1608695] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1608695] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1608713] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1608713] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1608745] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1608745] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1608767] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1608767] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1609027] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1609027] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1609299] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1609299] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1609809] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1609809] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1610317] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1610317] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1610585] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1610585] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1610607] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1610607] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1610774] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1610774] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1610878] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1610878] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1610910] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1610910] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1610950] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1610950] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1611214] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1611214] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1611480] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1611480] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1611740] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1611740] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1611758] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1611758] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1612058] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1612058] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1612094] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1612094] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1612113] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1612113] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1612143] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1612143] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1612165] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1612165] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1612432] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1612432] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1612451] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1612451] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1612712] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1612712] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1613239] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1613239] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1613750] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1613750] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1614030] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1614030] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1614306] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1614306] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1614340] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1614340] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1614360] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1614360] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1614391] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1614391] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1614431] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1614431] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1614696] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1614696] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1614973] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1614973] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1615253] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1615253] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1615533] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1615533] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1615577] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1615577] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1615609] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1615609] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1615645] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1615645] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1615663] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1615663] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1615923] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1615923] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1615962] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1615962] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1616239] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1616239] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1616256] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1616256] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1616784] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1616784] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1617298] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1617298] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1617597] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1617597] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1617933] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1617933] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1617989] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1617989] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1618010] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1618010] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1618057] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1618057] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1618079] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1618079] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1618367] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1618367] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1618644] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1618644] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1618925] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1618925] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1619284] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1619284] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1619345] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1619345] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1619389] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1619389] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1619425] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1619425] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1619703] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1619703] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1619749] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1619749] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1620005] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1620005] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1620048] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1620048] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1620519] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1620519] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1620615] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1620615] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1621147] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1621147] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1621470] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1621470] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1621801] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1621801] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1621866] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1621866] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1621904] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1621904] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1621945] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1621945] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1622242] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1622242] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1622267] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1622267] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1622570] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1622570] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1622875] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1622875] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1623233] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1623233] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1623342] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1623342] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1623381] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1623381] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1623667] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1623667] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1623753] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1623753] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1624016] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1624016] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1624071] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1624071] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1624375] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1624375] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1624675] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1624675] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1625140] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1625140] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1625259] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1625259] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1625627] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1625627] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1626117] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1626117] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n517:1237871:0:1237871] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14ae30400000)
[uc2n517:1237873:0:1237873] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x146c7c400000)
[uc2n517:1237872:0:1237872] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x147314400000)
[uc2n517:1237874:0:1237874] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14a784400000)
==== backtrace (tid:1237873) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000002cf0e MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
 9 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
10 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
11 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
12 0x0000000000403591 main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1237872) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000002cf0e MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
 9 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
10 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
11 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
12 0x0000000000403591 main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1237874) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000002cf0e MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
 9 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
10 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
11 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
12 0x0000000000403591 main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1237871) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000002cf0e MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
 9 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
10 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
11 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
12 0x0000000000403591 main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec noticed that process rank 4 with PID 1237871 on node uc2n517 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n517:1238180:0:1238180] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14bdbd000000)
==== backtrace (tid:1238180) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015ddb5 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000cb0a6 mca_btl_smcuda_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x000000000002cf0e MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
11 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
12 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
13 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
14 0x0000000000403591 main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
[uc2n517:1238185:0:1238185] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x1496fe000010)
==== backtrace (tid:1238185) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x00000000001d5e80 cuMemGetAttribute()  ???:0
 3 0x00000000002ffe53 cudbgGetAPI()  ???:0
 4 0x00000000004359a6 cudbgApiInit()  ???:0
 5 0x000000000018d730 ???()  /lib64/libcuda.so.1:0
 6 0x000000000018df04 ???()  /lib64/libcuda.so.1:0
 7 0x00000000001904d9 ???()  /lib64/libcuda.so.1:0
 8 0x00000000001fe77e cuGraphAddMemAllocNode()  ???:0
 9 0x00000000000a2944 mca_common_cuda_cu_memcpy()  common_cuda.c:0
10 0x000000000007c563 opal_cuda_memcpy_sync()  ???:0
11 0x0000000000077119 non_overlap_cuda_copy_content_same_ddt()  opal_datatype_copy.c:0
12 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
13 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
14 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
15 0x000000000009042b PMPI_Alltoallv()  ???:0
16 0x000000000002cf0e MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
17 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
18 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
19 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
20 0x0000000000403591 main()  ???:0
21 0x00000000000236a3 __libc_start_main()  ???:0
22 0x00000000004039ee _start()  ???:0
=================================
[uc2n517:1238184:0:1238184] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x151a96000010)
[uc2n517:1238187:0:1238187] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x146782000010)
[uc2n517:1238186:0:1238186] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x1535fe000010)
==== backtrace (tid:1238184) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x00000000001d5e80 cuMemGetAttribute()  ???:0
 3 0x00000000002ffe53 cudbgGetAPI()  ???:0
 4 0x00000000004359a6 cudbgApiInit()  ???:0
 5 0x000000000018d730 ???()  /lib64/libcuda.so.1:0
 6 0x000000000018df04 ???()  /lib64/libcuda.so.1:0
 7 0x00000000001904d9 ???()  /lib64/libcuda.so.1:0
 8 0x00000000001fe77e cuGraphAddMemAllocNode()  ???:0
 9 0x00000000000a2944 mca_common_cuda_cu_memcpy()  common_cuda.c:0
10 0x000000000007c563 opal_cuda_memcpy_sync()  ???:0
11 0x0000000000077119 non_overlap_cuda_copy_content_same_ddt()  opal_datatype_copy.c:0
12 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
13 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
14 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
15 0x000000000009042b PMPI_Alltoallv()  ???:0
16 0x000000000002cf0e MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
17 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
18 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
19 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
20 0x0000000000403591 main()  ???:0
21 0x00000000000236a3 __libc_start_main()  ???:0
22 0x00000000004039ee _start()  ???:0
=================================
[uc2n517:1238181:0:1238181] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x148900000000)
[uc2n517:1238183:0:1238183] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x148a26000000)
[uc2n517:1238182:0:1238182] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x152a1e000000)
==== backtrace (tid:1238187) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x00000000001d5e80 cuMemGetAttribute()  ???:0
 3 0x00000000002ffe53 cudbgGetAPI()  ???:0
 4 0x00000000004359a6 cudbgApiInit()  ???:0
 5 0x000000000018d730 ???()  /lib64/libcuda.so.1:0
 6 0x000000000018df04 ???()  /lib64/libcuda.so.1:0
 7 0x00000000001904d9 ???()  /lib64/libcuda.so.1:0
 8 0x00000000001fe77e cuGraphAddMemAllocNode()  ???:0
 9 0x00000000000a2944 mca_common_cuda_cu_memcpy()  common_cuda.c:0
10 0x000000000007c563 opal_cuda_memcpy_sync()  ???:0
11 0x0000000000077119 non_overlap_cuda_copy_content_same_ddt()  opal_datatype_copy.c:0
12 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
13 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
14 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
15 0x000000000009042b PMPI_Alltoallv()  ???:0
16 0x000000000002cf0e MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
17 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
18 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
19 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
20 0x0000000000403591 main()  ???:0
21 0x00000000000236a3 __libc_start_main()  ???:0
22 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1238186) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x00000000001d5e80 cuMemGetAttribute()  ???:0
 3 0x00000000002ffe53 cudbgGetAPI()  ???:0
 4 0x00000000004359a6 cudbgApiInit()  ???:0
 5 0x000000000018d730 ???()  /lib64/libcuda.so.1:0
 6 0x000000000018df04 ???()  /lib64/libcuda.so.1:0
 7 0x00000000001904d9 ???()  /lib64/libcuda.so.1:0
 8 0x00000000001fe77e cuGraphAddMemAllocNode()  ???:0
 9 0x00000000000a2944 mca_common_cuda_cu_memcpy()  common_cuda.c:0
10 0x000000000007c563 opal_cuda_memcpy_sync()  ???:0
11 0x0000000000077119 non_overlap_cuda_copy_content_same_ddt()  opal_datatype_copy.c:0
12 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
13 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
14 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
15 0x000000000009042b PMPI_Alltoallv()  ???:0
16 0x000000000002cf0e MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
17 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
18 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
19 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
20 0x0000000000403591 main()  ???:0
21 0x00000000000236a3 __libc_start_main()  ???:0
22 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1238182) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015ddb5 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000cb0a6 mca_btl_smcuda_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x000000000002cf0e MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
11 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
12 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
13 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
14 0x0000000000403591 main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1238181) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015ddb5 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000cb0a6 mca_btl_smcuda_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x000000000002cf0e MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
11 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
12 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
13 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
14 0x0000000000403591 main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1238183) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015ddb5 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x00000000000cb0a6 mca_btl_smcuda_prepare_src()  ???:0
 5 0x000000000020ae1e mca_pml_ob1_send_request_start_rndv()  ???:0
 6 0x000000000020d3ea mca_pml_ob1_start()  ???:0
 7 0x00000000000e2769 ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 8 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 9 0x000000000009042b PMPI_Alltoallv()  ???:0
10 0x000000000002cf0e MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
11 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
12 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
13 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
14 0x0000000000403591 main()  ???:0
15 0x00000000000236a3 __libc_start_main()  ???:0
16 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec noticed that process rank 0 with PID 1238180 on node uc2n517 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
[uc2n515.localdomain:1626137] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1626137] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1626155] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1626155] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n517:1238909:0:1238909] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14fb72000000)
[uc2n517:1238908:0:1238908] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x146bf6000000)
==== backtrace (tid:1238909) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x000000000008d803 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000002d5ef MPIcuFFT_Slab_Z_Then_YX<double>::All2All_MPIType()  ???:0
 8 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
 9 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
10 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
[uc2n517:1238906:0:1238906] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x154672000000)
[uc2n517:1238907:0:1238907] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x150dd6000000)
==== backtrace (tid:1238908) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x000000000008d803 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000002d5ef MPIcuFFT_Slab_Z_Then_YX<double>::All2All_MPIType()  ???:0
 8 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
 9 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
10 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1238907) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x000000000008d803 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000002d5ef MPIcuFFT_Slab_Z_Then_YX<double>::All2All_MPIType()  ???:0
 8 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
 9 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
10 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1238906) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x00000000000731e8 opal_convertor_pack()  ???:0
 4 0x000000000008d803 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000002d5ef MPIcuFFT_Slab_Z_Then_YX<double>::All2All_MPIType()  ???:0
 8 0x000000000002bb71 MPIcuFFT_Slab_Z_Then_YX<double>::execC2R()  ???:0
 9 0x0000000000029707 Tests_Slab_Random_Z_Then_YX<double>::testcase2()  ???:0
10 0x0000000000028b99 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec noticed that process rank 7 with PID 1238909 on node uc2n517 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
[uc2n515.localdomain:1626289] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1626289] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[31585,1],4]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1626305] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1626305] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[31437,1],6]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1626477] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1626477] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1626324] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1626324] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515:1626334:0:1626334] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x152984000000)
[uc2n515:1626337:0:1626337] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14e5ec000000)
[uc2n515:1626335:0:1626335] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14611e000000)
[uc2n515:1626336:0:1626336] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x1509fe000000)
==== backtrace (tid:1626337) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000002cd18 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
 9 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
10 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
11 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
12 0x0000000000403591 main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1626334) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000002cd18 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
 9 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
10 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
11 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
12 0x0000000000403591 main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1626335) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000002cd18 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
 9 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
10 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
11 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
12 0x0000000000403591 main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1626336) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000076438 non_overlap_copy_content_same_ddt()  opal_datatype_copy.c:0
 4 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
 5 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
 6 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
 7 0x000000000009042b PMPI_Alltoallv()  ???:0
 8 0x000000000002cd18 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
 9 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
10 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
11 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
12 0x0000000000403591 main()  ???:0
13 0x00000000000236a3 __libc_start_main()  ???:0
14 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec noticed that process rank 7 with PID 1626337 on node uc2n515 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[31297,1],7]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1626593] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1626593] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[31196,1],6]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1626748] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1626748] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515:1626669:0:1626669] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x145f5a000010)
[uc2n515:1626666:0:1626666] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14eea6000010)
[uc2n515:1626668:0:1626668] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x15465a000010)
[uc2n515:1626667:0:1626667] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x1520d6000010)
==== backtrace (tid:1626668) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x00000000001d5e98 cuMemGetAttribute()  ???:0
 3 0x00000000002ffe53 cudbgGetAPI()  ???:0
 4 0x00000000003009e8 cudbgGetAPI()  ???:0
 5 0x0000000000435d90 cudbgApiInit()  ???:0
 6 0x000000000018d730 ???()  /lib64/libcuda.so.1:0
 7 0x000000000018df04 ???()  /lib64/libcuda.so.1:0
 8 0x00000000001904d9 ???()  /lib64/libcuda.so.1:0
 9 0x00000000001fe77e cuGraphAddMemAllocNode()  ???:0
10 0x00000000000a2944 mca_common_cuda_cu_memcpy()  common_cuda.c:0
11 0x000000000007c563 opal_cuda_memcpy_sync()  ???:0
12 0x0000000000077119 non_overlap_cuda_copy_content_same_ddt()  opal_datatype_copy.c:0
13 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
14 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
15 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
16 0x000000000009042b PMPI_Alltoallv()  ???:0
17 0x000000000002cd18 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
18 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
19 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
20 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
==== backtrace (tid:1626667) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x00000000001d5e98 cuMemGetAttribute()  ???:0
 3 0x00000000002ffe53 cudbgGetAPI()  ???:0
 4 0x00000000003009e8 cudbgGetAPI()  ???:0
 5 0x0000000000435d90 cudbgApiInit()  ???:0
 6 0x000000000018d730 ???()  /lib64/libcuda.so.1:0
 7 0x000000000018df04 ???()  /lib64/libcuda.so.1:0
 8 0x00000000001904d9 ???()  /lib64/libcuda.so.1:0
 9 0x00000000001fe77e cuGraphAddMemAllocNode()  ???:0
10 0x00000000000a2944 mca_common_cuda_cu_memcpy()  common_cuda.c:0
11 0x000000000007c563 opal_cuda_memcpy_sync()  ???:0
12 0x0000000000077119 non_overlap_cuda_copy_content_same_ddt()  opal_datatype_copy.c:0
13 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
14 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
15 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
16 0x000000000009042b PMPI_Alltoallv()  ???:0
17 0x000000000002cd18 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
21 0x0000000000403591 main()  ???:0
22 0x00000000000236a3 __libc_start_main()  ???:0
23 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1626669) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x00000000001d5e98 cuMemGetAttribute()  ???:0
 3 0x00000000002ffe53 cudbgGetAPI()  ???:0
18 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
19 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
20 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
21 0x0000000000403591 main()  ???:0
22 0x00000000000236a3 __libc_start_main()  ???:0
23 0x00000000004039ee _start()  ???:0
=================================
 4 0x00000000003009e8 cudbgGetAPI()  ???:0
 5 0x0000000000435d90 cudbgApiInit()  ???:0
 6 0x000000000018d730 ???()  /lib64/libcuda.so.1:0
 7 0x000000000018df04 ???()  /lib64/libcuda.so.1:0
 8 0x00000000001904d9 ???()  /lib64/libcuda.so.1:0
 9 0x00000000001fe77e cuGraphAddMemAllocNode()  ???:0
10 0x00000000000a2944 mca_common_cuda_cu_memcpy()  common_cuda.c:0
11 0x000000000007c563 opal_cuda_memcpy_sync()  ???:0
12 0x0000000000077119 non_overlap_cuda_copy_content_same_ddt()  opal_datatype_copy.c:0
13 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
14 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
15 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
16 0x000000000009042b PMPI_Alltoallv()  ???:0
17 0x000000000002cd18 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
18 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
19 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
20 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
21 0x0000000000403591 main()  ???:0
22 0x00000000000236a3 __libc_start_main()  ???:0
23 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1626666) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x00000000001d5e98 cuMemGetAttribute()  ???:0
 3 0x00000000002ffe53 cudbgGetAPI()  ???:0
 4 0x00000000003009e8 cudbgGetAPI()  ???:0
 5 0x0000000000435d90 cudbgApiInit()  ???:0
 6 0x000000000018d730 ???()  /lib64/libcuda.so.1:0
 7 0x000000000018df04 ???()  /lib64/libcuda.so.1:0
 8 0x00000000001904d9 ???()  /lib64/libcuda.so.1:0
 9 0x00000000001fe77e cuGraphAddMemAllocNode()  ???:0
10 0x00000000000a2944 mca_common_cuda_cu_memcpy()  common_cuda.c:0
11 0x000000000007c563 opal_cuda_memcpy_sync()  ???:0
12 0x0000000000077119 non_overlap_cuda_copy_content_same_ddt()  opal_datatype_copy.c:0
13 0x000000000008db2a ompi_datatype_sndrcv()  ???:0
14 0x00000000000e280e ompi_coll_base_alltoallv_intra_basic_linear()  ???:0
15 0x00000000000ee702 ompi_coll_tuned_alltoallv_intra_dec_fixed()  ???:0
16 0x000000000009042b PMPI_Alltoallv()  ???:0
17 0x000000000002cd18 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_Sync()  ???:0
18 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
19 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
20 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
21 0x0000000000403591 main()  ???:0
22 0x00000000000236a3 __libc_start_main()  ???:0
23 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec noticed that process rank 4 with PID 1626666 on node uc2n515 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
[uc2n515.localdomain:1626656] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1626656] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[31078,1],7]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1626822] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1626822] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[30752,1],1]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1627008] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1627008] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[uc2n515.localdomain:1626979] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1626979] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515:1626989:0:1626989] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x150404000000)
[uc2n515:1626990:0:1626990] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x152ed0000000)
[uc2n515:1626991:0:1626991] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14cc3c000000)
[uc2n515:1626992:0:1626992] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14bacc000000)
==== backtrace (tid:1626989) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000002d3d9 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_MPIType()  ???:0
 8 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
 9 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
10 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1626990) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000002d3d9 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_MPIType()  ???:0
 8 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
 9 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
10 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1626991) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000002d3d9 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_MPIType()  ???:0
 8 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
 9 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
10 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1626992) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000002d3d9 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_MPIType()  ???:0
 8 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
 9 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
10 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[32767,1],6]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1627231] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1627231] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec noticed that process rank 5 with PID 1626990 on node uc2n515 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[32559,1],2]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1627279] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1627279] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515:1627297:0:1627297] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14d888000000)
[uc2n515:1627298:0:1627298] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x146186000000)
[uc2n515:1627299:0:1627299] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x14c1f8000000)
[uc2n515:1627296:0:1627296] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x154d4e000000)
==== backtrace (tid:1627298) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000002d3d9 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_MPIType()  ???:0
 8 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
 9 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
10 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1627299) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000002d3d9 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_MPIType()  ???:0
 8 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
 9 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
10 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1627296) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000002d3d9 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_MPIType()  ???:0
 8 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
 9 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
10 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
==== backtrace (tid:1627297) ====
 0 0x00000000000565ee ucs_debug_print_backtrace()  /home/es/es_es/es_rakeller/WORK/BWHPC-C5/bwhpc-rpmbuild-svn/BUILD/openmpi-4.1.0/ucx-1.9.0/src/ucs/debug/debug.c:656
 1 0x0000000000012dd0 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000015dbf3 __memmove_avx_unaligned_erms()  :0
 3 0x0000000000073399 opal_convertor_unpack()  ???:0
 4 0x000000000008d817 ompi_datatype_sndrcv()  ???:0
 5 0x0000000000121984 mca_coll_basic_alltoallw_intra()  ???:0
 6 0x0000000000090ccb PMPI_Alltoallw()  ???:0
 7 0x000000000002d3d9 MPIcuFFT_Slab_Z_Then_YX<double>::All2All_MPIType()  ???:0
 8 0x000000000002b792 MPIcuFFT_Slab_Z_Then_YX<double>::execR2C()  ???:0
 9 0x0000000000029076 Tests_Slab_Random_Z_Then_YX<double>::testcase0()  ???:0
10 0x0000000000028b61 Tests_Slab_Random_Z_Then_YX<double>::run()  ???:0
11 0x0000000000403591 main()  ???:0
12 0x00000000000236a3 __libc_start_main()  ???:0
13 0x00000000004039ee _start()  ???:0
=================================
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[32403,1],5]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1627443] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1627443] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec noticed that process rank 7 with PID 1627299 on node uc2n515 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
[uc2n515.localdomain:1627286] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1627286] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[32322,1],2]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1627618] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1627618] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[32355,1],6]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1627587] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1627587] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Starting computation for size 128
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 12:46:05.233573
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 12:46:10.530712
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 12:46:16.469134
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 12:46:21.708910
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 12:46:27.655902
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 12:46:32.838133
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 12:46:41.793849
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 12:46:46.949269
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 12:46:52.647532
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 12:46:57.856858
b''

Starting computation for size [128, 128, 256]
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 12:47:07.436309
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 12:47:12.710378
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 12:47:18.595291
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 12:47:23.810392
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 12:47:29.684051
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 12:47:34.967982
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 12:47:43.963361
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 12:47:49.374954
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 12:47:55.152058
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 12:48:00.245563
b''

Starting computation for size [128, 256, 256]
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 12:48:09.774794
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 12:48:15.173802
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 12:48:21.165447
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 12:48:26.470016
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 12:48:32.506752
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 12:48:37.877469
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 12:48:49.422668
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 12:48:54.724247
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 12:49:00.749740
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 12:49:06.165332
b''

Starting computation for size 256
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 12:49:19.173458
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 12:49:24.744306
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 12:49:30.840829
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 12:49:36.420870
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 12:49:43.448831
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 12:49:49.101805
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 12:50:06.321659
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 12:50:11.930091
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 12:50:18.026289
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 12:50:23.598633
b''

Starting computation for size [256, 256, 512]
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 12:50:43.048156
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 12:50:48.923987
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 12:50:55.135233
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 12:51:00.927175
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 12:51:07.121846
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 12:51:13.366194
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 12:51:30.974218
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 12:51:36.846944
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 12:51:43.085608
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 12:51:49.269866
b''

Starting computation for size [256, 512, 512]
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 12:52:08.623465
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 12:52:15.459668
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 12:52:22.071235
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 12:52:28.833145
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 12:52:35.948666
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 12:52:43.104507
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 12:53:13.254033
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 12:53:20.254713
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 12:53:26.948700
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 12:53:34.055876
b''

Starting computation for size 512
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 12:54:06.732555
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 12:54:15.160343
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 12:54:22.530702
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 12:54:30.237576
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 12:54:37.727557
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 12:54:47.144178
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 12:55:41.379205
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 12:55:50.140268
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 12:55:57.683350
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 12:56:06.921268
b''

Starting computation for size [512, 512, 1024]
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 12:57:08.606871
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 12:57:20.702437
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 12:57:29.958907
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 12:57:40.844769
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 12:57:50.217007
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 12:58:03.303471
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 12:59:02.051424
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 12:59:14.614194
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 12:59:23.793149
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 12:59:37.408726
b''

Starting computation for size [512, 1024, 1024]
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 13:00:42.692721
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 13:01:01.935521
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 13:01:15.111645
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 13:01:31.559347
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 13:01:44.079047
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 13:02:06.483256
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 13:03:52.066012
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 13:04:11.716679
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 13:04:24.840733
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 13:04:46.889227
b''

Starting computation for size 1024
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 13:06:45.241043
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 13:07:18.250356
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 13:07:37.429116
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 13:08:05.381583
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 13:08:25.087826
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 13:09:04.962731
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 13:12:29.941317
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 13:13:04.669421
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 13:13:24.407506
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 13:14:03.073858
b''

Starting computation for size [1024, 1024, 2048]
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 13:17:52.912077
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 13:18:54.307299
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 13:19:29.079766
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 13:20:20.334262
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 13:20:54.932776
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 13:22:05.602217
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 13:25:50.042978
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 13:26:54.492537
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 13:27:29.232839
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 13:28:37.610055
b''

Starting computation for size [1024, 2048, 2048]
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 13:32:50.993962
b''

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 13:34:46.325520
b''

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 13:35:50.795668
b''

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 13:37:27.315892
b''

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 13:38:28.716830
b''

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 13:40:46.570602
b''

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 13:48:11.805025
b''

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 13:48:24.618671
b''

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 13:48:34.432547
b''

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 13:50:46.962405
b''

Starting computation for size 2048
-> Executing test 0
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 13:50:52.048357
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\n'

-> Executing test 1
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 13:50:58.143341
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\n'

-> Executing test 2
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 13:51:03.498150
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\n'

-> Executing test 3
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 13:51:08.168724
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\n'

-> Executing test 4
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 13:51:12.781982
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\n'

-> Executing test 5
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 13:51:17.436654
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\n'

-> Executing test 6
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 13:51:22.204072
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\n'

-> Executing test 7
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 13:51:27.411002
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\n'

-> Executing test 8
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 13:51:32.487176
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\n'

-> Executing test 9
mpiexec -n 8 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_2 --rankfile ../mpi/rankfile_2 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 8 -b ../benchmarks/bwunicluster/gpu8/small/inverse -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 13:51:37.091222
b'Error 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/st/st_us-051200/st_st160727/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\n'

--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[32117,1],2]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1627861] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1627861] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[31865,1],0]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1628121] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1628121] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[29562,1],6]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1628378] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1628378] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[29308,1],6]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1628636] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1628636] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[28993,1],4]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1628897] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1628897] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[28739,1],6]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1629155] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1629155] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[30533,1],5]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1629413] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1629413] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[30294,1],4]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1629686] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1629686] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[30041,1],2]
  Exit code:    1
--------------------------------------------------------------------------
[uc2n515.localdomain:1629945] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1629945] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1630203] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1630203] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1630466] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1630466] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1630725] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1630725] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1631006] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1631006] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1631290] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1631290] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1631548] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1631548] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1631805] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1631805] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1632063] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1632063] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1632323] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1632323] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1632581] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1632581] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1632852] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1632852] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1633110] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1633110] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1633368] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1633368] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1633650] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1633650] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1633924] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1633924] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1634195] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1634195] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1634453] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1634453] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1634713] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1634713] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1634970] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1634970] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1635230] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1635230] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1635498] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1635498] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1635755] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1635755] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1636017] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1636017] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1636311] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1636311] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1636587] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1636587] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1636847] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1636847] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1637109] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1637109] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1637383] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1637383] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1637640] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1637640] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1637900] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1637900] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1638172] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1638172] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1638451] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1638451] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1638733] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1638733] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1639037] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1639037] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1639330] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1639330] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1639614] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1639614] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1639897] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1639897] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1640179] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1640179] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1640461] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1640461] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n515.localdomain:1640740] 7 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n515.localdomain:1640740] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
slurmstepd: error: *** JOB 19765165 ON uc2n515 CANCELLED AT 2021-09-19T14:00:47 DUE TO TIME LIMIT ***

============================= JOB FEEDBACK =============================

NodeName=uc2n[515,517]
Job ID: 19765165
Cluster: uc2
User/Group: st_st160727/st_us-051200
State: TIMEOUT (exit code 0)
Nodes: 2
Cores per node: 80
CPU Utilized: 5-11:50:07
CPU Efficiency: 6.86% of 80-00:37:20 core-walltime
Job Wall-clock time: 12:00:14
Memory Utilized: 198.63 GB
Memory Efficiency: 0.00% of 0.00 MB
