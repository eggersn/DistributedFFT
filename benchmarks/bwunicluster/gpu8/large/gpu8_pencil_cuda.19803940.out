Modules loaded
uc2n508
uc2n508
uc2n508
uc2n508
uc2n508
uc2n508
uc2n508
uc2n508
uc2n509
uc2n509
uc2n509
uc2n509
uc2n509
uc2n509
uc2n509
uc2n509
uc2n510
uc2n510
uc2n510
uc2n510
uc2n510
uc2n510
uc2n510
uc2n510
uc2n511
uc2n511
uc2n511
uc2n511
uc2n511
uc2n511
uc2n511
uc2n511
uc2n514
uc2n514
uc2n514
uc2n514
uc2n514
uc2n514
uc2n514
uc2n514
uc2n515
uc2n515
uc2n515
uc2n515
uc2n515
uc2n515
uc2n515
uc2n515
uc2n516
uc2n516
uc2n516
uc2n516
uc2n516
uc2n516
uc2n516
uc2n516
uc2n517
uc2n517
uc2n517
uc2n517
uc2n517
uc2n517
uc2n517
uc2n517
64: uc2n508 uc2n508 uc2n508 uc2n508 uc2n508 uc2n508 uc2n508 uc2n508 uc2n509 uc2n509 uc2n509 uc2n509 uc2n509 uc2n509 uc2n509 uc2n509 uc2n510 uc2n510 uc2n510 uc2n510 uc2n510 uc2n510 uc2n510 uc2n510 uc2n511 uc2n511 uc2n511 uc2n511 uc2n511 uc2n511 uc2n511 uc2n511 uc2n514 uc2n514 uc2n514 uc2n514 uc2n514 uc2n514 uc2n514 uc2n514 uc2n515 uc2n515 uc2n515 uc2n515 uc2n515 uc2n515 uc2n515 uc2n515 uc2n516 uc2n516 uc2n516 uc2n516 uc2n516 uc2n516 uc2n516 uc2n516 uc2n517 uc2n517 uc2n517 uc2n517 uc2n517 uc2n517 uc2n517 uc2n517
start building
-- The CUDA compiler identification is NVIDIA 11.0.194
-- The CXX compiler identification is GNU 8.3.1
-- Detecting CUDA compiler ABI info
-- Detecting CUDA compiler ABI info - done
-- Check for working CUDA compiler: /opt/bwhpc/common/devel/cuda/11.0/bin/nvcc - skipped
-- Detecting CUDA compile features
-- Detecting CUDA compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/g++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found MPI_CXX: /pfs/data5/software_uc2/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libmpi.so (found version "3.1") 
-- Found MPI: TRUE (found version "3.1")  
-- Found CUDAToolkit: /opt/bwhpc/common/devel/cuda/11.0/include (found version "11.0.194") 
-- Looking for C++ include pthread.h
-- Looking for C++ include pthread.h - found
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed
-- Looking for pthread_create in pthreads
-- Looking for pthread_create in pthreads - not found
-- Looking for pthread_create in pthread
-- Looking for pthread_create in pthread - found
-- Found Threads: TRUE  
-- Configuring done
-- Generating done
-- Build files have been written to: /home/st/st_us-051200/st_st160727/DistributedFFT/build_gpu8
Scanning dependencies of target test_base
[  3%] Building CUDA object CMakeFiles/test_base.dir/tests/src/base.cu.o
nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[  6%] Linking CUDA shared library libtest_base.so
[  6%] Built target test_base
Scanning dependencies of target mpicufft
[  9%] Building CXX object CMakeFiles/mpicufft.dir/src/mpicufft.cpp.o
[ 12%] Linking CXX shared library libmpicufft.so
[ 12%] Built target mpicufft
Scanning dependencies of target timer
[ 15%] Building CXX object CMakeFiles/timer.dir/src/timer.cpp.o
[ 18%] Linking CXX shared library libtimer.so
[ 18%] Built target timer
Scanning dependencies of target pencil_decomp
[ 21%] Building CXX object CMakeFiles/pencil_decomp.dir/src/pencil/mpicufft_pencil.cpp.o
[ 24%] Building CXX object CMakeFiles/pencil_decomp.dir/src/pencil/mpicufft_pencil_opt1.cpp.o
[ 27%] Linking CXX shared library libpencil_decomp.so
[ 27%] Built target pencil_decomp
Scanning dependencies of target pencil_tests
[ 30%] Building CUDA object CMakeFiles/pencil_tests.dir/tests/src/pencil/base.cu.o
nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 33%] Building CUDA object CMakeFiles/pencil_tests.dir/tests/src/pencil/random_dist_1D.cu.o
nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 36%] Building CUDA object CMakeFiles/pencil_tests.dir/tests/src/pencil/random_dist_2D.cu.o
nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 39%] Building CUDA object CMakeFiles/pencil_tests.dir/tests/src/pencil/random_dist_3D.cu.o
nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 42%] Linking CUDA shared library libpencil_tests.so
[ 42%] Built target pencil_tests
Scanning dependencies of target pencil
[ 45%] Building CXX object CMakeFiles/pencil.dir/tests/src/pencil/main.cpp.o
[ 48%] Linking CXX executable pencil
[ 48%] Built target pencil
Scanning dependencies of target slab_decomp
[ 51%] Building CXX object CMakeFiles/slab_decomp.dir/src/slab/default/mpicufft_slab.cpp.o
[ 54%] Building CXX object CMakeFiles/slab_decomp.dir/src/slab/default/mpicufft_slab_opt1.cpp.o
[ 57%] Building CXX object CMakeFiles/slab_decomp.dir/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp.o
[ 60%] Building CXX object CMakeFiles/slab_decomp.dir/src/slab/z_then_yx/mpicufft_slab_z_then_yx_opt1.cpp.o
[ 63%] Building CXX object CMakeFiles/slab_decomp.dir/src/slab/y_then_zx/mpicufft_slab_y_then_zx.cpp.o
[ 66%] Linking CXX shared library libslab_decomp.so
[ 66%] Built target slab_decomp
Scanning dependencies of target slab_tests
[ 69%] Building CUDA object CMakeFiles/slab_tests.dir/tests/src/slab/base.cu.o
nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 72%] Building CUDA object CMakeFiles/slab_tests.dir/tests/src/slab/random_dist_default.cu.o
nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 75%] Building CUDA object CMakeFiles/slab_tests.dir/tests/src/slab/random_dist_y_then_zx.cu.o
nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 78%] Building CUDA object CMakeFiles/slab_tests.dir/tests/src/slab/random_dist_z_then_yx.cu.o
nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 81%] Linking CUDA shared library libslab_tests.so
[ 81%] Built target slab_tests
Scanning dependencies of target slab
[ 84%] Building CXX object CMakeFiles/slab.dir/tests/src/slab/main.cpp.o
[ 87%] Linking CXX executable slab
[ 87%] Built target slab
Scanning dependencies of target reference_tests
[ 90%] Building CUDA object CMakeFiles/reference_tests.dir/tests/src/reference/reference.cu.o
nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 93%] Linking CUDA shared library libreference_tests.so
[ 93%] Built target reference_tests
Scanning dependencies of target reference
[ 96%] Building CXX object CMakeFiles/reference.dir/tests/src/reference/main.cpp.o
[100%] Linking CXX executable reference
[100%] Built target reference
finished building
Starting on HOST64
*****************************************************************************
Partition 8x8
-----------------------------------------------------------------------------
Pencil Default
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n508
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1486432] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1486432] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1486697] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1486697] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1487223] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1487223] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1487498] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1487498] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1487784] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1487784] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1488059] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1488059] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1488581] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1488581] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1488864] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1488864] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1489139] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1489139] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1489415] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1489415] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1489706] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1489706] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1490229] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1490229] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1490504] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1490504] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1490785] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1490785] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1491059] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1491059] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1491584] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1491584] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1491868] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1491868] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1492139] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1492139] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1492412] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1492412] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1492696] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1492696] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1493217] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1493217] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1493504] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1493504] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1493778] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1493778] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1494054] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1494054] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1494585] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1494585] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1494858] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1494858] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1495130] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1495130] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1495418] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1495418] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1495693] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1495693] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1496216] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1496216] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1496498] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1496498] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1496774] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1496774] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1497059] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1497059] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1497582] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1497582] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1497857] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1497857] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1498147] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1498147] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1498420] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1498420] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1498705] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1498705] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1499224] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1499224] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1499500] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1499500] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1499784] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1499784] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1500060] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1500060] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1500597] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1500597] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1500872] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1500872] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1501152] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1501152] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1501426] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1501426] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1501701] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1501701] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1502230] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1502230] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1502503] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1502503] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1502790] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1502790] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1503074] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1503074] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1503608] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1503608] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1503881] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1503881] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1504155] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1504155] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1504441] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1504441] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1504713] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1504713] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1505251] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1505251] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1505530] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1505530] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1505813] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1505813] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1506092] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1506092] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1506627] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1506627] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1506902] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1506902] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1507176] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1507176] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1507465] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1507465] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1507738] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1507738] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1508261] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1508261] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1508545] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1508545] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1508820] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1508820] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1509127] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1509127] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1509649] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1509649] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1509924] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1509924] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1510207] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1510207] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n514
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1510482] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1510482] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1510770] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1510770] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1511293] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1511293] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1511589] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1511589] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1511863] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1511863] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1512157] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1512157] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1512691] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1512691] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1512966] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1512966] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1513253] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1513253] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1513528] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1513528] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1513819] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1513819] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1514342] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1514342] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1514657] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1514657] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1514931] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1514931] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1515238] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1515238] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1515762] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1515762] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1516053] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1516053] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1516328] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1516328] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1516614] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1516614] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1516889] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1516889] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1517423] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1517423] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1517742] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1517742] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1518016] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1518016] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1518328] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1518328] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1518854] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1518854] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1519141] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1519141] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1519416] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1519416] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1519710] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1519710] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1520003] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1520003] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1520540] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1520540] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1520880] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1520880] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1521160] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1521160] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1521502] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1521502] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1522039] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1522039] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1522330] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1522330] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1522619] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1522619] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1522899] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1522899] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1523212] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1523212] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1523755] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1523755] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1524151] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1524151] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1524452] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1524452] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1524848] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1524848] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1525399] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1525399] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1525701] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1525701] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1525997] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1525997] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1526307] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1526307] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1526582] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1526582] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1526880] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1526880] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1527166] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1527166] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1527439] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1527439] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1527721] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1527721] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1528010] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1528010] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1528283] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1528283] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1528575] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1528575] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1528848] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1528848] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1529121] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1529121] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1529428] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1529428] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1529718] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1529718] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1529990] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1529990] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1530266] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1530266] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1530569] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1530569] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1530844] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1530844] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1531120] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1531120] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1531405] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1531405] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1531677] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1531677] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1531985] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1531985] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1532262] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1532262] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1532537] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1532537] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1532827] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1532827] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1533114] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1533114] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1533397] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1533397] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1533670] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1533670] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1533943] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1533943] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1534228] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1534228] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1534528] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1534528] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1534903] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1534903] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1535181] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1535181] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1535466] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1535466] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1535759] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1535759] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1536045] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1536045] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1536320] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1536320] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1536609] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1536609] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1536902] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1536902] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1537221] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1537221] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1537533] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1537533] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1537827] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1537827] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1538144] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1538144] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1538457] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1538457] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1538752] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1538752] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1539049] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1539049] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Starting computation for size 128
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 128 -ny 128 -nz 128
2021-09-21 20:01:35.081572
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 128 -ny 128 -nz 128
2021-09-21 20:01:49.188396
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 128 -ny 128 -nz 128
2021-09-21 20:01:58.895755
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 128 -ny 128 -nz 128
2021-09-21 20:02:09.386996
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 128 -ny 128 -nz 128
2021-09-21 20:02:31.008882
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 128 -ny 128 -nz 128
2021-09-21 20:02:41.576754
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 128 -ny 128 -nz 128
2021-09-21 20:02:51.348056
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 128 -ny 128 -nz 128
2021-09-21 20:03:01.214792
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 128 -ny 128 -nz 128
2021-09-21 20:03:10.944046
b''

Starting computation for size [128, 128, 256]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 128 -ny 128 -nz 256
2021-09-21 20:03:20.600193
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 128 -ny 128 -nz 256
2021-09-21 20:03:30.790668
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 128 -ny 128 -nz 256
2021-09-21 20:03:40.820340
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 128 -ny 128 -nz 256
2021-09-21 20:03:50.960460
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 128 -ny 128 -nz 256
2021-09-21 20:04:00.991635
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 128 -ny 128 -nz 256
2021-09-21 20:04:11.103847
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 128 -ny 128 -nz 256
2021-09-21 20:04:24.075367
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 128 -ny 128 -nz 256
2021-09-21 20:04:35.847184
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 128 -ny 128 -nz 256
2021-09-21 20:04:45.551519
b''

Starting computation for size [128, 256, 256]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 128 -ny 256 -nz 256
2021-09-21 20:04:55.323315
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 128 -ny 256 -nz 256
2021-09-21 20:05:04.788355
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 128 -ny 256 -nz 256
2021-09-21 20:05:14.247057
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 128 -ny 256 -nz 256
2021-09-21 20:05:27.581107
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 128 -ny 256 -nz 256
2021-09-21 20:05:41.199055
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 128 -ny 256 -nz 256
2021-09-21 20:05:51.843240
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 128 -ny 256 -nz 256
2021-09-21 20:06:01.391565
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 128 -ny 256 -nz 256
2021-09-21 20:06:10.875233
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 128 -ny 256 -nz 256
2021-09-21 20:06:20.530712
b''

Starting computation for size 256
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 256 -ny 256 -nz 256
2021-09-21 20:06:30.482555
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 256 -ny 256 -nz 256
2021-09-21 20:06:40.829854
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 256 -ny 256 -nz 256
2021-09-21 20:06:50.847761
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 256 -ny 256 -nz 256
2021-09-21 20:07:03.165448
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 256 -ny 256 -nz 256
2021-09-21 20:07:15.991708
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 256 -ny 256 -nz 256
2021-09-21 20:07:29.375376
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 256 -ny 256 -nz 256
2021-09-21 20:07:40.617891
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 256 -ny 256 -nz 256
2021-09-21 20:07:50.542203
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 256 -ny 256 -nz 256
2021-09-21 20:08:12.842035
b''

Starting computation for size [256, 256, 512]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 256 -ny 256 -nz 512
2021-09-21 20:08:22.896951
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 256 -ny 256 -nz 512
2021-09-21 20:08:33.056623
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 256 -ny 256 -nz 512
2021-09-21 20:08:43.102223
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 256 -ny 256 -nz 512
2021-09-21 20:08:56.298220
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 256 -ny 256 -nz 512
2021-09-21 20:09:07.401003
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 256 -ny 256 -nz 512
2021-09-21 20:09:25.399744
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 256 -ny 256 -nz 512
2021-09-21 20:09:36.716616
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 256 -ny 256 -nz 512
2021-09-21 20:09:47.014765
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 256 -ny 256 -nz 512
2021-09-21 20:09:57.154166
b''

Starting computation for size [256, 512, 512]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 256 -ny 512 -nz 512
2021-09-21 20:10:07.812610
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 256 -ny 512 -nz 512
2021-09-21 20:10:18.198716
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 256 -ny 512 -nz 512
2021-09-21 20:10:28.513914
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 256 -ny 512 -nz 512
2021-09-21 20:10:43.690927
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 256 -ny 512 -nz 512
2021-09-21 20:10:57.049635
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 256 -ny 512 -nz 512
2021-09-21 20:11:17.441694
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 256 -ny 512 -nz 512
2021-09-21 20:11:27.708162
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 256 -ny 512 -nz 512
2021-09-21 20:11:38.499891
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 256 -ny 512 -nz 512
2021-09-21 20:11:48.893171
b''

Starting computation for size 512
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 512 -ny 512 -nz 512
2021-09-21 20:12:05.369830
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 512 -ny 512 -nz 512
2021-09-21 20:12:16.070141
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 512 -ny 512 -nz 512
2021-09-21 20:12:27.773298
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 512 -ny 512 -nz 512
2021-09-21 20:12:45.897847
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 512 -ny 512 -nz 512
2021-09-21 20:13:00.284359
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 512 -ny 512 -nz 512
2021-09-21 20:13:18.539229
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 512 -ny 512 -nz 512
2021-09-21 20:13:29.708186
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 512 -ny 512 -nz 512
2021-09-21 20:13:40.367244
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 512 -ny 512 -nz 512
2021-09-21 20:13:51.102090
b''

Starting computation for size [512, 512, 1024]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 512 -ny 512 -nz 1024
2021-09-21 20:14:02.330288
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 512 -ny 512 -nz 1024
2021-09-21 20:14:13.841723
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 512 -ny 512 -nz 1024
2021-09-21 20:14:25.149168
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 512 -ny 512 -nz 1024
2021-09-21 20:14:44.736537
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 512 -ny 512 -nz 1024
2021-09-21 20:14:56.088994
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 512 -ny 512 -nz 1024
2021-09-21 20:15:28.336379
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 512 -ny 512 -nz 1024
2021-09-21 20:15:39.396536
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 512 -ny 512 -nz 1024
2021-09-21 20:15:50.457620
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 512 -ny 512 -nz 1024
2021-09-21 20:16:01.634603
b''

Starting computation for size [512, 1024, 1024]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 512 -ny 1024 -nz 1024
2021-09-21 20:16:13.883022
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 512 -ny 1024 -nz 1024
2021-09-21 20:16:31.340847
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 512 -ny 1024 -nz 1024
2021-09-21 20:16:43.314833
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 512 -ny 1024 -nz 1024
2021-09-21 20:17:10.012734
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 512 -ny 1024 -nz 1024
2021-09-21 20:17:21.870129
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 512 -ny 1024 -nz 1024
2021-09-21 20:17:48.217718
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 512 -ny 1024 -nz 1024
2021-09-21 20:18:06.634182
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 512 -ny 1024 -nz 1024
2021-09-21 20:18:25.526988
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 512 -ny 1024 -nz 1024
2021-09-21 20:18:37.600249
b''

Starting computation for size 1024
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 1024 -ny 1024 -nz 1024
2021-09-21 20:18:49.661266
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 1024 -ny 1024 -nz 1024
2021-09-21 20:19:07.300123
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 1024 -ny 1024 -nz 1024
2021-09-21 20:19:21.698410
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 1024 -ny 1024 -nz 1024
2021-09-21 20:20:05.846620
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 1024 -ny 1024 -nz 1024
2021-09-21 20:20:23.687126
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 1024 -ny 1024 -nz 1024
2021-09-21 20:21:06.593816
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 1024 -ny 1024 -nz 1024
2021-09-21 20:21:23.005940
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 1024 -ny 1024 -nz 1024
2021-09-21 20:21:36.451119
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 1024 -ny 1024 -nz 1024
2021-09-21 20:21:49.945590
b''

Starting computation for size [1024, 1024, 2048]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 1024 -ny 1024 -nz 2048
2021-09-21 20:22:03.902364
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 1024 -ny 1024 -nz 2048
2021-09-21 20:22:20.671084
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 1024 -ny 1024 -nz 2048
2021-09-21 20:22:41.232732
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 1024 -ny 1024 -nz 2048
2021-09-21 20:23:27.923036
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 1024 -ny 1024 -nz 2048
2021-09-21 20:23:44.862389
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 1024 -ny 1024 -nz 2048
2021-09-21 20:24:30.758877
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 1024 -ny 1024 -nz 2048
2021-09-21 20:24:49.591024
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 1024 -ny 1024 -nz 2048
2021-09-21 20:25:06.477988
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 1024 -ny 1024 -nz 2048
2021-09-21 20:25:23.405134
b''

Starting computation for size [1024, 2048, 2048]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 1024 -ny 2048 -nz 2048
2021-09-21 20:25:50.598137
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 1024 -ny 2048 -nz 2048
2021-09-21 20:26:15.288216
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 1024 -ny 2048 -nz 2048
2021-09-21 20:26:38.384172
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 1024 -ny 2048 -nz 2048
2021-09-21 20:28:00.488252
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 1024 -ny 2048 -nz 2048
2021-09-21 20:28:23.877896
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 1024 -ny 2048 -nz 2048
2021-09-21 20:29:46.294844
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 1024 -ny 2048 -nz 2048
2021-09-21 20:30:11.331926
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 1024 -ny 2048 -nz 2048
2021-09-21 20:30:34.795093
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 1024 -ny 2048 -nz 2048
2021-09-21 20:30:59.227628
b''

Starting computation for size 2048
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 2048 -ny 2048 -nz 2048
2021-09-21 20:31:23.855262
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 2048 -ny 2048 -nz 2048
2021-09-21 20:32:04.564578
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 2048 -ny 2048 -nz 2048
2021-09-21 20:32:42.263336
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 2048 -ny 2048 -nz 2048
2021-09-21 20:35:17.659717
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 2048 -ny 2048 -nz 2048
2021-09-21 20:35:55.292236
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 2048 -ny 2048 -nz 2048
2021-09-21 20:38:26.955024
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 2048 -ny 2048 -nz 2048
2021-09-21 20:39:03.034712
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 2048 -ny 2048 -nz 2048
2021-09-21 20:39:40.024331
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 2048 -ny 2048 -nz 2048
2021-09-21 20:40:16.669854
b''

Starting computation for size 128
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-21 20:41:02.435755
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-21 20:41:14.731157
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-21 20:41:26.051222
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-21 20:41:39.569323
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-21 20:41:49.636139
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-21 20:41:59.936045
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-21 20:42:10.015517
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-21 20:42:22.747297
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-21 20:42:35.533179
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

Starting computation for size 256
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-21 20:42:46.017678
b'Result (avg): 4.77529e-09\nResult (max): 5.11245e-08\n'

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-21 20:42:57.325441
b'Result (avg): 4.77529e-09\nResult (max): 5.11245e-08\n'

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-21 20:43:13.713689
b'Result (avg): 4.77529e-09\nResult (max): 5.11245e-08\n'

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-21 20:43:28.821584
b'Result (avg): 4.77529e-09\nResult (max): 5.11245e-08\n'

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-21 20:43:38.998703
b'Result (avg): 4.77529e-09\nResult (max): 5.11245e-08\n'

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-21 20:43:49.564552
b'Result (avg): 4.77529e-09\nResult (max): 5.11245e-08\n'

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-21 20:44:01.846657
b'Result (avg): 4.77529e-09\nResult (max): 5.11245e-08\n'

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-21 20:44:18.176528
b'Result (avg): 4.77529e-09\nResult (max): 5.11245e-08\n'

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-21 20:44:28.558367
b'Result (avg): 4.77529e-09\nResult (max): 5.11245e-08\n'

Starting computation for size 512
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-21 20:44:38.899786
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-21 20:44:49.602116
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-21 20:45:02.005522
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-21 20:45:15.353900
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-21 20:45:25.913753
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-21 20:45:37.089486
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-21 20:45:47.929682
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-21 20:45:58.910401
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-21 20:46:10.417251
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

Starting computation for size 1024
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-21 20:46:21.726767
b'Result (avg): 7.33017e-07\nResult (max): 8.68929e-06\n'

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-21 20:46:34.995332
b'Result (avg): 7.33017e-07\nResult (max): 8.68929e-06\n'

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-21 20:46:52.638029
b'Result (avg): 7.33017e-07\nResult (max): 8.68929e-06\n'

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-21 20:47:12.087850
b'Result (avg): 7.33017e-07\nResult (max): 8.68929e-06\n'

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-21 20:47:25.468895
b'Result (avg): 7.33017e-07\nResult (max): 8.68929e-06\n'

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-21 20:47:40.913302
b'Result (avg): 7.33017e-07\nResult (max): 8.68929e-06\n'

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-21 20:47:55.152148
b'Result (avg): 7.33017e-07\nResult (max): 8.68929e-06\n'

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-21 20:48:14.168417
b'Result (avg): 7.33017e-07\nResult (max): 8.68929e-06\n'

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-21 20:48:27.337558
b'Result (avg): 7.33017e-07\nResult (max): 8.68929e-06\n'

Starting computation for size 2048
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-21 20:48:40.533761
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-21 20:49:15.839153
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-21 20:49:51.136656
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-21 20:50:40.624530
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-21 20:51:16.741809
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-21 20:52:00.625472
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-21 20:52:36.204422
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-21 20:53:13.524053
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-21 20:53:48.346565
b'Result (avg): -nan\nResult (max): -nan\n'

Pencil Opt1
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1539349] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1539349] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1539635] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1539635] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1539908] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1539908] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1540189] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1540189] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1540462] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1540462] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1540736] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1540736] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1541019] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1541019] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1541292] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1541292] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1541567] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1541567] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1541852] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1541852] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1542125] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1542125] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1542400] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1542400] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1542682] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1542682] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1542955] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1542955] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1543228] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1543228] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1543513] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1543513] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1543788] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1543788] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1544079] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1544079] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1544350] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1544350] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1544625] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1544625] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1544908] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1544908] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1545184] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1545184] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1545466] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1545466] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1545739] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1545739] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1546014] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1546014] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1546301] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1546301] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1546574] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1546574] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1546847] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1546847] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1547135] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1547135] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1547407] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1547407] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1547691] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1547691] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1547966] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1547966] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1548240] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1548240] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1548528] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1548528] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1548802] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1548802] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1549125] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1549125] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1549400] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1549400] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1549690] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1549690] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1549963] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1549963] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1550248] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1550248] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1550523] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1550523] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1550810] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1550810] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1551085] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1551085] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1551359] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1551359] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1551642] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1551642] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1551917] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1551917] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1552190] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1552190] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1552483] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1552483] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1552754] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1552754] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1553044] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1553044] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1553319] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1553319] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1553601] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1553601] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1553878] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1553878] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1554167] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1554167] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1554448] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1554448] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1554728] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1554728] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1555003] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1555003] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1555277] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1555277] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1555564] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1555564] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1555851] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1555851] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1556125] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1556125] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1556400] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1556400] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1556683] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1556683] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1556957] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1556957] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1557242] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1557242] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1557515] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1557515] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1557807] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1557807] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1558083] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1558083] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1558369] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1558369] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1558642] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1558642] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1558927] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1558927] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1559200] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1559200] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1559494] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1559494] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1559771] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1559771] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1560056] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1560056] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1560334] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1560334] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1560621] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1560621] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1560915] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1560915] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1561190] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1561190] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1561483] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1561483] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1561759] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1561759] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1562051] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1562051] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1562339] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1562339] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1562615] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1562615] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1562907] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1562907] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1563200] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1563200] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1563499] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1563499] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1563774] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1563774] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1564077] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1564077] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1564352] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1564352] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1564652] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1564652] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1564939] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1564939] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1565216] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1565216] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1565531] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1565531] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1565807] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1565807] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1566119] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1566119] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1566409] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1566409] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1566708] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1566708] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1567001] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1567001] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1567304] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1567304] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1567595] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1567595] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1567885] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1567885] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1568207] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1568207] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1568485] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1568485] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1568804] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1568804] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1569097] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1569097] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1569434] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1569434] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1569724] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1569724] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1570058] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1570058] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1570349] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1570349] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1570658] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1570658] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1571009] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1571009] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n508
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1571302] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1571302] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1571655] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1571655] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1571963] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1571963] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1572311] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1572311] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1572605] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1572605] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1572960] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1572960] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1573247] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1573247] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1573534] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1573534] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1573819] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1573819] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1574094] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1574094] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1574371] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1574371] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1574672] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1574672] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1574949] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1574949] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1575224] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1575224] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1575509] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1575509] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1575785] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1575785] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1576074] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1576074] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1576362] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1576362] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1576635] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1576635] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1576918] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1576918] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1577209] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1577209] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1577483] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1577483] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1577772] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1577772] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1578047] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1578047] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1578330] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1578330] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1578622] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1578622] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1578899] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1578899] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1579184] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1579184] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1579459] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1579459] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1579762] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1579762] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1580037] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1580037] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1580312] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1580312] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1580597] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1580597] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1580872] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1580872] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1581175] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1581175] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1581452] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1581452] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1581729] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1581729] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1582018] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1582018] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1582308] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1582308] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1582594] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1582594] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1582869] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1582869] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1583162] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1583162] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1583454] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1583454] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1583765] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1583765] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1584060] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1584060] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1584372] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1584372] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1584666] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1584666] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1584981] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1584981] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1585279] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1585279] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1585575] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1585575] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Starting computation for size 128
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 128
2021-09-21 20:54:27.441259
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 128
2021-09-21 20:54:39.632915
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 128
2021-09-21 20:54:52.517982
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 128
2021-09-21 20:55:02.538614
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 128
2021-09-21 20:55:13.457276
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 128
2021-09-21 20:55:23.808413
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 128
2021-09-21 20:55:33.737331
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 128
2021-09-21 20:55:43.941852
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 128
2021-09-21 20:55:53.824165
b''

Starting computation for size [128, 128, 256]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 256
2021-09-21 20:56:04.077422
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 256
2021-09-21 20:56:14.025406
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 256
2021-09-21 20:56:23.841431
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 256
2021-09-21 20:56:34.201926
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 256
2021-09-21 20:56:44.209819
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 256
2021-09-21 20:56:54.763321
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 256
2021-09-21 20:57:10.335454
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 256
2021-09-21 20:57:24.021110
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 256
2021-09-21 20:57:35.969945
b''

Starting computation for size [128, 256, 256]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 256 -nz 256
2021-09-21 20:57:46.420531
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 256 -nz 256
2021-09-21 20:57:56.901796
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 256 -nz 256
2021-09-21 20:58:06.797744
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 256 -nz 256
2021-09-21 20:58:20.839404
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 256 -nz 256
2021-09-21 20:58:30.728713
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 256 -nz 256
2021-09-21 20:58:41.185035
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 256 -nz 256
2021-09-21 20:58:51.534709
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 256 -nz 256
2021-09-21 20:59:02.386551
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 256 -nz 256
2021-09-21 20:59:12.210742
b''

Starting computation for size 256
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 256
2021-09-21 20:59:23.315986
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 256
2021-09-21 20:59:38.570049
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 256
2021-09-21 20:59:48.705551
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 256
2021-09-21 20:59:59.767512
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 256
2021-09-21 21:00:09.660066
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 256
2021-09-21 21:00:26.288939
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 256
2021-09-21 21:00:42.958997
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 256
2021-09-21 21:00:53.859283
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 256
2021-09-21 21:01:04.227249
b''

Starting computation for size [256, 256, 512]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 512
2021-09-21 21:01:15.159062
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 512
2021-09-21 21:01:41.384585
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 512
2021-09-21 21:01:51.469794
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 512
2021-09-21 21:02:03.376332
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 512
2021-09-21 21:02:14.573945
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 512
2021-09-21 21:02:32.788989
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 512
2021-09-21 21:02:43.018148
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 512
2021-09-21 21:02:54.911585
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 512
2021-09-21 21:03:04.932118
b''

Starting computation for size [256, 512, 512]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 512 -nz 512
2021-09-21 21:03:16.789070
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 512 -nz 512
2021-09-21 21:03:26.889985
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 512 -nz 512
2021-09-21 21:03:37.006875
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 512 -nz 512
2021-09-21 21:03:49.240418
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 512 -nz 512
2021-09-21 21:04:05.485498
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 512 -nz 512
2021-09-21 21:04:20.920199
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 512 -nz 512
2021-09-21 21:04:32.119773
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 512 -nz 512
2021-09-21 21:04:45.395532
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 512 -nz 512
2021-09-21 21:05:03.343157
b''

Starting computation for size 512
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 512
2021-09-21 21:05:21.630732
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 512
2021-09-21 21:05:32.883088
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 512
2021-09-21 21:05:43.439005
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 512
2021-09-21 21:05:59.684424
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 512
2021-09-21 21:06:16.846301
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 512
2021-09-21 21:06:31.476803
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 512
2021-09-21 21:06:43.255926
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 512
2021-09-21 21:06:57.971617
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 512
2021-09-21 21:07:08.950173
b''

Starting computation for size [512, 512, 1024]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-21 21:07:24.862145
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-21 21:07:36.709624
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-21 21:07:47.261345
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-21 21:08:06.724543
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-21 21:08:26.757750
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-21 21:08:45.196216
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-21 21:08:56.264242
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-21 21:09:13.752759
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-21 21:09:24.425930
b''

Starting computation for size [512, 1024, 1024]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-21 21:09:43.858133
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-21 21:09:55.236942
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-21 21:10:07.251382
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-21 21:10:27.167089
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-21 21:10:46.680678
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-21 21:11:06.026663
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-21 21:11:17.402185
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-21 21:11:42.983324
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-21 21:11:55.434739
b''

Starting computation for size 1024
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-21 21:12:26.728149
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-21 21:12:39.638771
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-21 21:12:52.547136
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-21 21:13:22.512618
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-21 21:13:39.531781
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-21 21:14:15.586511
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-21 21:14:28.424837
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-21 21:15:01.127989
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-21 21:15:14.182995
b''

Starting computation for size [1024, 1024, 2048]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-21 21:15:42.648972
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-21 21:16:03.869852
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-21 21:16:19.730920
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-21 21:17:07.045514
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-21 21:17:23.620289
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-21 21:18:13.960446
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-21 21:18:32.462665
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-21 21:19:15.922188
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-21 21:19:38.109605
b''

Starting computation for size [1024, 2048, 2048]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-21 21:20:24.722388
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-21 21:20:47.109728
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-21 21:21:08.894342
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-21 21:22:08.882537
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-21 21:22:30.904967
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-21 21:23:30.326638
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-21 21:23:51.943166
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-21 21:25:11.280230
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-21 21:25:33.240305
b''

Starting computation for size 2048
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-21 21:26:56.615323
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-21 21:27:30.698020
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-21 21:28:04.711551
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-21 21:29:39.952251
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-21 21:30:14.339770
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-21 21:31:59.466406
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-21 21:32:33.171860
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-21 21:34:07.463603
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-21 21:34:41.712386
b''

Starting computation for size 128
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-21 21:36:27.649526
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-21 21:36:39.157341
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-21 21:36:50.197984
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-21 21:37:03.291900
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-21 21:37:14.191795
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-21 21:37:24.231559
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-21 21:37:34.766234
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-21 21:37:46.239214
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-21 21:37:56.129637
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

Starting computation for size 256
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-21 21:38:06.158491
b'Result (avg): 5.19278e-09\nResult (max): 5.37366e-08\n'

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-21 21:38:16.735640
b'Result (avg): 5.19278e-09\nResult (max): 5.37366e-08\n'

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-21 21:38:27.667101
b'Result (avg): 5.19278e-09\nResult (max): 5.37366e-08\n'

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-21 21:38:47.150724
b'Result (avg): 5.19278e-09\nResult (max): 5.37366e-08\n'

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-21 21:38:57.081703
b'Result (avg): 5.19278e-09\nResult (max): 5.37366e-08\n'

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-21 21:39:06.921135
b'Result (avg): 5.19278e-09\nResult (max): 5.37366e-08\n'

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-21 21:39:16.822653
b'Result (avg): 5.45133e-09\nResult (max): 6.01731e-08\n'

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-21 21:39:26.726048
b'Result (avg): 5.19278e-09\nResult (max): 5.37366e-08\n'

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-21 21:39:37.047186
b'Result (avg): 5.19278e-09\nResult (max): 5.37366e-08\n'

Starting computation for size 512
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-21 21:39:53.130891
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-21 21:40:04.573494
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-21 21:40:14.628681
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-21 21:40:26.950127
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-21 21:40:41.902198
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-21 21:40:57.550428
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-21 21:41:07.400331
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-21 21:41:17.557670
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-21 21:41:27.585703
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

Starting computation for size 1024
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-21 21:41:37.635803
b'Result (avg): 7.69766e-07\nResult (max): 9.22813e-06\n'

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-21 21:41:50.182556
b'Result (avg): 7.69766e-07\nResult (max): 9.22813e-06\n'

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-21 21:42:02.612543
b'Result (avg): 7.69766e-07\nResult (max): 9.22813e-06\n'

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-21 21:42:15.968183
b'Result (avg): 7.69766e-07\nResult (max): 9.22813e-06\n'

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-21 21:42:28.417216
b'Result (avg): 7.69766e-07\nResult (max): 9.22813e-06\n'

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-21 21:42:41.903884
b'Result (avg): 7.69766e-07\nResult (max): 9.22813e-06\n'

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-21 21:42:54.370660
b'Result (avg): 7.67725e-07\nResult (max): 9.16082e-06\n'

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-21 21:43:07.253740
b'Result (avg): 7.69766e-07\nResult (max): 9.22813e-06\n'

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-21 21:43:19.643241
b'Result (avg): 7.69766e-07\nResult (max): 9.22813e-06\n'

Starting computation for size 2048
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-21 21:43:38.041857
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-21 21:44:11.798656
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-21 21:44:46.028258
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-21 21:45:26.452268
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-21 21:46:06.396497
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-21 21:46:44.076264
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-21 21:47:17.824275
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-21 21:47:53.935110
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 1 --iterations 0 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-21 21:48:30.281266
b'Result (avg): -nan\nResult (max): -nan\n'

Pencil Default Inverse
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1585886] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1585886] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1586160] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1586160] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1586684] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1586684] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1586971] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1586971] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1587242] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1587242] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1587518] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1587518] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1587807] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1587807] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1588078] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1588078] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1588363] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1588363] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1588639] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1588639] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1588915] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1588915] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1589448] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1589448] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1589724] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1589724] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1590013] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1590013] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1590286] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1590286] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1590560] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1590560] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1590841] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1590841] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1591115] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1591115] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1591390] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1591390] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1591673] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1591673] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1592194] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1592194] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1592467] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1592467] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1592754] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1592754] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1593030] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1593030] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1593303] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1593303] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1593590] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1593590] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1593864] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1593864] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1594139] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1594139] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1594420] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1594420] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1594941] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1594941] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1595217] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1595217] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1595505] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1595505] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1595778] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1595778] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1596054] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1596054] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1596337] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1596337] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1596609] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1596609] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1596897] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1596897] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1597172] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1597172] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1597693] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1597693] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1597982] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1597982] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1598255] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1598255] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1598540] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1598540] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1598813] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1598813] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1599089] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1599089] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1599370] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1599370] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1599645] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1599645] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1599918] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1599918] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1600457] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1600457] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1600726] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1600726] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1601009] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1601009] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1601285] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1601285] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1601560] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1601560] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1601842] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1601842] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1602113] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1602113] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1602388] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1602388] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1602678] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1602678] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1603204] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1603204] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1603492] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1603492] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1603767] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1603767] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1604054] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1604054] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1604327] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1604327] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1604616] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1604616] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1604889] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1604889] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1605164] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1605164] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1605491] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1605491] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1606027] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1606027] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1606307] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1606307] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1606596] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1606596] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1606882] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1606882] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1607159] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1607159] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1607432] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1607432] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1607715] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1607715] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1607988] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1607988] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1608282] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1608282] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1608808] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1608808] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1609096] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1609096] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1609385] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1609385] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1609684] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1609684] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1609962] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1609962] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1610252] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1610252] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1610527] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1610527] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1610800] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1610800] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1611087] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1611087] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1611611] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1611611] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1611924] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1611924] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1612214] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1612214] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1612516] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1612516] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1612806] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1612806] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1613094] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1613094] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1613367] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1613367] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1613665] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1613665] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1613953] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1613953] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1614477] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1614477] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1614794] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1614794] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1615069] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1615069] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1615387] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1615387] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1615674] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1615674] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1615948] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1615948] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1616234] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1616234] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1616528] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1616528] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1616804] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1616804] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1617346] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1617346] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1617691] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1617691] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1617980] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1617980] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1618323] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1618323] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1618613] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1618613] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n514
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1618907] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1618907] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1619186] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1619186] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1619475] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1619475] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1619775] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1619775] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1620333] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1620333] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1620733] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1620733] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1621036] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1621036] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1621443] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1621443] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1621741] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1621741] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1622042] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1622042] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1622341] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1622341] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Starting computation for size 128
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-21 21:49:08.281283
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-21 21:49:18.713575
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-21 21:49:28.791487
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-21 21:49:39.212255
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-21 21:49:48.940118
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-21 21:49:59.215739
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-21 21:50:08.930909
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-21 21:50:19.338246
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-21 21:50:37.408379
b''

Starting computation for size [128, 128, 256]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-21 21:50:51.746918
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-21 21:51:01.497965
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-21 21:51:12.199999
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-21 21:51:25.430038
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-21 21:51:37.693076
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-21 21:51:47.783200
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-21 21:51:57.399612
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-21 21:52:07.015193
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-21 21:52:16.613089
b''

Starting computation for size [128, 256, 256]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-21 21:52:26.246838
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-21 21:52:35.959480
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-21 21:52:46.948195
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-21 21:52:58.820189
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-21 21:53:08.364074
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-21 21:53:18.959242
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-21 21:53:28.563879
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-21 21:53:38.414269
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-21 21:53:47.983625
b''

Starting computation for size 256
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-21 21:53:57.869933
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-21 21:54:07.490243
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-21 21:54:17.234045
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-21 21:54:29.986202
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-21 21:54:39.532306
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-21 21:54:51.194170
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-21 21:55:00.774075
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-21 21:55:10.521805
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-21 21:55:20.158632
b''

Starting computation for size [256, 256, 512]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-21 21:55:38.213630
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-21 21:55:48.119004
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-21 21:55:58.027375
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-21 21:56:10.366158
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-21 21:56:20.832758
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-21 21:56:33.560244
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-21 21:56:43.675962
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-21 21:56:53.929868
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-21 21:57:04.221710
b''

Starting computation for size [256, 512, 512]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-21 21:57:15.349420
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-21 21:57:28.259874
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-21 21:57:41.186866
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-21 21:57:55.447300
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-21 21:58:05.837879
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-21 21:58:20.649228
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-21 21:58:30.995778
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-21 21:58:41.273204
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-21 21:58:51.457563
b''

Starting computation for size 512
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-21 21:59:01.793155
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-21 21:59:13.945818
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-21 21:59:30.338703
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-21 21:59:47.890062
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-21 21:59:57.950470
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-21 22:00:17.599751
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-21 22:00:29.344529
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-21 22:00:39.829479
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-21 22:00:49.903869
b''

Starting computation for size [512, 512, 1024]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-21 22:01:00.116356
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-21 22:01:17.213706
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-21 22:01:36.919129
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-21 22:01:58.367271
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-21 22:02:09.992021
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-21 22:02:33.674708
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-21 22:02:45.077846
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-21 22:02:57.056789
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-21 22:03:09.564267
b''

Starting computation for size [512, 1024, 1024]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-21 22:03:24.546061
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-21 22:03:42.787351
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-21 22:03:55.112361
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-21 22:04:23.540271
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-21 22:04:37.260014
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-21 22:05:06.110058
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-21 22:05:19.287018
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-21 22:05:35.267743
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-21 22:05:47.429021
b''

Starting computation for size 1024
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-21 22:05:59.745713
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-21 22:06:19.044387
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-21 22:06:33.071872
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-21 22:07:19.516007
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-21 22:07:36.788894
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-21 22:08:22.922197
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-21 22:08:53.128943
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-21 22:09:07.129673
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-21 22:09:20.999855
b''

Starting computation for size [1024, 1024, 2048]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-21 22:09:47.721144
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-21 22:10:05.163664
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-21 22:10:22.573113
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-21 22:11:13.812807
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-21 22:11:31.725800
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-21 22:12:27.110356
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-21 22:12:45.469243
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-21 22:13:02.796197
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-21 22:13:21.159821
b''

Starting computation for size [1024, 2048, 2048]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-21 22:13:40.611444
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-21 22:14:03.722851
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-21 22:14:27.000712
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-21 22:15:56.347400
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-21 22:16:19.484014
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-21 22:17:47.312152
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-21 22:18:12.053485
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-21 22:18:35.329337
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-21 22:18:58.473232
b''

Starting computation for size 2048
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-21 22:19:23.271598
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-21 22:20:04.465131
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-21 22:20:42.494608
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-21 22:23:20.733030
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-21 22:23:58.523621
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-21 22:26:36.837281
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-21 22:27:14.381253
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-21 22:27:54.495869
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-21 22:28:31.269431
b''

Pencil Opt1 Inverse
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1622653] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1622653] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1622929] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1622929] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1623463] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1623463] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1623736] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1623736] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1624011] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1624011] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1624299] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1624299] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1624822] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1624822] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1625111] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1625111] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1625383] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1625383] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1625655] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1625655] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1625938] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1625938] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1626459] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1626459] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1626735] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1626735] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1627020] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1627020] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1627292] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1627292] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1627830] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1627830] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1628103] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1628103] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n514
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1628376] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1628376] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1628661] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1628661] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1628934] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1628934] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1629469] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1629469] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1629745] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1629745] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1630018] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1630018] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1630307] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1630307] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1630830] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1630830] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1631118] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1631118] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1631393] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1631393] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1631667] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1631667] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1631952] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1631952] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1632474] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1632474] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1632747] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1632747] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1633038] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1633038] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1633314] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1633314] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1633848] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1633848] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1634121] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1634121] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1634396] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1634396] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1634682] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1634682] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1634955] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1634955] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1635479] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1635479] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1635769] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1635769] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1636045] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1636045] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1636328] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1636328] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1636849] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1636849] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1637136] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1637136] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1637415] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1637415] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1637704] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1637704] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1637975] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1637975] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1638500] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1638500] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1638784] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1638784] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1639059] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1639059] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1639344] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1639344] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1639863] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1639863] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1640154] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1640154] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1640433] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1640433] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1640706] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1640706] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1640991] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1640991] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1641510] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1641510] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1641797] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1641797] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1642068] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1642068] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1642359] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1642359] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1642880] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1642880] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1643158] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1643158] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1643434] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1643434] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1643707] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1643707] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1643992] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1643992] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1644512] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1644512] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1644804] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1644804] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1645079] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1645079] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1645367] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1645367] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1645891] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1645891] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1646183] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1646183] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1646458] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1646458] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1646745] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1646745] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1647022] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1647022] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1647553] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1647553] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1647831] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1647831] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1648112] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1648112] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1648392] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1648392] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1648931] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1648931] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1649207] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1649207] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1649491] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1649491] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1649782] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1649782] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1650057] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1650057] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1650580] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1650580] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1650875] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1650875] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1651162] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1651162] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1651453] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1651453] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1651976] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1651976] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1652259] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1652259] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1652535] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1652535] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1652823] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1652823] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1653098] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1653098] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1653635] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1653635] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1653935] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1653935] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1654222] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1654222] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1654518] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1654518] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1655060] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1655060] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1655348] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1655348] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1655622] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1655622] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1655915] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1655915] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1656210] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1656210] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1656748] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1656748] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1657048] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1657048] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1657342] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1657342] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1657658] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1657658] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1658198] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1658198] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1658489] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1658489] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1658769] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1658769] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1659080] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1659080] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1659380] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1659380] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1659922] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1659922] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1660363] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1660363] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n517
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1660715] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1660715] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1661063] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1661063] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1661607] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1661607] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1661920] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1661920] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n508.localdomain:1662217] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n508.localdomain:1662217] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Starting computation for size 128
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-21 22:29:13.021186
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-21 22:29:30.209480
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-21 22:29:41.901840
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-21 22:29:52.229033
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-21 22:30:03.258911
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-21 22:30:15.631216
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-21 22:30:32.303666
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-21 22:30:43.114301
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-21 22:30:53.467832
b''

Starting computation for size [128, 128, 256]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-21 22:31:03.503718
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-21 22:31:13.453054
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-21 22:31:23.435317
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-21 22:31:33.900717
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-21 22:31:43.838062
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-21 22:31:54.319659
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-21 22:32:05.996504
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-21 22:32:21.895516
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-21 22:32:32.063328
b''

Starting computation for size [128, 256, 256]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-21 22:32:42.631722
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-21 22:32:54.670046
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-21 22:33:07.417068
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-21 22:33:17.942040
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-21 22:33:28.574344
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-21 22:33:39.041674
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-21 22:33:51.712865
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-21 22:34:10.602383
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-21 22:34:20.857180
b''

Starting computation for size 256
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-21 22:34:31.650320
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-21 22:34:41.975333
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-21 22:34:52.296400
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-21 22:35:03.865847
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-21 22:35:16.608991
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-21 22:35:31.250213
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-21 22:35:41.568976
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-21 22:35:52.559636
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-21 22:36:03.548516
b''

Starting computation for size [256, 256, 512]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-21 22:36:14.165278
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-21 22:36:24.767893
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-21 22:36:35.469955
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-21 22:36:50.911057
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-21 22:37:02.130600
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-21 22:37:14.729321
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-21 22:37:25.031486
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-21 22:37:39.514648
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-21 22:38:01.424266
b''

Starting computation for size [256, 512, 512]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-21 22:38:12.544315
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-21 22:38:23.193938
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-21 22:38:35.678273
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-21 22:38:50.410857
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-21 22:39:01.018601
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-21 22:39:13.751244
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-21 22:39:24.387134
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-21 22:39:38.227579
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-21 22:39:52.551268
b''

Starting computation for size 512
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-21 22:40:04.459569
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-21 22:40:15.251628
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-21 22:40:26.267359
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-21 22:40:43.716274
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-21 22:40:54.600125
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-21 22:41:09.244312
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-21 22:41:20.071586
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-21 22:41:39.053034
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-21 22:41:51.120679
b''

Starting computation for size [512, 512, 1024]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-21 22:42:03.837582
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-21 22:42:15.226790
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-21 22:42:26.441116
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-21 22:42:45.978139
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-21 22:43:02.796314
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-21 22:43:21.363490
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-21 22:43:33.240013
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-21 22:43:46.613814
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-21 22:43:58.507308
b''

Starting computation for size [512, 1024, 1024]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-21 22:44:11.972850
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-21 22:44:26.410295
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-21 22:44:38.632076
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-21 22:44:57.232979
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-21 22:45:09.497847
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-21 22:45:28.888671
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-21 22:45:47.261945
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-21 22:46:01.886545
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-21 22:46:13.746066
b''

Starting computation for size 1024
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-21 22:46:39.047453
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-21 22:46:52.431610
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-21 22:47:05.627994
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-21 22:47:30.541033
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-21 22:47:46.167207
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-21 22:48:14.812711
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-21 22:48:27.813228
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-21 22:48:43.132079
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-21 22:48:56.126726
b''

Starting computation for size [1024, 1024, 2048]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-21 22:49:17.737611
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-21 22:49:33.728932
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-21 22:49:49.679943
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-21 22:50:30.413129
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-21 22:50:46.444421
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-21 22:51:30.737321
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-21 22:51:47.104246
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-21 22:52:07.696612
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-21 22:52:23.820375
b''

Starting computation for size [1024, 2048, 2048]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-21 22:52:53.281818
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-21 22:53:15.313727
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-21 22:53:37.331509
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-21 22:54:26.195270
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-21 22:54:48.132527
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-21 22:55:45.361733
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-21 22:56:07.531815
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-21 22:56:39.153426
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-21 22:57:01.087116
b''

Starting computation for size 2048
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-21 22:57:41.987363
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Streams -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-21 22:58:27.131060
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-21 22:59:01.444867
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 Sync -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-21 23:00:35.539925
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 All2All -snd1 MPI_Type -comm2 Peer2Peer -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-21 23:01:09.990250
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 Streams --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-21 23:02:47.821315
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 Peer2Peer -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-21 23:03:25.664629
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 Sync --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-21 23:04:07.962743
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 pencil -comm1 Peer2Peer -snd1 Sync -comm2 All2All -snd2 MPI_Type --warmup-rounds 10 --iterations 20 --double_prec --cuda_aware -p1 8 -p2 8 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-21 23:04:47.299019
b''

all done

============================= JOB FEEDBACK =============================

NodeName=uc2n[508-511,514-517]
Job ID: 19803940
Cluster: uc2
User/Group: st_st160727/st_us-051200
State: COMPLETED (exit code 0)
Nodes: 8
Cores per node: 80
CPU Utilized: 4-11:25:38
CPU Efficiency: 5.43% of 82-11:33:20 core-walltime
Job Wall-clock time: 03:05:35
Memory Utilized: 23.27 GB
Memory Efficiency: 0.00% of 0.00 MB
