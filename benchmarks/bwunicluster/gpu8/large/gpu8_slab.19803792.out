Modules loaded
uc2n507
uc2n507
uc2n507
uc2n507
uc2n507
uc2n507
uc2n507
uc2n507
uc2n508
uc2n508
uc2n508
uc2n508
uc2n508
uc2n508
uc2n508
uc2n508
uc2n509
uc2n509
uc2n509
uc2n509
uc2n509
uc2n509
uc2n509
uc2n509
uc2n510
uc2n510
uc2n510
uc2n510
uc2n510
uc2n510
uc2n510
uc2n510
uc2n511
uc2n511
uc2n511
uc2n511
uc2n511
uc2n511
uc2n511
uc2n511
uc2n514
uc2n514
uc2n514
uc2n514
uc2n514
uc2n514
uc2n514
uc2n514
uc2n515
uc2n515
uc2n515
uc2n515
uc2n515
uc2n515
uc2n515
uc2n515
uc2n516
uc2n516
uc2n516
uc2n516
uc2n516
uc2n516
uc2n516
uc2n516
64: uc2n507 uc2n507 uc2n507 uc2n507 uc2n507 uc2n507 uc2n507 uc2n507 uc2n508 uc2n508 uc2n508 uc2n508 uc2n508 uc2n508 uc2n508 uc2n508 uc2n509 uc2n509 uc2n509 uc2n509 uc2n509 uc2n509 uc2n509 uc2n509 uc2n510 uc2n510 uc2n510 uc2n510 uc2n510 uc2n510 uc2n510 uc2n510 uc2n511 uc2n511 uc2n511 uc2n511 uc2n511 uc2n511 uc2n511 uc2n511 uc2n514 uc2n514 uc2n514 uc2n514 uc2n514 uc2n514 uc2n514 uc2n514 uc2n515 uc2n515 uc2n515 uc2n515 uc2n515 uc2n515 uc2n515 uc2n515 uc2n516 uc2n516 uc2n516 uc2n516 uc2n516 uc2n516 uc2n516 uc2n516
start building
-- The CUDA compiler identification is NVIDIA 11.0.194
-- The CXX compiler identification is GNU 8.3.1
-- Detecting CUDA compiler ABI info
-- Detecting CUDA compiler ABI info - done
-- Check for working CUDA compiler: /opt/bwhpc/common/devel/cuda/11.0/bin/nvcc - skipped
-- Detecting CUDA compile features
-- Detecting CUDA compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/g++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found MPI_CXX: /pfs/data5/software_uc2/bwhpc/common/mpi/openmpi/4.1.0-gnu-8.3.1/lib/libmpi.so (found version "3.1") 
-- Found MPI: TRUE (found version "3.1")  
-- Found CUDAToolkit: /opt/bwhpc/common/devel/cuda/11.0/include (found version "11.0.194") 
-- Looking for C++ include pthread.h
-- Looking for C++ include pthread.h - found
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed
-- Looking for pthread_create in pthreads
-- Looking for pthread_create in pthreads - not found
-- Looking for pthread_create in pthread
-- Looking for pthread_create in pthread - found
-- Found Threads: TRUE  
-- Configuring done
-- Generating done
-- Build files have been written to: /home/st/st_us-051200/st_st160727/DistributedFFT/build_gpu8
Scanning dependencies of target timer
[  3%] Building CXX object CMakeFiles/timer.dir/src/timer.cpp.o
[  6%] Linking CXX shared library libtimer.so
[  6%] Built target timer
Scanning dependencies of target test_base
[  9%] Building CUDA object CMakeFiles/test_base.dir/tests/src/base.cu.o
nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 12%] Linking CUDA shared library libtest_base.so
[ 12%] Built target test_base
Scanning dependencies of target mpicufft
[ 15%] Building CXX object CMakeFiles/mpicufft.dir/src/mpicufft.cpp.o
[ 18%] Linking CXX shared library libmpicufft.so
[ 18%] Built target mpicufft
Scanning dependencies of target pencil_decomp
[ 21%] Building CXX object CMakeFiles/pencil_decomp.dir/src/pencil/mpicufft_pencil.cpp.o
[ 24%] Building CXX object CMakeFiles/pencil_decomp.dir/src/pencil/mpicufft_pencil_opt1.cpp.o
[ 27%] Linking CXX shared library libpencil_decomp.so
[ 27%] Built target pencil_decomp
Scanning dependencies of target pencil_tests
[ 30%] Building CUDA object CMakeFiles/pencil_tests.dir/tests/src/pencil/base.cu.o
nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 33%] Building CUDA object CMakeFiles/pencil_tests.dir/tests/src/pencil/random_dist_1D.cu.o
nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 36%] Building CUDA object CMakeFiles/pencil_tests.dir/tests/src/pencil/random_dist_2D.cu.o
nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 39%] Building CUDA object CMakeFiles/pencil_tests.dir/tests/src/pencil/random_dist_3D.cu.o
nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 42%] Linking CUDA shared library libpencil_tests.so
[ 42%] Built target pencil_tests
Scanning dependencies of target pencil
[ 45%] Building CXX object CMakeFiles/pencil.dir/tests/src/pencil/main.cpp.o
[ 48%] Linking CXX executable pencil
[ 48%] Built target pencil
Scanning dependencies of target slab_decomp
[ 51%] Building CXX object CMakeFiles/slab_decomp.dir/src/slab/default/mpicufft_slab.cpp.o
[ 54%] Building CXX object CMakeFiles/slab_decomp.dir/src/slab/default/mpicufft_slab_opt1.cpp.o
[ 57%] Building CXX object CMakeFiles/slab_decomp.dir/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp.o
[ 60%] Building CXX object CMakeFiles/slab_decomp.dir/src/slab/z_then_yx/mpicufft_slab_z_then_yx_opt1.cpp.o
[ 63%] Building CXX object CMakeFiles/slab_decomp.dir/src/slab/y_then_zx/mpicufft_slab_y_then_zx.cpp.o
[ 66%] Linking CXX shared library libslab_decomp.so
[ 66%] Built target slab_decomp
Scanning dependencies of target slab_tests
[ 69%] Building CUDA object CMakeFiles/slab_tests.dir/tests/src/slab/base.cu.o
nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 72%] Building CUDA object CMakeFiles/slab_tests.dir/tests/src/slab/random_dist_default.cu.o
nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 75%] Building CUDA object CMakeFiles/slab_tests.dir/tests/src/slab/random_dist_y_then_zx.cu.o
nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 78%] Building CUDA object CMakeFiles/slab_tests.dir/tests/src/slab/random_dist_z_then_yx.cu.o
nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 81%] Linking CUDA shared library libslab_tests.so
[ 81%] Built target slab_tests
Scanning dependencies of target slab
[ 84%] Building CXX object CMakeFiles/slab.dir/tests/src/slab/main.cpp.o
[ 87%] Linking CXX executable slab
[ 87%] Built target slab
Scanning dependencies of target reference_tests
[ 90%] Building CUDA object CMakeFiles/reference_tests.dir/tests/src/reference/reference.cu.o
nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 93%] Linking CUDA shared library libreference_tests.so
[ 93%] Built target reference_tests
Scanning dependencies of target reference
[ 96%] Building CXX object CMakeFiles/reference.dir/tests/src/reference/main.cpp.o
[100%] Linking CXX executable reference
[100%] Built target reference
finished building
start python script
Starting on HOST64
-----------------------------------------------------------------------------
Slab 2D->1D default
Traceback (most recent call last):
  File "launch.py", line 271, in <module>
    main()
  File "launch.py", line 239, in main
    data["global_test_settings"][program_args[i]] = program_args[i+1]
IndexError: list index out of range
Slab 2D->1D opt1
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n507
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1020910] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1020910] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1021189] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1021189] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1021460] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1021460] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1021983] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1021983] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1022268] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1022268] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1022544] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1022544] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1022818] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1022818] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1023104] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1023104] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1023375] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1023375] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1023649] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1023649] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1023941] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1023941] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1024216] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1024216] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1024499] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1024499] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1025020] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1025020] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1025291] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1025291] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n514
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1025576] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1025576] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1025848] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1025848] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1026123] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1026123] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1026413] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1026413] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1026689] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1026689] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1026977] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1026977] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1027252] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1027252] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n514
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1027527] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1027527] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1028061] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1028061] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1028393] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1028393] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1028685] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1028685] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1028965] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1028965] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n514
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1029249] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1029249] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1029525] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1029525] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1029812] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1029812] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1030088] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1030088] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1030377] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1030377] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1030651] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1030651] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1031176] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1031176] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1031460] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1031460] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1031731] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1031731] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1032025] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1032025] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1032298] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1032298] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1032583] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1032583] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n514
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1032859] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1032859] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1033149] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1033149] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1033423] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1033423] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1033711] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1033711] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1034232] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1034232] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1034507] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1034507] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1034800] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1034800] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1035087] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1035087] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1035360] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1035360] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1035648] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1035648] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1035921] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1035921] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1036217] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1036217] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1036491] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1036491] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1036776] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1036776] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1037299] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1037299] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1037575] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1037575] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1037863] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1037863] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1038162] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1038162] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1038446] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1038446] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1038720] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1038720] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1038993] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1038993] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1039305] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1039305] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1039584] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1039584] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1039871] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1039871] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1040394] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1040394] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1040668] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1040668] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n514
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1040952] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1040952] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1041258] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1041258] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1041546] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1041546] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1041819] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1041819] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1042094] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1042094] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1042403] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1042403] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1042676] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1042676] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1042949] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1042949] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1043488] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1043488] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1043762] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1043762] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1044045] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1044045] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1044368] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1044368] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1044643] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1044643] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1044926] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1044926] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1045205] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1045205] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1045538] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1045538] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1045812] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1045812] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1046086] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1046086] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1046622] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1046622] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1046897] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1046897] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1047181] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1047181] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1047549] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1047549] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1047826] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1047826] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1048116] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1048116] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1048391] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1048391] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1048755] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1048755] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1049042] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1049042] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1049317] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1049317] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1049859] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1049859] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1050134] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1050134] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1050411] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1050411] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1050777] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1050777] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1051061] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1051061] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1051336] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1051336] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1051627] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1051627] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1051998] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1051998] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1052286] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1052286] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1052564] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1052564] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1053108] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1053108] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1053385] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1053385] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1053673] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1053673] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1054119] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1054119] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1054416] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1054416] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1054690] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1054690] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1054978] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1054978] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1055429] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1055429] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1055725] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1055725] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1056008] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1056008] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1056550] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1056550] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1056840] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1056840] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1057133] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1057133] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1057810] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1057810] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1058099] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1058099] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1058379] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1058379] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1058671] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1058671] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1059357] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1059357] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1059671] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1059671] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1059952] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1059952] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1060512] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1060512] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1060805] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1060805] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1061104] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1061104] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n507
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1061744] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1061744] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1062043] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1062043] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n514
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1062337] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1062337] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1062651] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1062651] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1063278] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1063278] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1063566] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1063566] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1063838] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1063838] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1064137] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1064137] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1064439] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1064439] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1064710] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1064710] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1064995] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1064995] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1065273] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1065273] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1065544] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1065544] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1065833] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1065833] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1066108] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1066108] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1066382] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1066382] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1066666] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1066666] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1066963] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1066963] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1067253] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1067253] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1067538] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1067538] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1067811] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1067811] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1068086] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1068086] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1068375] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1068375] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1068645] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1068645] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1068931] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1068931] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n514
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1069204] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1069204] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1069477] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1069477] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1069786] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1069786] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1070078] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1070078] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1070351] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1070351] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1070640] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1070640] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1070915] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1070915] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1071191] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1071191] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1071475] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1071475] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1071746] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1071746] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1072034] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1072034] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1072309] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1072309] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1072621] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1072621] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1072916] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1072916] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1073192] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1073192] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n514
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1073483] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1073483] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1073760] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1073760] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1074045] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1074045] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1074320] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1074320] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1074617] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1074617] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1074909] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1074909] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1075205] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1075205] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1075540] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1075540] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1075849] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1075849] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1076148] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1076148] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1076449] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1076449] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1076759] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1076759] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1077054] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1077054] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1077350] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1077350] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Starting computation for size 128
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 128
2021-09-19 20:01:38.403812
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 128
2021-09-19 20:01:51.209721
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 128
2021-09-19 20:02:01.084228
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 128
2021-09-19 20:02:10.140159
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 128
2021-09-19 20:02:19.974484
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 128
2021-09-19 20:02:29.062614
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 128
2021-09-19 20:02:40.683563
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 128
2021-09-19 20:02:50.498967
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 128
2021-09-19 20:03:01.229097
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 128
2021-09-19 20:03:11.198388
b''

Starting computation for size [128, 128, 256]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 256
2021-09-19 20:03:23.845740
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 256
2021-09-19 20:03:33.300679
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 256
2021-09-19 20:03:43.380623
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 256
2021-09-19 20:03:52.878315
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 256
2021-09-19 20:04:03.967545
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 256
2021-09-19 20:04:13.455431
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 256
2021-09-19 20:04:27.396378
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 256
2021-09-19 20:04:37.985270
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 256
2021-09-19 20:04:49.006113
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 256
2021-09-19 20:04:59.195798
b''

Starting computation for size [128, 256, 256]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 256 -nz 256
2021-09-19 20:05:13.401014
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 256 -nz 256
2021-09-19 20:05:26.546812
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 256 -nz 256
2021-09-19 20:05:38.652482
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 256 -nz 256
2021-09-19 20:05:53.067947
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 256 -nz 256
2021-09-19 20:06:05.204719
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 256 -nz 256
2021-09-19 20:06:17.799903
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 256 -nz 256
2021-09-19 20:06:35.821360
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 256 -nz 256
2021-09-19 20:06:48.289781
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 256 -nz 256
2021-09-19 20:07:01.032312
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 256 -nz 256
2021-09-19 20:07:13.305424
b''

Starting computation for size 256
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 256
2021-09-19 20:07:30.999917
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 256
2021-09-19 20:07:44.999270
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 256
2021-09-19 20:07:56.927339
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 256
2021-09-19 20:08:10.571339
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 256
2021-09-19 20:08:22.558867
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 256
2021-09-19 20:08:34.699035
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 256
2021-09-19 20:08:52.595234
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 256
2021-09-19 20:09:05.659329
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 256
2021-09-19 20:09:17.633423
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 256
2021-09-19 20:09:29.852099
b''

Starting computation for size [256, 256, 512]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 512
2021-09-19 20:09:47.697203
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 512
2021-09-19 20:10:01.028586
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 512
2021-09-19 20:10:14.486562
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 512
2021-09-19 20:10:26.589694
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 512
2021-09-19 20:10:40.261574
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 512
2021-09-19 20:10:53.820510
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 512
2021-09-19 20:11:19.174557
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 512
2021-09-19 20:11:31.796147
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 512
2021-09-19 20:11:43.600083
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 512
2021-09-19 20:11:57.010883
b''

Starting computation for size [256, 512, 512]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 512 -nz 512
2021-09-19 20:12:22.083889
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 512 -nz 512
2021-09-19 20:12:34.680346
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 512 -nz 512
2021-09-19 20:12:47.503390
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 512 -nz 512
2021-09-19 20:12:58.022268
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 512 -nz 512
2021-09-19 20:13:11.409735
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 512 -nz 512
2021-09-19 20:13:24.596480
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 512 -nz 512
2021-09-19 20:14:02.871470
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 512 -nz 512
2021-09-19 20:14:15.227924
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 512 -nz 512
2021-09-19 20:14:26.889115
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 512 -nz 512
2021-09-19 20:14:40.197030
b''

Starting computation for size 512
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 512
2021-09-19 20:15:18.575178
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 512
2021-09-19 20:15:30.966854
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 512
2021-09-19 20:15:43.528543
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 512
2021-09-19 20:15:54.195886
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 512
2021-09-19 20:16:07.371678
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 512
2021-09-19 20:16:20.674067
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 512
2021-09-19 20:17:00.813847
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 512
2021-09-19 20:17:13.826669
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 512
2021-09-19 20:17:25.598807
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 512
2021-09-19 20:17:38.811701
b''

Starting computation for size [512, 512, 1024]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-19 20:18:17.271897
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-19 20:18:29.394240
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-19 20:18:40.901883
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-19 20:18:51.983359
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-19 20:19:03.675248
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-19 20:19:16.313455
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-19 20:20:25.400515
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-19 20:20:38.740025
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-19 20:20:50.916946
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-19 20:21:03.315803
b''

Starting computation for size [512, 1024, 1024]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-19 20:22:14.485979
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-19 20:22:27.216869
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-19 20:22:39.846938
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-19 20:22:51.699296
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-19 20:23:03.664918
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-19 20:23:18.044793
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-19 20:25:20.106247
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-19 20:25:34.447002
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-19 20:25:46.935280
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-19 20:26:00.191258
b''

Starting computation for size 1024
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-19 20:28:04.324978
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-19 20:28:18.884551
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-19 20:28:31.623420
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-19 20:28:45.585644
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-19 20:28:58.417007
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-19 20:29:13.371199
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-19 20:31:12.286265
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-19 20:31:29.710321
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-19 20:31:43.334351
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-19 20:31:58.282336
b''

Starting computation for size [1024, 1024, 2048]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-19 20:33:58.278946
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-19 20:34:17.788830
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-19 20:34:31.839222
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-19 20:34:49.383052
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-19 20:35:03.466810
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-19 20:35:22.482392
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-19 20:39:03.309993
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-19 20:39:23.382854
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-19 20:39:38.788026
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-19 20:39:58.744130
b''

Starting computation for size [1024, 2048, 2048]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-19 20:43:43.332853
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-19 20:44:12.910517
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-19 20:44:30.850267
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-19 20:44:57.380363
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-19 20:45:15.722753
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-19 20:45:44.843476
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-19 20:52:52.978948
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-19 20:53:21.799279
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-19 20:53:41.819749
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-19 20:54:11.056480
b''

Starting computation for size 2048
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-19 21:01:26.666941
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-19 21:02:17.196391
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-19 21:02:43.638767
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-19 21:03:27.853627
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-19 21:03:54.140716
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-19 21:04:40.224434
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-19 21:11:54.395894
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-19 21:12:41.680989
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-19 21:13:10.164852
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-19 21:13:56.504120
b''

Starting computation for size 128
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 21:21:14.686932
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 21:21:26.246346
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 21:21:36.359759
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 21:21:46.389008
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 21:21:56.880389
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 21:22:06.751737
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 21:22:17.156886
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 21:22:27.846205
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 21:22:38.915321
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 21:22:48.954058
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

Starting computation for size 256
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 21:22:59.542223
b'Result (avg): 5.19278e-09\nResult (max): 5.37366e-08\n'

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 21:23:09.732956
b'Result (avg): 5.19278e-09\nResult (max): 5.37366e-08\n'

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 21:23:20.526664
b'Result (avg): 5.19278e-09\nResult (max): 5.37366e-08\n'

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 21:23:30.656372
b'Result (avg): 5.19278e-09\nResult (max): 5.37366e-08\n'

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 21:23:41.619669
b'Result (avg): 5.19278e-09\nResult (max): 5.37366e-08\n'

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 21:23:51.972096
b'Result (avg): 5.19278e-09\nResult (max): 5.37366e-08\n'

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 21:24:03.090272
b'Result (avg): 5.19278e-09\nResult (max): 5.37366e-08\n'

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 21:24:14.166834
b'Result (avg): 5.19278e-09\nResult (max): 5.37366e-08\n'

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 21:24:26.144532
b'Result (avg): 5.19278e-09\nResult (max): 5.37366e-08\n'

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 21:24:36.369886
b'Result (avg): 5.19278e-09\nResult (max): 5.37366e-08\n'

Starting computation for size 512
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 21:24:47.412766
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 21:24:57.774931
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 21:25:09.096855
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 21:25:19.551950
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 21:25:30.506458
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 21:25:41.362916
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 21:25:53.639787
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 21:26:05.143039
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 21:26:17.070880
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 21:26:27.658440
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

Starting computation for size 1024
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 21:26:39.790130
b'Result (avg): 7.69766e-07\nResult (max): 9.22813e-06\n'

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 21:26:52.817197
b'Result (avg): 7.69766e-07\nResult (max): 9.22813e-06\n'

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 21:27:06.364022
b'Result (avg): 7.69766e-07\nResult (max): 9.22813e-06\n'

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 21:27:18.885642
b'Result (avg): 7.69766e-07\nResult (max): 9.22813e-06\n'

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 21:27:32.306353
b'Result (avg): 7.69766e-07\nResult (max): 9.22813e-06\n'

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 21:27:45.313946
b'Result (avg): 7.69766e-07\nResult (max): 9.22813e-06\n'

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 21:28:03.009835
b'Result (avg): 7.69766e-07\nResult (max): 9.22813e-06\n'

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 21:28:16.776257
b'Result (avg): 7.69766e-07\nResult (max): 9.22813e-06\n'

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 21:28:30.874227
b'Result (avg): 7.69766e-07\nResult (max): 9.22813e-06\n'

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 21:28:44.361722
b'Result (avg): 7.69766e-07\nResult (max): 9.22813e-06\n'

Starting computation for size 2048
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 21:29:02.016075
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 21:29:35.776830
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 21:30:11.184885
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 21:30:47.888566
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 21:31:19.896847
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 21:31:54.423003
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 21:32:44.351979
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 21:33:18.990629
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 21:33:51.954247
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 21:34:26.464119
b'Result (avg): -nan\nResult (max): -nan\n'

Slab 1D->2D default
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1077659] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1077659] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1077941] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1077941] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1078216] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1078216] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1078737] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1078737] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1079270] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1079270] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1079545] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1079545] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1079819] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1079819] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1080109] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1080109] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1080380] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1080380] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1080665] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1080665] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1080943] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1080943] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1081218] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1081218] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1081501] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1081501] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1082026] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1082026] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1082547] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1082547] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1082834] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1082834] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1083111] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1083111] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1083386] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1083386] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1083671] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1083671] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1083944] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1083944] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1084236] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1084236] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1084509] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1084509] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1084788] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1084788] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1085322] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1085322] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1085847] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1085847] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1086130] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1086130] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1086405] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1086405] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1086690] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1086690] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1086968] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1086968] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1087243] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1087243] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1087530] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1087530] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1087811] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1087811] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1088093] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1088093] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1088628] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1088628] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1089151] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1089151] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1089434] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1089434] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1089710] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1089710] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1090001] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1090001] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1090276] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1090276] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1090551] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1090551] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1090838] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1090838] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1091109] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1091109] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1091394] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1091394] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1091919] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1091919] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1092442] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1092442] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1092731] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1092731] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1093003] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1093003] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1093293] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1093293] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1093564] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1093564] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1093837] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1093837] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1094124] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1094124] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1094393] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1094393] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1094684] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1094684] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1095205] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1095205] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1095729] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1095729] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1096013] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1096013] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1096292] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1096292] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1096578] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1096578] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1096849] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1096849] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1097138] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1097138] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1097415] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1097415] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1097699] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1097699] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1097974] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1097974] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1098496] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1098496] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1099028] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1099028] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1099300] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1099300] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1099594] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1099594] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1099884] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1099884] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1100159] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1100159] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1100436] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1100436] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1100726] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1100726] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1101015] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1101015] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1101289] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1101289] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1101811] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1101811] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1102342] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1102342] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1102618] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1102618] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1102913] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1102913] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1103202] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1103202] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1103475] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1103475] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n514
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1103748] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1103748] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1104041] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1104041] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1104326] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1104326] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1104601] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1104601] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1105134] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1105134] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1105655] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1105655] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1105940] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1105940] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1106241] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1106241] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1106530] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1106530] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1106810] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1106810] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1107084] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1107084] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1107395] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1107395] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1107686] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1107686] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1107961] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1107961] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1108492] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1108492] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1109015] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1109015] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1109290] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1109290] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1109630] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1109630] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1109916] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1109916] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1110191] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1110191] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1110484] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1110484] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1110833] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1110833] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1111109] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1111109] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1111397] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1111397] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n514
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1111965] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1111965] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1112499] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1112499] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1112775] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1112775] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1113112] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1113112] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1113404] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1113404] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1113679] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1113679] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1113965] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1113965] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1114311] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1114311] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1114603] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1114603] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1114892] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1114892] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1115419] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1115419] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1115956] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1115956] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1116250] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1116250] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1116646] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1116646] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1116938] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1116938] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1117232] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1117232] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1117522] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1117522] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1117923] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1117923] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1118244] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1118244] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1118538] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1118538] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1119083] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1119083] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1119620] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1119620] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1119934] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1119934] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1120452] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1120452] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1120756] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1120756] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1121047] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1121047] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1121360] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1121360] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1121893] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1121893] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1122175] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1122175] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1122450] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1122450] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1122749] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1122749] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1123052] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1123052] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1123327] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1123327] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1123602] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1123602] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1123888] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1123888] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1124163] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1124163] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1124438] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1124438] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1124720] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1124720] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1124995] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1124995] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1125268] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1125268] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1125579] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1125579] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1125868] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1125868] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1126143] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1126143] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1126426] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1126426] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1126701] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1126701] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1126991] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1126991] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1127266] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1127266] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1127541] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1127541] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1127831] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1127831] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1128106] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1128106] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1128405] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1128405] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1128704] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1128704] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1128979] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1128979] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1129256] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1129256] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1129539] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1129539] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1129815] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1129815] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1130090] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1130090] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1130395] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1130395] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1130670] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1130670] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1130953] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1130953] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1131255] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1131255] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1131556] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1131556] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1131831] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1131831] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1132123] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1132123] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1132399] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1132399] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1132686] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1132686] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1132961] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1132961] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1133257] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1133257] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1133553] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1133553] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1133846] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1133846] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1134172] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1134172] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1134487] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1134487] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1134792] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1134792] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1135117] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1135117] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1135410] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1135410] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1135710] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1135710] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1136009] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1136009] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Starting computation for size 128
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 128 -ny 128 -nz 128
2021-09-19 21:35:16.612425
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 128 -ny 128 -nz 128
2021-09-19 21:35:25.788992
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 128 -ny 128 -nz 128
2021-09-19 21:35:35.646206
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 128 -ny 128 -nz 128
2021-09-19 21:35:44.776274
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 128 -ny 128 -nz 128
2021-09-19 21:35:54.497024
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 128 -ny 128 -nz 128
2021-09-19 21:36:03.553517
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 128 -ny 128 -nz 128
2021-09-19 21:36:17.580760
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 128 -ny 128 -nz 128
2021-09-19 21:36:28.025616
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 128 -ny 128 -nz 128
2021-09-19 21:36:38.508471
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 128 -ny 128 -nz 128
2021-09-19 21:36:48.217901
b''

Starting computation for size [128, 128, 256]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 128 -ny 128 -nz 256
2021-09-19 21:37:02.198813
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 128 -ny 128 -nz 256
2021-09-19 21:37:12.597538
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 128 -ny 128 -nz 256
2021-09-19 21:37:22.743752
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 128 -ny 128 -nz 256
2021-09-19 21:37:31.868501
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 128 -ny 128 -nz 256
2021-09-19 21:37:41.662312
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 128 -ny 128 -nz 256
2021-09-19 21:37:51.375435
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 128 -ny 128 -nz 256
2021-09-19 21:38:04.694071
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 128 -ny 128 -nz 256
2021-09-19 21:38:15.552799
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 128 -ny 128 -nz 256
2021-09-19 21:38:27.322682
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 128 -ny 128 -nz 256
2021-09-19 21:38:38.048068
b''

Starting computation for size [128, 256, 256]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 128 -ny 256 -nz 256
2021-09-19 21:38:52.200539
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 128 -ny 256 -nz 256
2021-09-19 21:39:05.075884
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 128 -ny 256 -nz 256
2021-09-19 21:39:17.890701
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 128 -ny 256 -nz 256
2021-09-19 21:39:31.200204
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 128 -ny 256 -nz 256
2021-09-19 21:39:43.222942
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 128 -ny 256 -nz 256
2021-09-19 21:39:53.848289
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 128 -ny 256 -nz 256
2021-09-19 21:40:07.066247
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 128 -ny 256 -nz 256
2021-09-19 21:40:19.093715
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 128 -ny 256 -nz 256
2021-09-19 21:40:31.523691
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 128 -ny 256 -nz 256
2021-09-19 21:40:42.805147
b''

Starting computation for size 256
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 256 -ny 256 -nz 256
2021-09-19 21:40:56.237311
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 256 -ny 256 -nz 256
2021-09-19 21:41:09.222451
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 256 -ny 256 -nz 256
2021-09-19 21:41:20.850895
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 256 -ny 256 -nz 256
2021-09-19 21:41:35.108513
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 256 -ny 256 -nz 256
2021-09-19 21:41:45.934598
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 256 -ny 256 -nz 256
2021-09-19 21:41:56.031845
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 256 -ny 256 -nz 256
2021-09-19 21:42:10.564060
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 256 -ny 256 -nz 256
2021-09-19 21:42:23.039395
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 256 -ny 256 -nz 256
2021-09-19 21:42:34.335262
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 256 -ny 256 -nz 256
2021-09-19 21:42:44.004507
b''

Starting computation for size [256, 256, 512]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 256 -ny 256 -nz 512
2021-09-19 21:42:58.919807
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 256 -ny 256 -nz 512
2021-09-19 21:43:11.610933
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 256 -ny 256 -nz 512
2021-09-19 21:43:24.820323
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 256 -ny 256 -nz 512
2021-09-19 21:43:35.539047
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 256 -ny 256 -nz 512
2021-09-19 21:43:46.691815
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 256 -ny 256 -nz 512
2021-09-19 21:43:57.154752
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 256 -ny 256 -nz 512
2021-09-19 21:44:11.579511
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 256 -ny 256 -nz 512
2021-09-19 21:44:23.413691
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 256 -ny 256 -nz 512
2021-09-19 21:44:34.536404
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 256 -ny 256 -nz 512
2021-09-19 21:44:44.008422
b''

Starting computation for size [256, 512, 512]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 256 -ny 512 -nz 512
2021-09-19 21:44:58.867847
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 256 -ny 512 -nz 512
2021-09-19 21:45:11.387443
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 256 -ny 512 -nz 512
2021-09-19 21:45:24.448437
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 256 -ny 512 -nz 512
2021-09-19 21:45:34.715738
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 256 -ny 512 -nz 512
2021-09-19 21:45:46.128586
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 256 -ny 512 -nz 512
2021-09-19 21:45:56.658833
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 256 -ny 512 -nz 512
2021-09-19 21:46:15.645210
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 256 -ny 512 -nz 512
2021-09-19 21:46:27.919548
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 256 -ny 512 -nz 512
2021-09-19 21:46:39.534177
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 256 -ny 512 -nz 512
2021-09-19 21:46:50.288533
b''

Starting computation for size 512
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 512 -ny 512 -nz 512
2021-09-19 21:47:10.200388
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 512 -ny 512 -nz 512
2021-09-19 21:47:22.817682
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 512 -ny 512 -nz 512
2021-09-19 21:47:35.730415
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 512 -ny 512 -nz 512
2021-09-19 21:47:46.440625
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 512 -ny 512 -nz 512
2021-09-19 21:47:57.670067
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 512 -ny 512 -nz 512
2021-09-19 21:48:08.669154
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 512 -ny 512 -nz 512
2021-09-19 21:48:37.017068
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 512 -ny 512 -nz 512
2021-09-19 21:48:49.360185
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 512 -ny 512 -nz 512
2021-09-19 21:49:01.263532
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 512 -ny 512 -nz 512
2021-09-19 21:49:12.171834
b''

Starting computation for size [512, 512, 1024]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 512 -ny 512 -nz 1024
2021-09-19 21:49:41.895031
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 512 -ny 512 -nz 1024
2021-09-19 21:49:54.896892
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 512 -ny 512 -nz 1024
2021-09-19 21:50:06.702627
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 512 -ny 512 -nz 1024
2021-09-19 21:50:17.605989
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 512 -ny 512 -nz 1024
2021-09-19 21:50:29.239467
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 512 -ny 512 -nz 1024
2021-09-19 21:50:40.796578
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 512 -ny 512 -nz 1024
2021-09-19 21:51:10.565928
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 512 -ny 512 -nz 1024
2021-09-19 21:51:23.745572
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 512 -ny 512 -nz 1024
2021-09-19 21:51:36.081086
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 512 -ny 512 -nz 1024
2021-09-19 21:51:47.208616
b''

Starting computation for size [512, 1024, 1024]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 512 -ny 1024 -nz 1024
2021-09-19 21:52:16.583300
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 512 -ny 1024 -nz 1024
2021-09-19 21:52:29.915556
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 512 -ny 1024 -nz 1024
2021-09-19 21:52:42.364799
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 512 -ny 1024 -nz 1024
2021-09-19 21:52:54.770116
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 512 -ny 1024 -nz 1024
2021-09-19 21:53:06.798650
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 512 -ny 1024 -nz 1024
2021-09-19 21:53:19.634259
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 512 -ny 1024 -nz 1024
2021-09-19 21:54:06.821213
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 512 -ny 1024 -nz 1024
2021-09-19 21:54:21.759134
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 512 -ny 1024 -nz 1024
2021-09-19 21:54:34.885700
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 512 -ny 1024 -nz 1024
2021-09-19 21:54:47.927658
b''

Starting computation for size 1024
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024
2021-09-19 21:55:36.477240
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024
2021-09-19 21:55:51.513017
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024
2021-09-19 21:56:05.018984
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024
2021-09-19 21:56:19.145243
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024
2021-09-19 21:56:32.114244
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024
2021-09-19 21:56:46.852295
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024
2021-09-19 21:58:08.913989
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024
2021-09-19 21:58:25.520131
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024
2021-09-19 21:58:39.968269
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024
2021-09-19 21:58:55.018839
b''

Starting computation for size [1024, 1024, 2048]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 2048
2021-09-19 22:00:20.755005
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 2048
2021-09-19 22:00:41.120157
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 2048
2021-09-19 22:00:56.064061
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 2048
2021-09-19 22:01:14.132064
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 2048
2021-09-19 22:01:29.066258
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 2048
2021-09-19 22:01:47.517638
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 2048
2021-09-19 22:03:10.359246
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 2048
2021-09-19 22:03:31.015572
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 2048
2021-09-19 22:03:47.068861
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 2048
2021-09-19 22:04:06.620204
b''

Starting computation for size [1024, 2048, 2048]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 1024 -ny 2048 -nz 2048
2021-09-19 22:05:33.483267
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 1024 -ny 2048 -nz 2048
2021-09-19 22:06:02.844618
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 1024 -ny 2048 -nz 2048
2021-09-19 22:06:22.063221
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 1024 -ny 2048 -nz 2048
2021-09-19 22:06:48.524628
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 1024 -ny 2048 -nz 2048
2021-09-19 22:07:07.607383
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 1024 -ny 2048 -nz 2048
2021-09-19 22:07:34.697712
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 1024 -ny 2048 -nz 2048
2021-09-19 22:10:10.743677
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 1024 -ny 2048 -nz 2048
2021-09-19 22:10:39.276213
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 1024 -ny 2048 -nz 2048
2021-09-19 22:10:59.137826
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 1024 -ny 2048 -nz 2048
2021-09-19 22:11:28.203743
b''

Starting computation for size 2048
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048
2021-09-19 22:14:12.409250
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048
2021-09-19 22:15:03.659538
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048
2021-09-19 22:15:31.308822
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048
2021-09-19 22:16:15.133012
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048
2021-09-19 22:16:42.785622
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048
2021-09-19 22:17:28.049561
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048
2021-09-19 22:22:29.774235
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048
2021-09-19 22:23:14.753120
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048
2021-09-19 22:23:43.206238
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048
2021-09-19 22:24:30.856743
b''

Starting computation for size 128
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 22:29:47.636389
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 22:29:58.293138
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 22:30:08.072658
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 22:30:17.365552
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 22:30:27.050683
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 22:30:36.430758
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 22:30:46.713083
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 22:30:56.836251
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 22:31:08.397988
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 22:31:18.380817
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

Starting computation for size 256
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 22:31:28.721897
b'Result (avg): 5.01733e-09\nResult (max): 5.42241e-08\n'

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 22:31:38.697805
b'Result (avg): 5.01733e-09\nResult (max): 5.42241e-08\n'

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 22:31:49.047164
b'Result (avg): 5.01733e-09\nResult (max): 5.42241e-08\n'

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 22:31:58.617369
b'Result (avg): 5.01733e-09\nResult (max): 5.42241e-08\n'

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 22:32:09.617478
b'Result (avg): 5.01733e-09\nResult (max): 5.42241e-08\n'

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 22:32:19.823855
b'Result (avg): 1.97165e-08\nResult (max): 8.10327e-06\n'

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 22:32:30.876798
b'Result (avg): 5.01733e-09\nResult (max): 5.42241e-08\n'

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 22:32:41.523405
b'Result (avg): 5.01733e-09\nResult (max): 5.42241e-08\n'

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 22:32:52.743129
b'Result (avg): 5.01733e-09\nResult (max): 5.42241e-08\n'

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 22:33:02.833397
b'Result (avg): 5.01733e-09\nResult (max): 5.42241e-08\n'

Starting computation for size 512
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 22:33:13.654603
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 22:33:23.720046
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 22:33:34.377207
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 22:33:44.123120
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 22:33:54.689563
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 22:34:04.950479
b'Result (avg): 0.000153466\nResult (max): 0.000595004\n'

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 22:34:17.913585
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 22:34:29.044695
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 22:34:40.757697
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 22:34:50.918713
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

Starting computation for size 1024
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 22:35:04.420883
b'Result (avg): 7.38396e-07\nResult (max): 8.69717e-06\n'

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 22:35:16.912681
b'Result (avg): 7.38396e-07\nResult (max): 8.69717e-06\n'

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 22:35:29.860933
b'Result (avg): 7.38396e-07\nResult (max): 8.69717e-06\n'

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 22:35:42.463129
b'Result (avg): 7.38396e-07\nResult (max): 8.69717e-06\n'

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 22:35:55.083884
b'Result (avg): 7.38396e-07\nResult (max): 8.69717e-06\n'

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 22:36:07.924986
b'Result (avg): 7.38396e-07\nResult (max): 8.69717e-06\n'

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 22:36:30.386825
b'Result (avg): 7.38396e-07\nResult (max): 8.69717e-06\n'

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 22:36:43.787634
b'Result (avg): 7.38396e-07\nResult (max): 8.69717e-06\n'

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 22:36:57.496991
b'Result (avg): 7.38396e-07\nResult (max): 8.69717e-06\n'

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 22:37:10.227103
b'Result (avg): 7.38396e-07\nResult (max): 8.69717e-06\n'

Starting computation for size 2048
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 22:37:32.564100
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 22:38:07.340050
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 22:38:40.328843
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 22:39:14.696900
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 22:39:47.547752
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 22:40:23.244756
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 22:41:32.366498
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 22:42:08.012732
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 22:42:42.064385
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 22:43:17.647212
b'Result (avg): -nan\nResult (max): -nan\n'

Slab 1D->2D opt1
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1136343] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1136343] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1136614] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1136614] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1136889] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1136889] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1137423] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1137423] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1137698] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1137698] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1137971] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1137971] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1138258] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1138258] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1138533] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1138533] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1138806] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1138806] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1139091] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1139091] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1139364] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1139364] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1139638] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1139638] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1139921] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1139921] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1140444] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1140444] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1140718] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1140718] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1141009] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1141009] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1141284] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1141284] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1141559] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1141559] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1141842] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1141842] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1142118] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1142118] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n514
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1142395] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1142395] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1142680] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1142680] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1142955] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1142955] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n514
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1143488] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1143488] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1143764] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1143764] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1144054] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1144054] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1144328] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1144328] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1144602] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1144602] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1144888] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1144888] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1145161] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1145161] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1145448] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1145448] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1145722] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1145722] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1146009] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1146009] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1146532] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1146532] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1146807] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1146807] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1147090] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1147090] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1147376] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1147376] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1147667] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1147667] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1147942] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1147942] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1148216] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1148216] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1148501] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1148501] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1148777] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1148777] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1149062] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1149062] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1149587] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1149587] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1149861] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1149861] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1150154] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1150154] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1150427] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1150427] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1150713] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1150713] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n514
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1150988] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1150988] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1151276] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1151276] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1151552] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1151552] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1151825] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1151825] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1152113] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1152113] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1152636] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1152636] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1152921] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1152921] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1153196] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1153196] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1153484] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1153484] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1153759] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1153759] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1154032] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1154032] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n507
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1154322] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1154322] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1154597] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1154597] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n514
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1154880] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1154880] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1155153] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1155153] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1155676] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1155676] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1155964] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1155964] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1156237] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1156237] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1156530] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1156530] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1156802] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1156802] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1157086] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1157086] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n514
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1157364] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1157364] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1157649] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1157649] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1157925] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1157925] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1158200] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1158200] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1158740] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1158740] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1159019] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1159019] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1159304] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1159304] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1159577] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1159577] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1159863] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1159863] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1160191] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1160191] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1160466] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1160466] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1160752] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1160752] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1161027] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1161027] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1161316] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1161316] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1161882] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1161882] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1162155] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1162155] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1162454] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1162454] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1162729] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1162729] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1163019] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1163019] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1163290] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1163290] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1163573] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1163573] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1163848] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1163848] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1164139] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1164139] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1164414] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1164414] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1164938] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1164938] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1165227] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1165227] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1165502] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1165502] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1165791] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1165791] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1166066] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1166066] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1166353] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1166353] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1166628] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1166628] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1166919] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1166919] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1167196] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1167196] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1167484] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1167484] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1168010] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1168010] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1168295] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1168295] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1168587] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1168587] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1168860] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1168860] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1169150] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1169150] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1169428] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1169428] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1169718] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1169718] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1169995] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1169995] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1170290] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1170290] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1170580] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1170580] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1171123] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1171123] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1171400] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1171400] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1171695] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1171695] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1171983] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1171983] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1172276] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1172276] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1172570] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1172570] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1172862] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1172862] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1173139] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1173139] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1173457] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1173457] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1173755] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1173755] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1174298] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1174298] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1174597] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1174597] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1174905] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1174905] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1175202] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1175202] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1175503] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1175503] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1175794] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1175794] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1176111] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1176111] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1176395] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1176395] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1176680] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1176680] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1176953] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1176953] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1177252] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1177252] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1177555] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1177555] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1177830] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1177830] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1178103] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1178103] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1178389] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1178389] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n514
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1178662] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1178662] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1178945] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1178945] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1179222] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1179222] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n514
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1179495] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1179495] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1179780] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1179780] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1180082] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1180082] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1180373] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1180373] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1180654] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1180654] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1180932] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1180932] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1181207] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1181207] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1181495] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1181495] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1181770] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1181770] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1182061] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1182061] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1182342] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1182342] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1182615] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1182615] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1182926] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1182926] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1183217] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1183217] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1183494] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1183494] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1183780] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1183780] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1184051] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1184051] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1184343] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1184343] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1184616] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1184616] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1184887] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1184887] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1185170] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1185170] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1185448] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1185448] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1185755] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1185755] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1186046] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1186046] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1186323] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1186323] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1186611] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1186611] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1186884] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1186884] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1187176] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1187176] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n514
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1187448] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1187448] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1187735] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1187735] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n514
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1188039] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1188039] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1188331] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1188331] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1188651] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1188651] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1188965] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1188965] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1189261] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1189261] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1189572] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1189572] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1189869] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1189869] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1190161] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1190161] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1190461] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1190461] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Starting computation for size 128
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128
2021-09-19 22:44:28.081501
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128
2021-09-19 22:44:37.625952
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128
2021-09-19 22:44:47.565950
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128
2021-09-19 22:44:56.556315
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128
2021-09-19 22:45:06.299643
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128
2021-09-19 22:45:15.415801
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128
2021-09-19 22:45:25.220341
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128
2021-09-19 22:45:34.973999
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128
2021-09-19 22:45:45.659276
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128
2021-09-19 22:45:55.344076
b''

Starting computation for size [128, 128, 256]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256
2021-09-19 22:46:05.871517
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256
2021-09-19 22:46:16.172603
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256
2021-09-19 22:46:26.375028
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256
2021-09-19 22:46:35.564754
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256
2021-09-19 22:46:45.812787
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256
2021-09-19 22:46:55.926384
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256
2021-09-19 22:47:06.888691
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256
2021-09-19 22:47:18.085499
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256
2021-09-19 22:47:30.108348
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256
2021-09-19 22:47:40.506716
b''

Starting computation for size [128, 256, 256]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256
2021-09-19 22:47:51.788569
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256
2021-09-19 22:48:04.698859
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256
2021-09-19 22:48:17.315873
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256
2021-09-19 22:48:30.760535
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256
2021-09-19 22:48:43.326683
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256
2021-09-19 22:48:56.358810
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256
2021-09-19 22:49:08.997541
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256
2021-09-19 22:49:21.238154
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256
2021-09-19 22:49:33.970352
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256
2021-09-19 22:49:46.463947
b''

Starting computation for size 256
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256
2021-09-19 22:49:58.290672
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256
2021-09-19 22:50:11.058950
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256
2021-09-19 22:50:22.648447
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256
2021-09-19 22:50:36.421213
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256
2021-09-19 22:50:47.792077
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256
2021-09-19 22:51:00.204679
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256
2021-09-19 22:51:11.617594
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256
2021-09-19 22:51:23.815110
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256
2021-09-19 22:51:35.905364
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256
2021-09-19 22:51:47.968047
b''

Starting computation for size [256, 256, 512]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512
2021-09-19 22:51:59.026837
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512
2021-09-19 22:52:11.691795
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512
2021-09-19 22:52:25.032211
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512
2021-09-19 22:52:36.250015
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512
2021-09-19 22:52:49.556823
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512
2021-09-19 22:53:02.470845
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512
2021-09-19 22:53:15.332042
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512
2021-09-19 22:53:27.621689
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512
2021-09-19 22:53:39.467455
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512
2021-09-19 22:53:52.634975
b''

Starting computation for size [256, 512, 512]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512
2021-09-19 22:54:05.040838
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512
2021-09-19 22:54:17.411955
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512
2021-09-19 22:54:31.175223
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512
2021-09-19 22:54:42.288885
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512
2021-09-19 22:54:55.746002
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512
2021-09-19 22:55:09.452845
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512
2021-09-19 22:55:22.744256
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512
2021-09-19 22:55:35.547479
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512
2021-09-19 22:55:47.932491
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512
2021-09-19 22:56:01.738273
b''

Starting computation for size 512
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512
2021-09-19 22:56:14.566628
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512
2021-09-19 22:56:27.445398
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512
2021-09-19 22:56:40.556323
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512
2021-09-19 22:56:51.636469
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512
2021-09-19 22:57:04.910214
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512
2021-09-19 22:57:19.341072
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512
2021-09-19 22:57:32.742140
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512
2021-09-19 22:57:46.460370
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512
2021-09-19 22:57:59.052065
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512
2021-09-19 22:58:12.237582
b''

Starting computation for size [512, 512, 1024]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-19 22:58:24.571236
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-19 22:58:37.226006
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-19 22:58:48.936761
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-19 22:59:00.054128
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-19 22:59:12.012817
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-19 22:59:25.807836
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-19 22:59:37.669461
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-19 22:59:53.384026
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-19 23:00:05.620931
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024
2021-09-19 23:00:19.473093
b''

Starting computation for size [512, 1024, 1024]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-19 23:00:31.572445
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-19 23:00:44.795216
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-19 23:00:57.278042
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-19 23:01:09.231496
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-19 23:01:21.781462
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-19 23:01:35.144614
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-19 23:01:47.833893
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-19 23:02:02.234637
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-19 23:02:14.872543
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024
2021-09-19 23:02:28.802687
b''

Starting computation for size 1024
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-19 23:02:40.921857
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-19 23:02:55.879216
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-19 23:03:08.960307
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-19 23:03:22.840053
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-19 23:03:35.752145
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-19 23:03:50.599976
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-19 23:04:03.800895
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-19 23:04:20.132439
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-19 23:04:33.989412
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024
2021-09-19 23:04:48.854269
b''

Starting computation for size [1024, 1024, 2048]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-19 23:05:01.783622
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-19 23:05:21.100467
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-19 23:05:35.445502
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-19 23:05:53.055968
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-19 23:06:07.482322
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-19 23:06:26.622116
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-19 23:06:41.115221
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-19 23:07:01.300785
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-19 23:07:16.991027
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 2048
2021-09-19 23:07:35.041956
b''

Starting computation for size [1024, 2048, 2048]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-19 23:07:49.655284
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-19 23:08:19.292127
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-19 23:08:37.283139
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-19 23:09:03.517289
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-19 23:09:21.801468
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-19 23:09:49.305377
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-19 23:10:08.063242
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-19 23:10:37.503933
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-19 23:10:57.690161
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 2048 -nz 2048
2021-09-19 23:11:23.913617
b''

Starting computation for size 2048
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-19 23:11:42.715131
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-19 23:12:33.272384
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-19 23:13:03.441382
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-19 23:13:47.719962
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-19 23:14:14.197878
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-19 23:14:59.777197
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-19 23:15:26.888097
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-19 23:16:13.112295
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-19 23:16:41.843528
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048
2021-09-19 23:17:24.717229
b''

Starting computation for size 128
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 23:17:53.385500
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 23:18:03.313933
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 23:18:13.130246
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 23:18:22.471467
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 23:18:32.722257
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 23:18:42.303909
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 23:18:52.460858
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 23:19:02.825425
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 23:19:14.264893
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-19 23:19:25.146794
b'Result (avg): 1.91723e-05\nResult (max): 7.43495e-05\n'

Starting computation for size 256
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 23:19:35.216097
b'Result (avg): 5.19268e-09\nResult (max): 5.08699e-08\n'

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 23:19:50.333389
b'Result (avg): 5.19268e-09\nResult (max): 5.08699e-08\n'

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 23:20:01.003151
b'Result (avg): 5.19268e-09\nResult (max): 5.08699e-08\n'

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 23:20:10.776373
b'Result (avg): 5.19268e-09\nResult (max): 5.08699e-08\n'

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 23:20:21.251905
b'Result (avg): 5.19268e-09\nResult (max): 5.08699e-08\n'

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 23:20:31.444117
b'Result (avg): 5.19268e-09\nResult (max): 5.08699e-08\n'

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 23:20:42.060094
b'Result (avg): 5.19268e-09\nResult (max): 5.08699e-08\n'

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 23:20:52.886615
b'Result (avg): 5.19268e-09\nResult (max): 5.08699e-08\n'

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 23:21:04.411728
b'Result (avg): 5.19268e-09\nResult (max): 5.08699e-08\n'

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-19 23:21:14.840559
b'Result (avg): 5.19268e-09\nResult (max): 5.08699e-08\n'

Starting computation for size 512
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 23:21:25.315853
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 23:21:35.471324
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 23:21:46.005979
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 23:21:56.799948
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 23:22:09.946178
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 23:22:20.737433
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 23:22:32.742313
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 23:22:44.942784
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 23:22:56.812090
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-19 23:23:07.239631
b'Result (avg): 0.000153465\nResult (max): 0.000595014\n'

Starting computation for size 1024
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 23:23:18.183235
b'Result (avg): 7.69765e-07\nResult (max): 9.22816e-06\n'

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 23:23:30.858992
b'Result (avg): 7.69765e-07\nResult (max): 9.22816e-06\n'

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 23:23:44.106115
b'Result (avg): 7.69765e-07\nResult (max): 9.22816e-06\n'

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 23:23:56.571839
b'Result (avg): 7.69765e-07\nResult (max): 9.22816e-06\n'

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 23:24:09.742482
b'Result (avg): 7.69765e-07\nResult (max): 9.22816e-06\n'

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 23:24:22.896739
b'Result (avg): 7.69765e-07\nResult (max): 9.22816e-06\n'

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 23:24:36.602028
b'Result (avg): 7.69765e-07\nResult (max): 9.22816e-06\n'

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 23:24:50.315418
b'Result (avg): 7.69765e-07\nResult (max): 9.22816e-06\n'

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 23:25:04.543550
b'Result (avg): 7.69765e-07\nResult (max): 9.22816e-06\n'

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-19 23:25:17.749078
b'Result (avg): 7.69765e-07\nResult (max): 9.22816e-06\n'

Starting computation for size 2048
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 23:25:30.920307
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 23:26:06.447370
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 23:26:39.800593
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 23:27:14.716580
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 23:27:48.385688
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 23:28:23.965533
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 23:28:57.537892
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 23:29:34.185783
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 23:30:08.977885
b'Result (avg): -nan\nResult (max): -nan\n'

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/forward -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 4
2021-09-19 23:30:44.905712
b'Result (avg): -nan\nResult (max): -nan\n'

Slab 2D->1D default (inverse)
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1190758] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1190758] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1191044] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1191044] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1191317] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1191317] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1191841] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1191841] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1192130] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1192130] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1192403] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1192403] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1192676] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1192676] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1192959] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1192959] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1193234] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1193234] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1193509] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1193509] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1193794] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1193794] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1194069] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1194069] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1194342] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1194342] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1194877] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1194877] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1195152] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1195152] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1195427] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1195427] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1195710] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1195710] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1195985] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1195985] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1196261] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1196261] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1196541] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1196541] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1196816] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1196816] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1197101] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1197101] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1197381] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1197381] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1197902] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1197902] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1198186] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1198186] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1198461] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1198461] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1198746] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1198746] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1199021] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1199021] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1199293] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1199293] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1199583] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1199583] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1199858] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1199858] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1200145] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1200145] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1200420] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1200420] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1200944] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1200944] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1201229] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1201229] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1201504] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1201504] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1201790] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1201790] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1202065] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1202065] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1202340] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1202340] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1202623] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1202623] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1202899] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1202899] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1203188] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1203188] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1203464] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1203464] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1203987] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1203987] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1204275] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1204275] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1204551] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1204551] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1204836] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1204836] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1205109] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1205109] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1205397] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1205397] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1205673] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1205673] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1205963] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1205963] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1206236] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1206236] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1206510] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1206510] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1207043] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1207043] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1207319] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1207319] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1207602] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1207602] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1207879] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1207879] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1208156] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1208156] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1208445] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1208445] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1208719] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1208719] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1209007] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1209007] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1209283] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1209283] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1209568] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1209568] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1210089] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1210089] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1210362] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1210362] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1210654] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1210654] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1210927] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1210927] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1211212] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1211212] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1211490] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1211490] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1211779] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1211779] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1212054] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1212054] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1212327] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1212327] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1212615] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1212615] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1213140] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1213140] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n514
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1213426] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1213426] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1213699] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1213699] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1213975] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1213975] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1214263] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1214263] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1214538] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1214538] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1214831] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1214831] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1215106] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1215106] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1215383] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1215383] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1215666] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1215666] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1216187] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1216187] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1216470] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1216470] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1216747] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1216747] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n514
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1217024] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1217024] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1217314] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1217314] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1217585] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1217585] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n514
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1217874] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1217874] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1218151] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1218151] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1218440] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1218440] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1218714] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1218714] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1219240] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1219240] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n514
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1219529] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1219529] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1219804] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1219804] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1220090] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1220090] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1220365] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1220365] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1220652] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1220652] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1220930] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1220930] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1221219] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1221219] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1221500] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1221500] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1221785] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1221785] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n514
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1222314] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1222314] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1222602] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1222602] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1222881] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1222881] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1223172] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1223172] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1223460] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1223460] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1223735] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1223735] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1224023] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1224023] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1224299] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1224299] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1224601] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1224601] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n514
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1224891] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1224891] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1225431] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1225431] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1225709] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1225709] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1226003] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1226003] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1226288] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1226288] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1226582] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1226582] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1226858] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1226858] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1227155] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1227155] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1227443] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1227443] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1227747] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1227747] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n514
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1228045] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1228045] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1228604] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1228604] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1228887] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1228887] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n514
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1229244] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1229244] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1229534] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1229534] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1229836] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1229836] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1230130] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1230130] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n514
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1230434] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1230434] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Starting computation for size 128
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 23:31:18.772345
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 23:31:28.658173
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 23:31:38.926859
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 23:31:48.538736
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 23:31:58.808493
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 23:32:08.459353
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 23:32:18.838043
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 23:32:28.819866
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 23:32:39.721031
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-19 23:32:50.008494
b''

Starting computation for size [128, 128, 256]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 23:33:00.850708
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 23:33:10.296627
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 23:33:20.503326
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 23:33:30.077696
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 23:33:40.431819
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 23:33:50.208022
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 23:34:00.855552
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 23:34:11.312568
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 23:34:22.387437
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-19 23:34:32.334262
b''

Starting computation for size [128, 256, 256]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 23:34:42.873322
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 23:34:55.376578
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 23:35:07.098907
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 23:35:20.710167
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 23:35:32.731885
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 23:35:45.126510
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 23:35:57.424825
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 23:36:09.498716
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 23:36:21.625370
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-19 23:36:33.348886
b''

Starting computation for size 256
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 23:36:44.911176
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 23:36:57.687972
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 23:37:09.266969
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 23:37:23.438988
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 23:37:34.740665
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 23:37:47.094890
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 23:37:58.264517
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 23:38:10.375928
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 23:38:22.007411
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-19 23:38:33.586002
b''

Starting computation for size [256, 256, 512]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 23:38:45.737846
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 23:38:58.290468
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 23:39:11.031343
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 23:39:22.336742
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 23:39:35.774175
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 23:39:53.987800
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 23:40:08.238854
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 23:40:20.328191
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 23:40:31.585144
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-19 23:40:44.322973
b''

Starting computation for size [256, 512, 512]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 23:40:56.381677
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 23:41:08.798104
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 23:41:21.661704
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 23:41:32.496139
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 23:41:45.770794
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 23:41:58.565702
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 23:42:12.005884
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 23:42:24.581921
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 23:42:36.006876
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-19 23:42:49.346206
b''

Starting computation for size 512
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 23:43:02.156576
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 23:43:14.653954
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 23:43:27.698422
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 23:43:38.487696
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 23:43:51.470954
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 23:44:04.985101
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 23:44:18.115673
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 23:44:30.792051
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 23:44:42.476679
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-19 23:44:55.831305
b''

Starting computation for size [512, 512, 1024]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 23:45:08.027950
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 23:45:20.695262
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 23:45:32.496404
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 23:45:44.407738
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 23:45:56.201960
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 23:46:09.076145
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 23:46:20.806356
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 23:46:33.794052
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 23:46:46.227032
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-19 23:46:58.490570
b''

Starting computation for size [512, 1024, 1024]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 23:47:10.039911
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 23:47:23.107837
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 23:47:34.603755
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 23:47:46.057081
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 23:47:57.688285
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 23:48:10.767209
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 23:48:22.307552
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 23:48:36.200946
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 23:48:48.365593
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-19 23:49:01.014032
b''

Starting computation for size 1024
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 23:49:12.876968
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 23:49:27.503364
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 23:49:40.376898
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 23:49:53.945498
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 23:50:06.692391
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 23:50:21.246580
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 23:50:33.974290
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 23:50:50.109081
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 23:51:03.765146
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-19 23:51:18.384522
b''

Starting computation for size [1024, 1024, 2048]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 23:51:31.222314
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 23:51:50.568794
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 23:52:04.739754
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 23:52:22.235974
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 23:52:36.311009
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 23:52:54.181622
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 23:53:08.810342
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 23:53:28.829346
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 23:53:44.237357
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-19 23:54:02.261888
b''

Starting computation for size [1024, 2048, 2048]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 23:54:16.537908
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 23:54:45.930814
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 23:55:03.831184
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 23:55:30.109080
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 23:55:48.265495
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 23:56:14.007888
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 23:56:31.986795
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 23:57:00.154801
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 23:57:19.785427
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-19 23:57:45.496667
b''

Starting computation for size 2048
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 23:58:03.938106
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 23:58:53.294356
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-19 23:59:18.439035
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-20 00:00:01.332252
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-20 00:00:26.048467
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-20 00:01:06.711989
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-20 00:01:32.239372
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-20 00:02:15.825453
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-20 00:02:42.893583
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-20 00:03:23.629213
b''

Slab 2D->1D opt1 (inverse)
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1230729] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1230729] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1231013] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1231013] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1231288] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1231288] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1231807] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1231807] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1232342] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1232342] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1232616] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1232616] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1232891] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1232891] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1233182] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1233182] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1233457] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1233457] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1233730] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1233730] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1234018] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1234018] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1234289] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1234289] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1234562] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1234562] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1235091] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1235091] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1235614] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1235614] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1235886] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1235886] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1236178] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1236178] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1236453] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1236453] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1236728] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1236728] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1237012] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1237012] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1237288] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1237288] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1237575] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1237575] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1237854] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1237854] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1238389] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1238389] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1238918] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1238918] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1239189] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1239189] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1239489] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1239489] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1239762] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1239762] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1240052] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1240052] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1240327] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1240327] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1240602] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1240602] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1240891] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1240891] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1241164] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1241164] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1241695] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1241695] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1242218] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1242218] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1242491] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1242491] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1242776] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1242776] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1243052] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1243052] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1243343] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1243343] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1243620] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1243620] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1243894] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1243894] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1244181] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1244181] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1244457] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1244457] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1244990] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1244990] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1245513] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1245513] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1245786] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1245786] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1246078] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1246078] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1246354] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1246354] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1246639] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1246639] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1246914] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1246914] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1247191] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1247191] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1247479] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1247479] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1247752] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1247752] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1248289] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1248289] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1248814] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1248814] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1249089] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1249089] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1249379] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1249379] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1249653] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1249653] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1249940] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1249940] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n514
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1250211] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1250211] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1250503] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1250503] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n514
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1250776] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1250776] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1251063] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1251063] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1251586] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1251586] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n514
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1252109] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1252109] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1252392] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1252392] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1252670] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1252670] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1252961] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1252961] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1253233] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1253233] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n514
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1253520] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1253520] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1253800] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1253800] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1254085] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1254085] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1254360] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1254360] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1254897] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1254897] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1255420] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1255420] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1255697] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1255697] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n514
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1255992] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1255992] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1256279] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1256279] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1256552] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1256552] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1256828] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1256828] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1257125] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1257125] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1257410] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1257410] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1257685] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1257685] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1258220] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1258220] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1258745] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1258745] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1259019] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1259019] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1259332] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1259332] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1259609] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1259609] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1259895] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1259895] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1260168] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1260168] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1260486] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1260486] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1260761] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1260761] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1261044] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1261044] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1261567] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1261567] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1262108] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1262108] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1262382] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1262382] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1262695] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1262695] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1262975] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1262975] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1263264] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1263264] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1263541] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1263541] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1263854] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1263854] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1264149] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1264149] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1264421] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1264421] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1264959] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1264959] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1265485] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1265485] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1265773] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1265773] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1266115] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1266115] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1266394] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1266394] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1266686] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1266686] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1266976] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1266976] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1267318] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1267318] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1267615] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1267615] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1267893] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1267893] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1268435] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1268435] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1268971] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1268971] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1269260] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1269260] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1269659] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1269659] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1269952] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1269952] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1270232] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1270232] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1270526] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1270526] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1270939] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1270939] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1271246] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1271246] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1271537] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1271537] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1272102] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1272102] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1272640] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1272640] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1272946] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1272946] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1273342] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1273342] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1273652] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1273652] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1273945] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1273945] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1274248] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1274248] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Starting computation for size 128
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-20 00:03:49.404554
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-20 00:03:58.961924
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-20 00:04:09.432282
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-20 00:04:18.799262
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-20 00:04:28.632546
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-20 00:04:37.841519
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-20 00:04:49.607342
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-20 00:04:59.662896
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-20 00:05:10.316207
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-20 00:05:20.220076
b''

Starting computation for size [128, 128, 256]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-20 00:05:32.598862
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-20 00:05:41.660993
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-20 00:05:51.692781
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-20 00:06:00.974962
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-20 00:06:11.006072
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-20 00:06:20.210986
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-20 00:06:33.583102
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-20 00:06:43.431126
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-20 00:06:54.077670
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-20 00:07:04.038612
b''

Starting computation for size [128, 256, 256]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-20 00:07:18.302089
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-20 00:07:31.225194
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-20 00:07:43.594845
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-20 00:07:57.237792
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-20 00:08:09.000983
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-20 00:08:19.753661
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-20 00:08:33.291657
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-20 00:08:45.539053
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-20 00:08:57.997841
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-20 00:09:08.773607
b''

Starting computation for size 256
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-20 00:09:22.183092
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-20 00:09:35.061707
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-20 00:09:46.845465
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-20 00:10:00.319050
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-20 00:10:11.090641
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-20 00:10:21.288698
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-20 00:10:34.381577
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-20 00:10:46.632926
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-20 00:10:58.119689
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-20 00:11:08.045046
b''

Starting computation for size [256, 256, 512]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-20 00:11:20.998933
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-20 00:11:33.730287
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-20 00:11:47.057185
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-20 00:11:58.602126
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-20 00:12:10.157511
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-20 00:12:20.532401
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-20 00:12:35.321861
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-20 00:12:47.583759
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-20 00:12:59.169919
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-20 00:13:09.309098
b''

Starting computation for size [256, 512, 512]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-20 00:13:24.683573
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-20 00:13:37.326452
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-20 00:13:51.120535
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-20 00:14:01.780454
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-20 00:14:12.975762
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-20 00:14:24.182426
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-20 00:14:43.810352
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-20 00:14:56.089191
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-20 00:15:08.129564
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-20 00:15:18.464037
b''

Starting computation for size 512
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-20 00:15:38.942587
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-20 00:15:51.470947
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-20 00:16:04.642353
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-20 00:16:15.722641
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-20 00:16:26.726785
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-20 00:16:37.717956
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-20 00:16:57.190773
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-20 00:17:10.109854
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-20 00:17:21.762422
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-20 00:17:32.374909
b''

Starting computation for size [512, 512, 1024]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-20 00:17:53.396272
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-20 00:18:07.174833
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-20 00:18:18.607078
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-20 00:18:30.106985
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-20 00:18:41.313936
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-20 00:18:52.167687
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-20 00:19:20.140134
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-20 00:19:33.067202
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-20 00:19:44.785808
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-20 00:19:55.352837
b''

Starting computation for size [512, 1024, 1024]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-20 00:20:24.404163
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-20 00:20:37.413514
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-20 00:20:49.775199
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-20 00:21:01.260799
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-20 00:21:12.581549
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-20 00:21:24.311876
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-20 00:22:10.017272
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-20 00:22:24.801333
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-20 00:22:38.659158
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-20 00:22:51.062696
b''

Starting computation for size 1024
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-20 00:23:39.676833
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-20 00:23:54.280484
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-20 00:24:07.202966
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-20 00:24:21.785291
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-20 00:24:35.283939
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-20 00:24:49.497553
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-20 00:25:36.879218
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-20 00:25:53.697462
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-20 00:26:07.778180
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-20 00:26:22.685003
b''

Starting computation for size [1024, 1024, 2048]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-20 00:27:13.023339
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-20 00:27:34.159652
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-20 00:27:50.476892
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-20 00:28:08.775656
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-20 00:28:23.859526
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-20 00:28:42.220587
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-20 00:30:05.485772
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-20 00:30:26.324449
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-20 00:30:42.623891
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-20 00:31:02.311975
b''

Starting computation for size [1024, 2048, 2048]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-20 00:32:29.740530
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-20 00:32:59.396287
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-20 00:33:17.985676
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-20 00:33:45.264439
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-20 00:34:04.526800
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-20 00:34:31.097867
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-20 00:37:06.927941
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-20 00:37:35.581264
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-20 00:37:55.312043
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-20 00:38:23.575034
b''

Starting computation for size 2048
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-20 00:41:07.184664
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-20 00:41:56.115059
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-20 00:42:21.788238
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-20 00:43:04.535324
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-20 00:43:31.288170
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-20 00:44:13.114738
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-20 00:46:54.150049
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-20 00:47:38.745877
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-20 00:48:06.754886
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-20 00:48:53.817126
b''

Slab 1D->2D default (inverse)
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1274668] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1274668] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1274940] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1274940] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1275225] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1275225] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1275749] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1275749] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1276022] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1276022] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1276307] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1276307] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1276591] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1276591] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1276865] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1276865] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1277157] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1277157] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1277432] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1277432] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1277706] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1277706] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1277988] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1277988] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1278265] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1278265] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1278788] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1278788] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1279071] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1279071] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1279346] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1279346] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1279635] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1279635] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1279910] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1279910] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1280185] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1280185] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1280470] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1280470] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1280744] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1280744] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1281032] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1281032] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1281307] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1281307] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1281846] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1281846] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1282131] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1282131] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1282407] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1282407] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1282694] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1282694] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1282968] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1282968] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1283251] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1283251] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1283528] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1283528] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1283820] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1283820] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1284098] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1284098] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1284384] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1284384] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1284907] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1284907] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1285183] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1285183] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1285472] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1285472] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1285769] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1285769] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1286044] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1286044] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1286331] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1286331] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1286608] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1286608] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1286898] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1286898] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1287175] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1287175] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1287462] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1287462] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n514
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1287983] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1287983] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1288260] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1288260] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n514
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1288591] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1288591] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1288882] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1288882] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n514
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1289159] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1289159] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1289446] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1289446] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1289724] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1289724] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1290013] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1290013] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1290290] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1290290] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1290577] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1290577] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1291099] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1291099] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1291378] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1291378] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1291665] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1291665] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1291961] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1291961] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1292248] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1292248] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1292525] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1292525] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1292798] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1292798] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1293110] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1293110] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1293389] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1293389] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1293664] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1293664] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1294197] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1294197] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1294472] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1294472] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1294761] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1294761] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1295081] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1295081] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1295356] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1295356] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1295646] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1295646] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1295921] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1295921] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1296241] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1296241] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1296531] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1296531] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1296805] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1296805] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1297329] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1297329] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1297613] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1297613] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1297888] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1297888] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1298224] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1298224] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1298501] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1298501] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1298772] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1298772] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1299056] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1299056] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1299382] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1299382] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1299670] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1299670] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1299945] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1299945] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1300471] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1300471] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1300758] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1300758] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1301037] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1301037] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1301404] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1301404] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1301693] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1301693] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1301966] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1301966] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1302257] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1302257] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1302630] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1302630] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n514
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1302909] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1302909] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1303198] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1303198] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1303724] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1303724] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1304013] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1304013] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1304290] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1304290] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1304741] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1304741] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1305025] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1305025] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n514
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1305317] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1305317] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1305592] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1305592] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1306051] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1306051] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1306347] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1306347] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1306623] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1306623] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n507
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1307161] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1307161] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1307440] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1307440] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1307731] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1307731] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n507
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1308185] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1308185] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1308472] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1308472] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1308759] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1308759] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1309038] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1309038] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1309499] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1309499] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1309796] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1309796] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1310075] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1310075] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1310617] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1310617] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1310908] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1310908] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1311201] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1311201] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1311823] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1311823] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1312117] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1312117] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1312410] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1312410] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1312707] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1312707] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1313332] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1313332] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1313648] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1313648] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1313944] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1313944] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1314487] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1314487] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1314782] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1314782] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1315103] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1315103] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1316113] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1316113] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1316417] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1316417] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1316711] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1316711] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1317031] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1317031] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Starting computation for size 128
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-20 00:51:42.725919
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-20 00:51:51.890627
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-20 00:52:01.723236
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-20 00:52:10.816567
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-20 00:52:20.762671
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-20 00:52:29.816208
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-20 00:52:42.859324
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-20 00:52:52.939108
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-20 00:53:04.002685
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-20 00:53:14.170098
b''

Starting computation for size [128, 128, 256]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-20 00:53:28.098949
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-20 00:53:37.848144
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-20 00:53:48.211920
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-20 00:53:57.560520
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-20 00:54:07.931717
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-20 00:54:17.731804
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-20 00:54:31.411716
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-20 00:54:43.240579
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-20 00:54:55.949651
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-20 00:55:06.447683
b''

Starting computation for size [128, 256, 256]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-20 00:55:20.737279
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-20 00:55:33.678377
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-20 00:55:46.121239
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-20 00:55:59.954884
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-20 00:56:12.727838
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-20 00:56:25.781259
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-20 00:56:43.710148
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-20 00:56:55.818756
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-20 00:57:08.787960
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-20 00:57:22.138933
b''

Starting computation for size 256
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-20 00:57:40.022851
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-20 00:57:53.382632
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-20 00:58:05.118380
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-20 00:58:17.858421
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-20 00:58:29.825169
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-20 00:58:42.257400
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-20 00:59:07.156476
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-20 00:59:19.879645
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-20 00:59:31.810552
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-20 00:59:44.197040
b''

Starting computation for size [256, 256, 512]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-20 01:00:08.664911
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-20 01:00:21.293907
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-20 01:00:34.950824
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-20 01:00:45.952589
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-20 01:00:58.906459
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-20 01:01:12.086379
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-20 01:01:36.933374
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-20 01:01:49.378521
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-20 01:02:00.997462
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-20 01:02:14.162507
b''

Starting computation for size [256, 512, 512]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-20 01:02:38.530154
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-20 01:02:51.011561
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-20 01:03:03.189872
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-20 01:03:13.761247
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-20 01:03:26.883630
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-20 01:03:40.976166
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-20 01:04:19.260965
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-20 01:04:31.888839
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-20 01:04:43.497783
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-20 01:04:56.650244
b''

Starting computation for size 512
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-20 01:05:34.850136
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-20 01:05:46.736273
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-20 01:05:59.417906
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-20 01:06:10.181346
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-20 01:06:22.619252
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-20 01:06:35.618066
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-20 01:07:40.789663
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-20 01:07:53.464915
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-20 01:08:04.886239
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-20 01:08:17.959571
b''

Starting computation for size [512, 512, 1024]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-20 01:09:22.693672
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-20 01:09:34.538155
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-20 01:09:45.784672
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-20 01:09:56.386108
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-20 01:10:07.582016
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-20 01:10:21.309774
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-20 01:11:30.889126
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-20 01:11:43.721266
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-20 01:11:55.388040
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-20 01:12:09.346575
b''

Starting computation for size [512, 1024, 1024]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-20 01:13:20.576295
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-20 01:13:33.107537
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-20 01:13:44.977417
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-20 01:13:57.473872
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-20 01:14:09.777976
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-20 01:14:24.514828
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-20 01:16:27.938198
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-20 01:16:42.469217
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-20 01:16:55.446301
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-20 01:17:10.455110
b''

Starting computation for size 1024
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-20 01:19:14.782398
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-20 01:19:29.908161
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-20 01:19:43.237113
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-20 01:19:58.129991
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-20 01:20:11.061773
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-20 01:20:26.991723
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-20 01:24:11.301665
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-20 01:24:28.187806
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-20 01:24:42.328347
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-20 01:24:58.070710
b''

Starting computation for size [1024, 1024, 2048]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-20 01:28:42.510361
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-20 01:29:03.123873
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-20 01:29:17.552104
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-20 01:29:37.939665
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-20 01:29:56.542544
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-20 01:30:16.658558
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-20 01:33:59.046299
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-20 01:34:20.153898
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-20 01:34:36.019365
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-20 01:34:56.149826
b''

Starting computation for size [1024, 2048, 2048]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-20 01:38:41.281059
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-20 01:39:11.063212
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-20 01:39:29.194194
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-20 01:39:55.317999
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-20 01:40:13.474441
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-20 01:40:43.964452
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-20 01:47:55.344782
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-20 01:48:23.751715
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-20 01:48:43.912502
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-20 01:49:13.368010
b''

Starting computation for size 2048
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-20 01:56:29.146407
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-20 01:57:19.217227
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-20 01:57:45.175941
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-20 01:58:27.889254
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-20 01:58:54.008966
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-20 01:59:44.571962
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-20 02:13:43.780678
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-20 02:14:28.409860
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-20 02:14:57.011898
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-20 02:15:45.421451
b''

Slab 1D->2D opt1 (inverse)
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1318005] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1318005] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1318287] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1318287] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1318562] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1318562] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1319082] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1319082] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1319618] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1319618] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1319893] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1319893] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1320166] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1320166] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1320456] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1320456] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1320730] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1320730] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1321003] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1321003] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1321287] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1321287] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1321560] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1321560] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1321833] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1321833] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1322370] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1322370] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1322894] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1322894] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1323167] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1323167] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1323453] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1323453] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1323729] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1323729] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1324007] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1324007] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1324291] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1324291] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1324566] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1324566] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1324856] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1324856] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1325133] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1325133] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1325664] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1325664] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1326188] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1326188] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1326461] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1326461] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1326748] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1326748] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1327021] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1327021] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1327312] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1327312] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1327586] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1327586] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1327861] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1327861] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1328148] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1328148] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1328421] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1328421] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1328956] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1328956] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1329479] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1329479] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1329751] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1329751] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1330037] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1330037] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1330312] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1330312] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1330600] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1330600] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1330875] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1330875] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1331147] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1331147] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1331436] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1331436] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1331714] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1331714] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1332251] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1332251] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1332771] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1332771] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1333044] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1333044] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1333332] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1333332] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1333605] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1333605] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1333892] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1333892] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1334165] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1334165] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1334441] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1334441] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1334728] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1334728] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1335002] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1335002] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1335537] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1335537] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1336061] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1336061] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1336334] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1336334] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1336621] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1336621] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1336894] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1336894] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1337186] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1337186] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1337461] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1337461] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1337738] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1337738] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1338023] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1338023] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1338300] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1338300] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1338833] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1338833] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1339370] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1339370] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1339644] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1339644] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1339933] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1339933] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1340210] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1340210] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1340484] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1340484] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1340768] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1340768] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1341041] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1341041] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1341328] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1341328] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n507
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1341603] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1341603] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1342124] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1342124] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1342657] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1342657] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1342932] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1342932] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1343217] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1343217] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1343492] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1343492] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1343767] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1343767] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1344054] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1344054] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1344327] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1344327] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1344616] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1344616] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1344892] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1344892] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1345413] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1345413] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1345947] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1345947] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1346218] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1346218] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1346500] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1346500] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1346778] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1346778] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1347067] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1347067] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1347344] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1347344] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1347618] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1347618] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1347905] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1347905] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1348180] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1348180] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1348714] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1348714] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1349235] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1349235] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1349524] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1349524] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1349800] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1349800] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1350088] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1350088] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1350366] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1350366] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1350641] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1350641] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1350925] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1350925] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1351214] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1351214] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1351492] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1351492] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1352025] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1352025] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1352549] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1352549] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1352838] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1352838] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1353115] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1353115] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1353406] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1353406] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1353682] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1353682] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1353969] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1353969] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1354245] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1354245] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1354536] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1354536] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1354827] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1354827] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1355363] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1355363] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1355893] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1355893] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1356182] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1356182] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n515
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1356475] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1356475] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1356770] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1356770] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n516
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1357050] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1357050] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1357338] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1357338] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1357631] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1357631] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1357936] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1357936] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1358226] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1358226] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n511
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1358788] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1358788] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1359326] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1359326] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1359628] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1359628] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1359916] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1359916] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1360217] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1360217] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1360512] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1360512] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
Open MPI failed an OFI Libfabric library call (fi_domain).  This is highly
unusual; your job may behave unpredictably (and/or abort) after this.

  Local host: uc2n510
  Location: mtl_ofi_component.c:911
  Error: No data available (61)
--------------------------------------------------------------------------
[uc2n507.localdomain:1360855] 63 more processes have sent help message help-mtl-ofi.txt / OFI call fail
[uc2n507.localdomain:1360855] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Starting computation for size 128
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-20 02:30:00.225634
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-20 02:30:09.284200
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-20 02:30:19.159470
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-20 02:30:28.399176
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-20 02:30:38.423111
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-20 02:30:47.599216
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-20 02:30:57.484475
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-20 02:31:07.408100
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-20 02:31:18.123051
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 128 --testcase 2
2021-09-20 02:31:28.003929
b''

Starting computation for size [128, 128, 256]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-20 02:31:38.640501
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-20 02:31:48.567268
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-20 02:31:58.901345
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-20 02:32:08.504551
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-20 02:32:19.102953
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-20 02:32:28.962210
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-20 02:32:39.466855
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-20 02:32:51.731968
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-20 02:33:04.675387
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 128 -nz 256 --testcase 2
2021-09-20 02:33:15.197767
b''

Starting computation for size [128, 256, 256]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-20 02:33:26.226162
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-20 02:33:39.471519
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-20 02:33:53.605443
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-20 02:34:07.497501
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-20 02:34:19.674043
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-20 02:34:30.668074
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-20 02:34:43.329370
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-20 02:34:56.485676
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-20 02:35:09.591781
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 128 -ny 256 -nz 256 --testcase 2
2021-09-20 02:35:20.674461
b''

Starting computation for size 256
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-20 02:35:32.873319
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-20 02:35:46.088451
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-20 02:35:58.165995
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-20 02:36:12.264484
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-20 02:36:23.598141
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-20 02:36:33.660783
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-20 02:36:45.110186
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-20 02:36:57.224071
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-20 02:37:08.890375
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 256 --testcase 2
2021-09-20 02:37:18.703045
b''

Starting computation for size [256, 256, 512]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-20 02:37:29.894628
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-20 02:37:42.774134
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-20 02:37:56.092034
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-20 02:38:07.424542
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-20 02:38:18.956035
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-20 02:38:29.427827
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-20 02:38:42.457730
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-20 02:38:54.540353
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-20 02:39:08.133226
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 256 -nz 512 --testcase 2
2021-09-20 02:39:19.378592
b''

Starting computation for size [256, 512, 512]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-20 02:39:31.615217
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-20 02:39:44.568719
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-20 02:39:57.859681
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-20 02:40:08.095076
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-20 02:40:19.785530
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-20 02:40:30.475056
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-20 02:40:43.198142
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-20 02:40:55.593711
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-20 02:41:07.364737
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 256 -ny 512 -nz 512 --testcase 2
2021-09-20 02:41:17.688327
b''

Starting computation for size 512
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-20 02:41:30.024623
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-20 02:41:42.773975
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-20 02:41:56.108475
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-20 02:42:06.773043
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-20 02:42:17.824551
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-20 02:42:28.578432
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-20 02:42:41.615606
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-20 02:42:54.248498
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-20 02:43:05.921799
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 512 --testcase 2
2021-09-20 02:43:16.339669
b''

Starting computation for size [512, 512, 1024]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-20 02:43:28.759290
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-20 02:43:41.514109
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-20 02:43:53.336066
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-20 02:44:04.126951
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-20 02:44:15.125561
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-20 02:44:26.205167
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-20 02:44:37.831588
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-20 02:44:50.934325
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-20 02:45:02.906432
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 512 -nz 1024 --testcase 2
2021-09-20 02:45:14.375008
b''

Starting computation for size [512, 1024, 1024]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-20 02:45:26.541064
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-20 02:45:40.351696
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-20 02:45:52.843599
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-20 02:46:05.324603
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-20 02:46:17.144862
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-20 02:46:29.116319
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-20 02:46:41.022416
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-20 02:46:55.147851
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-20 02:47:07.843674
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 512 -ny 1024 -nz 1024 --testcase 2
2021-09-20 02:47:19.833939
b''

Starting computation for size 1024
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-20 02:47:31.564062
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-20 02:47:46.241968
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-20 02:47:59.140502
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-20 02:48:12.853570
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-20 02:48:25.272374
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-20 02:48:39.134710
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-20 02:48:51.842921
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-20 02:49:08.181339
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-20 02:49:21.945132
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 1024 --testcase 2
2021-09-20 02:49:35.903096
b''

Starting computation for size [1024, 1024, 2048]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-20 02:49:48.615106
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-20 02:50:08.352206
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-20 02:50:22.722274
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-20 02:50:40.306559
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-20 02:50:54.464485
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-20 02:51:11.392501
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-20 02:51:25.742211
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-20 02:51:45.846197
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-20 02:52:01.342680
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 1024 -nz 2048 --testcase 2
2021-09-20 02:52:19.308156
b''

Starting computation for size [1024, 2048, 2048]
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-20 02:52:33.646877
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-20 02:53:02.566341
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-20 02:53:20.353527
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-20 02:53:46.314326
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-20 02:54:04.393819
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-20 02:54:28.151199
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-20 02:54:45.919419
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-20 02:55:13.935576
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-20 02:55:33.250821
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 1024 -ny 2048 -nz 2048 --testcase 2
2021-09-20 02:55:58.929452
b''

Starting computation for size 2048
-> Executing test 0
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-20 02:56:16.826090
b''

-> Executing test 1
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-20 02:57:05.223608
b''

-> Executing test 2
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-20 02:57:30.513430
b''

-> Executing test 3
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-20 02:58:12.798555
b''

-> Executing test 4
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-20 02:58:37.888661
b''

-> Executing test 5
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-20 02:59:15.760370
b''

-> Executing test 6
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-20 02:59:41.219806
b''

-> Executing test 7
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-20 03:00:27.320367
b''

-> Executing test 8
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-20 03:00:55.000010
b''

-> Executing test 9
mpiexec -n 64 --mca btl_smcuda_use_cuda_ipc 0 --mca btl_openib_warn_default_gid_prefix 0 --hostfile ../mpi/hostfile_0 --rankfile ../mpi/rankfile_0 slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 64 -b ../benchmarks/bwunicluster/gpu8/large/inverse -s Z_Then_YX --opt 1 -nx 2048 -ny 2048 -nz 2048 --testcase 2
2021-09-20 03:01:36.155723
b''


============================= JOB FEEDBACK =============================

NodeName=uc2n[507-511,514-516]
Job ID: 19803792
Cluster: uc2
User/Group: st_st160727/st_us-051200
State: COMPLETED (exit code 0)
Nodes: 8
Cores per node: 80
CPU Utilized: 12-08:44:55
CPU Efficiency: 6.60% of 187-11:22:40 core-walltime
Job Wall-clock time: 07:01:49
Memory Utilized: 55.08 GB
Memory Efficiency: 0.00% of 0.00 MB
