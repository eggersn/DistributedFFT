Modules loaded
start building
-- The CUDA compiler identification is NVIDIA 10.1.243
-- The CXX compiler identification is GNU 9.3.0
-- Check for working CUDA compiler: /usr/bin/nvcc
-- Check for working CUDA compiler: /usr/bin/nvcc -- works
-- Detecting CUDA compiler ABI info
-- Detecting CUDA compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++
-- Check for working CXX compiler: /usr/bin/c++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found MPI_CXX: /import/local.ubuntu/sw/openmpi/openmpi-4.1.1-cuda-u2004/lib/libmpi.so (found version "3.1") 
-- Found MPI: TRUE (found version "3.1")  
-- Configuring done
-- Generating done
-- Build files have been written to: /home/eggersn/DistributedFFT/build_argon
Scanning dependencies of target mpicufft
[  3%] Building CUDA object CMakeFiles/mpicufft.dir/src/mpicufft.cpp.o
[  6%] Linking CUDA shared library libmpicufft.so
[  6%] Built target mpicufft
Scanning dependencies of target test_base
[  9%] Building CUDA object CMakeFiles/test_base.dir/tests/src/base.cu.o
[ 12%] Linking CUDA shared library libtest_base.so
[ 12%] Built target test_base
Scanning dependencies of target timer
[ 15%] Building CXX object CMakeFiles/timer.dir/src/timer.cpp.o
[ 18%] Linking CXX shared library libtimer.so
[ 18%] Built target timer
Scanning dependencies of target reference_tests
[ 21%] Building CUDA object CMakeFiles/reference_tests.dir/tests/src/reference/reference.cu.o
/home/eggersn/DistributedFFT/tests/src/reference/reference.cu: In instantiation of ‘int Tests_Reference<T>::testcase3(int, int) [with T = float]’:
/home/eggersn/DistributedFFT/tests/src/reference/reference.cu:1155:16:   required from here
/home/eggersn/DistributedFFT/tests/src/reference/reference.cu:1073:7: warning: format not a string literal and no format arguments [-Wformat-security]
     printf(filename.c_str());
     ~~^~~~~~~~~~~~~~~~~~
/home/eggersn/DistributedFFT/tests/src/reference/reference.cu: In instantiation of ‘int Tests_Reference<T>::testcase3(int, int) [with T = double]’:
/home/eggersn/DistributedFFT/tests/src/reference/reference.cu:1156:16:   required from here
/home/eggersn/DistributedFFT/tests/src/reference/reference.cu:1073:7: warning: format not a string literal and no format arguments [-Wformat-security]
[ 24%] Linking CUDA shared library libreference_tests.so
[ 24%] Built target reference_tests
Scanning dependencies of target reference
[ 27%] Building CUDA object CMakeFiles/reference.dir/tests/src/reference/main.cpp.o
[ 30%] Linking CUDA executable reference
[ 30%] Built target reference
Scanning dependencies of target slab_decomp
[ 33%] Building CUDA object CMakeFiles/slab_decomp.dir/src/slab/default/mpicufft_slab.cpp.o
[ 36%] Building CUDA object CMakeFiles/slab_decomp.dir/src/slab/default/mpicufft_slab_opt1.cpp.o
[ 39%] Building CUDA object CMakeFiles/slab_decomp.dir/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp.o
/home/eggersn/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp(508): warning: variable "send_ptr" was set but never used
          detected during instantiation of "void MPIcuFFT_Slab_Z_Then_YX<T>::Peer2Peer_Communication(void *, __nv_bool) [with T=float]" 
(748): here

/home/eggersn/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp(508): warning: variable "send_ptr" was set but never used
          detected during instantiation of "void MPIcuFFT_Slab_Z_Then_YX<T>::Peer2Peer_Communication(void *, __nv_bool) [with T=double]" 
(748): here

[ 42%] Building CUDA object CMakeFiles/slab_decomp.dir/src/slab/z_then_yx/mpicufft_slab_z_then_yx_opt1.cpp.o
[ 45%] Building CUDA object CMakeFiles/slab_decomp.dir/src/slab/y_then_zx/mpicufft_slab_y_then_zx.cpp.o
[ 48%] Linking CUDA shared library libslab_decomp.so
[ 48%] Built target slab_decomp
Scanning dependencies of target slab_tests
[ 51%] Building CUDA object CMakeFiles/slab_tests.dir/tests/src/slab/base.cu.o
[ 54%] Building CUDA object CMakeFiles/slab_tests.dir/tests/src/slab/random_dist_default.cu.o
[ 57%] Building CUDA object CMakeFiles/slab_tests.dir/tests/src/slab/random_dist_y_then_zx.cu.o
[ 60%] Building CUDA object CMakeFiles/slab_tests.dir/tests/src/slab/random_dist_z_then_yx.cu.o
[ 63%] Linking CUDA shared library libslab_tests.so
[ 63%] Built target slab_tests
Scanning dependencies of target slab
[ 66%] Building CUDA object CMakeFiles/slab.dir/tests/src/slab/main.cpp.o
[ 69%] Linking CUDA executable slab
[ 69%] Built target slab
Scanning dependencies of target pencil_decomp
[ 72%] Building CUDA object CMakeFiles/pencil_decomp.dir/src/pencil/mpicufft_pencil.cpp.o
[ 75%] Building CUDA object CMakeFiles/pencil_decomp.dir/src/pencil/mpicufft_pencil_opt1.cpp.o
[ 78%] Linking CUDA shared library libpencil_decomp.so
[ 78%] Built target pencil_decomp
Scanning dependencies of target pencil_tests
[ 81%] Building CUDA object CMakeFiles/pencil_tests.dir/tests/src/pencil/base.cu.o
[ 84%] Building CUDA object CMakeFiles/pencil_tests.dir/tests/src/pencil/random_dist_1D.cu.o
[ 87%] Building CUDA object CMakeFiles/pencil_tests.dir/tests/src/pencil/random_dist_2D.cu.o
[ 90%] Building CUDA object CMakeFiles/pencil_tests.dir/tests/src/pencil/random_dist_3D.cu.o
[ 93%] Linking CUDA shared library libpencil_tests.so
[ 93%] Built target pencil_tests
Scanning dependencies of target pencil
[ 96%] Building CUDA object CMakeFiles/pencil.dir/tests/src/pencil/main.cpp.o
[100%] Linking CUDA executable pencil
[100%] Built target pencil
finished building
start python script
-----------------------------------------------------------------------------
Slab 2D->1D default
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[39029,1],2]
  Exit code:    1
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[38947,1],3]
  Exit code:    1
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[39360,1],2]
  Exit code:    1
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[39399,1],3]
  Exit code:    1
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[39300,1],2]
  Exit code:    1
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[39338,1],3]
  Exit code:    1
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[39247,1],3]
  Exit code:    1
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[39278,1],3]
  Exit code:    1
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[39187,1],3]
  Exit code:    1
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[39217,1],3]
  Exit code:    1
--------------------------------------------------------------------------
Starting computation for size 128
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/argon/forward -nx 128 -ny 128 -nz 128
2021-09-28 13:51:37.707978
b''

Starting computation for size [128, 128, 256]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/argon/forward -nx 128 -ny 128 -nz 256
2021-09-28 13:51:45.948214
b''

Starting computation for size [128, 256, 256]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/argon/forward -nx 128 -ny 256 -nz 256
2021-09-28 13:51:51.335913
b''

Starting computation for size 256
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/argon/forward -nx 256 -ny 256 -nz 256
2021-09-28 13:51:57.083915
b''

Starting computation for size [256, 256, 512]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/argon/forward -nx 256 -ny 256 -nz 512
2021-09-28 13:52:03.646996
b''

Starting computation for size [256, 512, 512]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/argon/forward -nx 256 -ny 512 -nz 512
2021-09-28 13:52:12.092206
b''

Starting computation for size 512
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/argon/forward -nx 512 -ny 512 -nz 512
2021-09-28 13:52:24.103899
b''

Starting computation for size [512, 512, 1024]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/argon/forward -nx 512 -ny 512 -nz 1024
2021-09-28 13:52:43.073361
b''

Starting computation for size [512, 1024, 1024]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/argon/forward -nx 512 -ny 1024 -nz 1024
2021-09-28 13:53:16.176153
b''

Starting computation for size 1024
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/argon/forward -nx 1024 -ny 1024 -nz 1024
2021-09-28 13:54:17.527884
b''

Starting computation for size 128
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-28 13:56:16.448130
b'Result (avg): 1.91723e-05\nResult (max): 7.43487e-05\n'

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-28 13:56:21.776233
b'Result (avg): 1.91723e-05\nResult (max): 7.43487e-05\n'

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-28 13:56:26.903827
b'Result (avg): 1.91723e-05\nResult (max): 7.43487e-05\n'

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-28 13:56:31.744429
b'Result (avg): 1.91723e-05\nResult (max): 7.43487e-05\n'

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-28 13:56:36.960078
b'Result (avg): 1.91723e-05\nResult (max): 7.43487e-05\n'

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-28 13:56:41.919858
b'Result (avg): 1.91723e-05\nResult (max): 7.43487e-05\n'

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-28 13:56:47.068490
b'Result (avg): 1.91723e-05\nResult (max): 7.43487e-05\n'

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-28 13:56:51.960020
b'Result (avg): 1.91723e-05\nResult (max): 7.43487e-05\n'

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-28 13:56:57.087163
b'Result (avg): 1.91723e-05\nResult (max): 7.43487e-05\n'

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-28 13:57:02.143886
b'Result (avg): 1.91723e-05\nResult (max): 7.43487e-05\n'

Starting computation for size 256
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-28 13:57:07.088010
b'Result (avg): 5.82853e-09\nResult (max): 6.87005e-08\n'

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-28 13:57:12.783835
b'Result (avg): 5.82853e-09\nResult (max): 6.87005e-08\n'

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-28 13:57:18.552277
b'Result (avg): 5.82853e-09\nResult (max): 6.87005e-08\n'

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-28 13:57:24.224019
b'Result (avg): 5.82853e-09\nResult (max): 6.87005e-08\n'

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-28 13:57:30.144382
b'Result (avg): 5.82853e-09\nResult (max): 6.87005e-08\n'

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-28 13:57:35.747965
b'Result (avg): 5.82853e-09\nResult (max): 6.87005e-08\n'

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-28 13:57:41.560275
b'Result (avg): 5.82853e-09\nResult (max): 6.87005e-08\n'

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-28 13:57:47.264486
b'Result (avg): 5.82853e-09\nResult (max): 6.87005e-08\n'

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-28 13:57:52.980205
b'Result (avg): 5.82853e-09\nResult (max): 6.87005e-08\n'

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-28 13:57:58.716060
b'Result (avg): 5.82853e-09\nResult (max): 6.87005e-08\n'

Starting computation for size 512
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-28 13:58:04.676037
b'Result (avg): 0.000153466\nResult (max): 0.00059502\n'

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-28 13:58:16.276384
b'Result (avg): 0.000153466\nResult (max): 0.00059502\n'

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-28 13:58:27.672164
b'Result (avg): 0.000153466\nResult (max): 0.00059502\n'

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-28 13:58:39.284089
b'Result (avg): 0.000153466\nResult (max): 0.00059502\n'

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-28 13:58:50.632191
b'Result (avg): 0.000153466\nResult (max): 0.00059502\n'

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-28 13:59:02.283858
b'Result (avg): 0.000153466\nResult (max): 0.00059502\n'

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-28 13:59:13.739825
b'Result (avg): 0.000153466\nResult (max): 0.00059502\n'

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-28 13:59:25.371771
b'Result (avg): 0.000153466\nResult (max): 0.00059502\n'

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-28 13:59:36.664043
b'Result (avg): 0.000153466\nResult (max): 0.00059502\n'

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-28 13:59:48.368060
b'Result (avg): 0.000153466\nResult (max): 0.00059502\n'

Starting computation for size 1024
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -t 4 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-28 13:59:59.803761
b'Error 2 at /home/eggersn/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/eggersn/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/eggersn/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -t 4 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-28 14:00:50.412160
b'Error 2 at /home/eggersn/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/eggersn/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/eggersn/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -t 4 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-28 14:01:41.459933
b'Error 2 at /home/eggersn/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/eggersn/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/eggersn/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -t 4 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-28 14:02:32.336216
b'Error 2 at /home/eggersn/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/eggersn/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/eggersn/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -t 4 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-28 14:03:23.172123
b'Error 2 at /home/eggersn/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/eggersn/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/eggersn/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -t 4 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-28 14:04:13.879862
b'Error 2 at /home/eggersn/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/eggersn/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/eggersn/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -t 4 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-28 14:05:04.687972
b'Error 2 at /home/eggersn/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/eggersn/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/eggersn/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -t 4 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-28 14:05:54.359967
b'Error 2 at /home/eggersn/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/eggersn/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/eggersn/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -t 4 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-28 14:06:45.120097
b'Error 2 at /home/eggersn/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/eggersn/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/eggersn/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -t 4 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-28 14:07:35.703848
b'Error 2 at /home/eggersn/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/eggersn/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\nError 2 at /home/eggersn/DistributedFFT/src/slab/default/mpicufft_slab.cpp:231\n'

Slab 1D->2D default
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[34705,1],3]
  Exit code:    1
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[34736,1],3]
  Exit code:    1
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[34644,1],3]
  Exit code:    1
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[34617,1],1]
  Exit code:    1
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[32984,1],3]
  Exit code:    1
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[33023,1],3]
  Exit code:    1
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[32924,1],3]
  Exit code:    1
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[32834,1],2]
  Exit code:    1
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[32864,1],1]
  Exit code:    1
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[32779,1],3]
  Exit code:    1
--------------------------------------------------------------------------
Starting computation for size 128
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/argon/forward -s Z_Then_YX -nx 128 -ny 128 -nz 128
2021-09-28 14:08:26.707923
b''

Starting computation for size [128, 128, 256]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/argon/forward -s Z_Then_YX -nx 128 -ny 128 -nz 256
2021-09-28 14:08:34.620291
b''

Starting computation for size [128, 256, 256]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/argon/forward -s Z_Then_YX -nx 128 -ny 256 -nz 256
2021-09-28 14:08:42.360225
b''

Starting computation for size 256
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/argon/forward -s Z_Then_YX -nx 256 -ny 256 -nz 256
2021-09-28 14:08:53.007951
b''

Starting computation for size [256, 256, 512]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/argon/forward -s Z_Then_YX -nx 256 -ny 256 -nz 512
2021-09-28 14:09:09.503060
b''

Starting computation for size [256, 512, 512]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/argon/forward -s Z_Then_YX -nx 256 -ny 512 -nz 512
2021-09-28 14:09:26.804280
b''

Starting computation for size 512
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/argon/forward -s Z_Then_YX -nx 512 -ny 512 -nz 512
2021-09-28 14:09:57.931938
b''

Starting computation for size [512, 512, 1024]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/argon/forward -s Z_Then_YX -nx 512 -ny 512 -nz 1024
2021-09-28 14:10:58.408072
b''

Starting computation for size [512, 1024, 1024]
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/argon/forward -s Z_Then_YX -nx 512 -ny 1024 -nz 1024
2021-09-28 14:12:06.654835
b''

Starting computation for size 1024
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 10 --iterations 20 --double_prec -p 4 -b ../benchmarks/argon/forward -s Z_Then_YX -nx 1024 -ny 1024 -nz 1024
2021-09-28 14:14:18.531981
b''

Starting computation for size 128
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -s Z_Then_YX -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-28 14:18:41.934764
b'Result (avg): 1.91723e-05\nResult (max): 7.43516e-05\n'

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -s Z_Then_YX -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-28 14:18:47.043896
b'Result (avg): 1.91723e-05\nResult (max): 7.43516e-05\n'

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -s Z_Then_YX -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-28 14:18:52.099372
b'Result (avg): 1.91723e-05\nResult (max): 7.43516e-05\n'

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -s Z_Then_YX -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-28 14:18:56.996309
b'Result (avg): 1.91723e-05\nResult (max): 7.43516e-05\n'

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -s Z_Then_YX -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-28 14:19:01.984179
b'Result (avg): 1.91723e-05\nResult (max): 7.43516e-05\n'

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -s Z_Then_YX -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-28 14:19:06.851959
b'Result (avg): 1.91723e-05\nResult (max): 7.43516e-05\n'

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -s Z_Then_YX -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-28 14:19:11.995734
b'Result (avg): 1.91723e-05\nResult (max): 7.43516e-05\n'

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -s Z_Then_YX -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-28 14:19:16.871899
b'Result (avg): 1.91723e-05\nResult (max): 7.43516e-05\n'

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -s Z_Then_YX -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-28 14:19:21.915866
b'Result (avg): 1.91723e-05\nResult (max): 7.43516e-05\n'

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -s Z_Then_YX -t 4 -nx 128 -ny 128 -nz 128 --testcase 4
2021-09-28 14:19:26.816343
b'Result (avg): 1.91723e-05\nResult (max): 7.43516e-05\n'

Starting computation for size 256
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -s Z_Then_YX -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-28 14:19:31.992075
b'Result (avg): 5.18626e-09\nResult (max): 5.32455e-08\n'

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -s Z_Then_YX -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-28 14:19:37.884298
b'Result (avg): 5.18626e-09\nResult (max): 5.32455e-08\n'

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -s Z_Then_YX -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-28 14:19:43.627749
b'Result (avg): 5.18626e-09\nResult (max): 5.32455e-08\n'

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -s Z_Then_YX -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-28 14:19:49.327764
b'Result (avg): 5.18626e-09\nResult (max): 5.32455e-08\n'

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -s Z_Then_YX -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-28 14:19:55.312063
b'Result (avg): 5.18626e-09\nResult (max): 5.32455e-08\n'

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -s Z_Then_YX -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-28 14:20:01.067652
b'Result (avg): 5.18626e-09\nResult (max): 5.32455e-08\n'

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -s Z_Then_YX -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-28 14:20:07.428396
b'Result (avg): 5.18626e-09\nResult (max): 5.32455e-08\n'

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -s Z_Then_YX -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-28 14:20:13.075700
b'Result (avg): 5.18626e-09\nResult (max): 5.32455e-08\n'

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -s Z_Then_YX -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-28 14:20:18.819662
b'Result (avg): 5.18626e-09\nResult (max): 5.32455e-08\n'

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -s Z_Then_YX -t 4 -nx 256 -ny 256 -nz 256 --testcase 4
2021-09-28 14:20:24.532163
b'Result (avg): 5.18626e-09\nResult (max): 5.32455e-08\n'

Starting computation for size 512
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -s Z_Then_YX -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-28 14:20:31.188093
b'Result (avg): 0.000153465\nResult (max): 0.000594862\n'

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -s Z_Then_YX -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-28 14:20:43.043754
b'Result (avg): 0.000153465\nResult (max): 0.000594862\n'

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -s Z_Then_YX -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-28 14:20:54.495995
b'Result (avg): 0.000153465\nResult (max): 0.000594862\n'

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -s Z_Then_YX -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-28 14:21:06.312147
b'Result (avg): 0.000153465\nResult (max): 0.000594862\n'

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -s Z_Then_YX -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-28 14:21:17.911740
b'Result (avg): 0.000153465\nResult (max): 0.000594862\n'

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -s Z_Then_YX -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-28 14:21:29.671939
b'Result (avg): 0.000153465\nResult (max): 0.000594862\n'

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -s Z_Then_YX -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-28 14:21:43.539695
b'Result (avg): 0.000153465\nResult (max): 0.000594862\n'

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -s Z_Then_YX -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-28 14:21:55.503908
b'Result (avg): 0.000153465\nResult (max): 0.000594862\n'

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -s Z_Then_YX -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-28 14:22:07.128160
b'Result (avg): 0.000153465\nResult (max): 0.000594862\n'

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -s Z_Then_YX -t 4 -nx 512 -ny 512 -nz 512 --testcase 4
2021-09-28 14:22:19.020325
b'Result (avg): 0.000153465\nResult (max): 0.000594862\n'

Starting computation for size 1024
-> Executing test 0
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -s Z_Then_YX -t 4 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-28 14:22:33.872030
b'Error 2 at /home/eggersn/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/eggersn/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/eggersn/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/eggersn/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\n'

-> Executing test 1
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -s Z_Then_YX -t 4 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-28 14:23:25.547975
b'Error 2 at /home/eggersn/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/eggersn/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/eggersn/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/eggersn/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\n'

-> Executing test 2
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd Streams --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -s Z_Then_YX -t 4 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-28 14:24:17.691764
b'Error 2 at /home/eggersn/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/eggersn/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/eggersn/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/eggersn/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\n'

-> Executing test 3
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd Streams --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -s Z_Then_YX -t 4 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-28 14:25:09.660031
b'Error 2 at /home/eggersn/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/eggersn/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/eggersn/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/eggersn/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\n'

-> Executing test 4
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -s Z_Then_YX -t 4 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-28 14:26:01.191872
b'Error 2 at /home/eggersn/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/eggersn/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/eggersn/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/eggersn/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\n'

-> Executing test 5
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm Peer2Peer -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -s Z_Then_YX -t 4 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-28 14:26:53.083601
b'Error 2 at /home/eggersn/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/eggersn/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/eggersn/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/eggersn/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\n'

-> Executing test 6
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm All2All -snd Sync --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -s Z_Then_YX -t 4 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-28 14:27:45.216031
b'Error 2 at /home/eggersn/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/eggersn/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/eggersn/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/eggersn/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\n'

-> Executing test 7
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm All2All -snd Sync --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -s Z_Then_YX -t 4 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-28 14:28:37.187893
b'Error 2 at /home/eggersn/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/eggersn/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/eggersn/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/eggersn/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\n'

-> Executing test 8
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm All2All -snd MPI_Type --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -s Z_Then_YX -t 4 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-28 14:29:29.279961
b'Error 2 at /home/eggersn/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/eggersn/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/eggersn/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/eggersn/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\n'

-> Executing test 9
mpiexec -n 4 --hostfile ../mpi/hostfile_argon --rankfile ../mpi/rankfile_argon slab -comm All2All -snd MPI_Type --cuda_aware --warmup-rounds 1 --iterations 0 --double_prec -p 4 -b ../benchmarks/argon/forward -s Z_Then_YX -t 4 -nx 1024 -ny 1024 -nz 1024 --testcase 4
2021-09-28 14:30:20.596075
b'Error 2 at /home/eggersn/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/eggersn/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/eggersn/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\nError 2 at /home/eggersn/DistributedFFT/src/slab/z_then_yx/mpicufft_slab_z_then_yx.cpp:204\n'

all done
